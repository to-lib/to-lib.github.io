---
sidebar_position: 9
title: "æœ€ä½³å®è·µ"
description: "Kafka ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ"
---

# Kafka æœ€ä½³å®è·µ

## è®¾è®¡åŸåˆ™

### Topic è®¾è®¡

#### å‘½åè§„èŒƒ

```
<ä¸šåŠ¡åŸŸ>.<å®ä½“>.<äº‹ä»¶ç±»å‹>

ç¤ºä¾‹ï¼š
- order.payment.completed
- user.registration.created
- inventory.stock.updated
```

#### åˆ†åŒºç­–ç•¥

```java
// 1. æŒ‰ä¸šåŠ¡ key åˆ†åŒºï¼ˆä¿è¯é¡ºåºï¼‰
producer.send(new ProducerRecord<>("orders", orderId, orderData));

// 2. æŒ‰æ—¶é—´åˆ†åŒºï¼ˆä¾¿äºæ¸…ç†ï¼‰
String key = LocalDate.now().toString();
producer.send(new ProducerRecord<>("logs", key, logData));

// 3. è½®è¯¢åˆ†åŒºï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
producer.send(new ProducerRecord<>("metrics", null, metricData));
```

### å‰¯æœ¬é…ç½®

```properties
# æ¨èé…ç½®
default.replication.factor=3
min.insync.replicas=2
unclean.leader.election.enable=false
```

> **è¯´æ˜**: 3 å‰¯æœ¬ + 2 æœ€å°åŒæ­¥ = å®¹å¿ 1 ä¸ªèŠ‚ç‚¹æ•…éšœ

## ç”Ÿäº§è€…æœ€ä½³å®è·µ

### å¯é æ€§é…ç½®

```java
Properties props = new Properties();

// ç¡®è®¤æœºåˆ¶ï¼šç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
props.put("acks", "all");

// å¹‚ç­‰æ€§ï¼šé˜²æ­¢æ¶ˆæ¯é‡å¤
props.put("enable.idempotence", "true");

// é‡è¯•é…ç½®
props.put("retries", Integer.MAX_VALUE);
props.put("delivery.timeout.ms", 120000);
props.put("max.in.flight.requests.per.connection", 5);
```

### èµ„æºç®¡ç†

```java
// ä½¿ç”¨ try-with-resources
try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
    for (String message : messages) {
        producer.send(new ProducerRecord<>("topic", message));
    }
} // è‡ªåŠ¨å…³é—­

// æˆ–è€…æ‰‹åŠ¨ç®¡ç†
producer.flush();  // ç¡®ä¿æ‰€æœ‰æ¶ˆæ¯å‘é€
producer.close();  // å…³é—­è¿æ¥
```

### å¼‚å¸¸å¤„ç†

```java
producer.send(record, (metadata, exception) -> {
    if (exception != null) {
        if (exception instanceof RetriableException) {
            // å¯é‡è¯•å¼‚å¸¸ï¼Œè®°å½•åé‡è¯•
            retryQueue.add(record);
        } else {
            // ä¸å¯é‡è¯•å¼‚å¸¸ï¼Œè®°å½•åˆ°æ­»ä¿¡é˜Ÿåˆ—
            deadLetterQueue.add(record);
        }
        logger.error("å‘é€å¤±è´¥", exception);
    }
});
```

## æ¶ˆè´¹è€…æœ€ä½³å®è·µ

### ä½ç§»ç®¡ç†

```java
// æ¨èï¼šæ‰‹åŠ¨æäº¤
props.put("enable.auto.commit", "false");

try {
    while (running) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

        for (ConsumerRecord<String, String> record : records) {
            processRecord(record);  // å…ˆå¤„ç†
        }

        consumer.commitSync();  // å†æäº¤
    }
} finally {
    consumer.commitSync();  // å…³é—­å‰æäº¤
    consumer.close();
}
```

### ä¼˜é›…å…³é—­

```java
private volatile boolean running = true;

public void shutdown() {
    running = false;
    consumer.wakeup();  // å”¤é†’é˜»å¡çš„ poll()
}

// åœ¨ä¸»å¾ªç¯ä¸­
try {
    while (running) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        // å¤„ç†æ¶ˆæ¯
    }
} catch (WakeupException e) {
    if (running) throw e;  // æ„å¤–å”¤é†’
} finally {
    consumer.commitSync();
    consumer.close();
}
```

### æ¶ˆæ¯å¤„ç†

```java
// å¹‚ç­‰å¤„ç†
public void processRecord(ConsumerRecord<String, String> record) {
    String messageId = record.headers().lastHeader("message-id").value().toString();

    // æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if (processedMessages.contains(messageId)) {
        logger.info("æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡: {}", messageId);
        return;
    }

    // å¤„ç†æ¶ˆæ¯
    doProcess(record);

    // è®°å½•å·²å¤„ç†
    processedMessages.add(messageId);
}
```

## è¿ç»´æœ€ä½³å®è·µ

### ç›‘æ§æŒ‡æ ‡

```java
// ç”Ÿäº§è€…ç›‘æ§
record-send-rate          // å‘é€é€Ÿç‡
record-error-rate         // é”™è¯¯ç‡
request-latency-avg       // å¹³å‡å»¶è¿Ÿ
batch-size-avg            // å¹³å‡æ‰¹æ¬¡å¤§å°

// æ¶ˆè´¹è€…ç›‘æ§
records-consumed-rate     // æ¶ˆè´¹é€Ÿç‡
records-lag-max           // æœ€å¤§ç§¯å‹
fetch-latency-avg         // æ‹‰å–å»¶è¿Ÿ
commit-latency-avg        // æäº¤å»¶è¿Ÿ

// Broker ç›‘æ§
UnderReplicatedPartitions // æ¬ å¤åˆ¶åˆ†åŒº
OfflinePartitionsCount    // ç¦»çº¿åˆ†åŒº
ActiveControllerCount     // æ´»è·ƒæ§åˆ¶å™¨
```

### å‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦è§„åˆ™ç¤ºä¾‹
groups:
  - name: kafka
    rules:
      - alert: KafkaConsumerLag
        expr: kafka_consumergroup_lag > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ¶ˆè´¹è€…ç§¯å‹è¶…è¿‡ 10000"

      - alert: KafkaUnderReplicated
        expr: kafka_server_replicamanager_underreplicatedpartitions > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "å­˜åœ¨æ¬ å¤åˆ¶åˆ†åŒº"
```

### æ—¥å¿—é…ç½®

```properties
# log4j.properties
log4j.rootLogger=INFO, stdout, kafkaAppender

log4j.appender.kafkaAppender=org.apache.kafka.log4jappender.KafkaLog4jAppender
log4j.appender.kafkaAppender.brokerList=localhost:9092
log4j.appender.kafkaAppender.topic=application-logs
```

## å®‰å…¨æœ€ä½³å®è·µ

### è®¤è¯é…ç½®

```properties
# ç”Ÿäº§è€…/æ¶ˆè´¹è€…
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-512
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="user" \
  password="password";
```

### åŠ å¯†ä¼ è¾“

```properties
# SSL é…ç½®
ssl.keystore.location=/path/to/keystore.jks
ssl.keystore.password=keystore-password
ssl.key.password=key-password
ssl.truststore.location=/path/to/truststore.jks
ssl.truststore.password=truststore-password
```

### æƒé™æ§åˆ¶

```bash
# æœ€å°æƒé™åŸåˆ™
# ç”Ÿäº§è€…åªå…è®¸å†™å…¥
kafka-acls.sh --add --allow-principal User:producer \
  --operation Write --topic orders

# æ¶ˆè´¹è€…åªå…è®¸è¯»å–
kafka-acls.sh --add --allow-principal User:consumer \
  --operation Read --topic orders \
  --group order-processor
```

## é”™è¯¯å¤„ç†æ¨¡å¼

### æ­»ä¿¡é˜Ÿåˆ—

```java
public void consumeWithDLQ() {
    while (running) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

        for (ConsumerRecord<String, String> record : records) {
            try {
                processRecord(record);
            } catch (Exception e) {
                // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                sendToDLQ(record, e);
            }
        }

        consumer.commitSync();
    }
}

private void sendToDLQ(ConsumerRecord<String, String> record, Exception e) {
    ProducerRecord<String, String> dlqRecord = new ProducerRecord<>(
        record.topic() + ".dlq",
        record.key(),
        record.value()
    );
    dlqRecord.headers()
        .add("original-topic", record.topic().getBytes())
        .add("error-message", e.getMessage().getBytes());

    dlqProducer.send(dlqRecord);
}
```

### é‡è¯•ç­–ç•¥

```java
public void processWithRetry(ConsumerRecord<String, String> record) {
    int maxRetries = 3;
    int retryCount = 0;

    while (retryCount < maxRetries) {
        try {
            processRecord(record);
            return;
        } catch (RetriableException e) {
            retryCount++;
            long backoff = (long) Math.pow(2, retryCount) * 100;
            Thread.sleep(backoff);
        }
    }

    // è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‘é€åˆ° DLQ
    sendToDLQ(record, new MaxRetriesExceededException());
}
```

## æµ‹è¯•æœ€ä½³å®è·µ

### å•å…ƒæµ‹è¯•

```java
// ä½¿ç”¨ MockProducer
MockProducer<String, String> mockProducer = new MockProducer<>(
    true, new StringSerializer(), new StringSerializer());

// æµ‹è¯•å‘é€
myService.sendMessage("test-message");
assertEquals(1, mockProducer.history().size());
assertEquals("test-message", mockProducer.history().get(0).value());
```

### é›†æˆæµ‹è¯•

```java
// ä½¿ç”¨ Testcontainers
@Container
static KafkaContainer kafka = new KafkaContainer(
    DockerImageName.parse("confluentinc/cp-kafka:7.5.0"));

@Test
void testProducerConsumer() {
    Properties props = new Properties();
    props.put("bootstrap.servers", kafka.getBootstrapServers());
    // ... æµ‹è¯•ä»£ç 
}
```

## æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

- [ ] Topic åˆ†åŒºæ•°å’Œå‰¯æœ¬æ•°é…ç½®æ­£ç¡®
- [ ] ç”Ÿäº§è€…é…ç½®äº† `acks=all` å’Œå¹‚ç­‰æ€§
- [ ] æ¶ˆè´¹è€…é…ç½®äº†æ‰‹åŠ¨æäº¤
- [ ] é…ç½®äº†ç›‘æ§å’Œå‘Šè­¦
- [ ] é…ç½®äº†è®¿é—®æ§åˆ¶å’ŒåŠ å¯†
- [ ] è¿›è¡Œäº†æ€§èƒ½æµ‹è¯•
- [ ] å‡†å¤‡äº†æ•…éšœæ¢å¤æ–¹æ¡ˆ

### è¿ç»´æ£€æŸ¥

- [ ] å®šæœŸæ£€æŸ¥æ¶ˆè´¹è€…ç§¯å‹
- [ ] ç›‘æ§ Broker ç£ç›˜ä½¿ç”¨ç‡
- [ ] æ£€æŸ¥æ¬ å¤åˆ¶åˆ†åŒº
- [ ] å®šæœŸå¤‡ä»½ Topic é…ç½®
- [ ] åˆ¶å®šæ—¥å¿—æ¸…ç†ç­–ç•¥

## ä¸‹ä¸€æ­¥

- âš¡ [æ€§èƒ½ä¼˜åŒ–](/docs/kafka/performance-optimization) - æ€§èƒ½è°ƒä¼˜æŒ‡å—
- ğŸ”§ [é›†ç¾¤ç®¡ç†](/docs/kafka/cluster-management) - é›†ç¾¤ç®¡ç†æ“ä½œ
- ğŸ“Š [ç›‘æ§ä¸è¿ç»´](/docs/kafka/monitoring) - ç›‘æ§å‘Šè­¦é…ç½®

## å‚è€ƒèµ„æ–™

- [Confluent æœ€ä½³å®è·µ](https://docs.confluent.io/platform/current/kafka/deployment.html)
- [Kafka å®˜æ–¹æ–‡æ¡£](https://kafka.apache.org/documentation/)
