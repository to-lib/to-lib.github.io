---
sidebar_position: 12
title: "è¿æ¥å™¨"
description: "Flink æ•°æ®æºå’Œæ¥æ”¶å™¨è¿æ¥å™¨è¯¦è§£"
---

# Flink è¿æ¥å™¨

## æ¦‚è¿°

Flink è¿æ¥å™¨ç”¨äºä¸å¤–éƒ¨ç³»ç»Ÿè¿›è¡Œæ•°æ®äº¤äº’ï¼ŒåŒ…æ‹¬æ•°æ®æºï¼ˆSourceï¼‰å’Œæ•°æ®æ¥æ”¶å™¨ï¼ˆSinkï¼‰ã€‚

## Kafka è¿æ¥å™¨

### æ·»åŠ ä¾èµ–

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka</artifactId>
    <version>${flink.version}</version>
</dependency>
```

### Kafka Source

```java
KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("input-topic")
    .setGroupId("my-group")
    .setStartingOffsets(OffsetsInitializer.earliest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();

DataStream<String> stream = env.fromSource(
    source,
    WatermarkStrategy.noWatermarks(),
    "Kafka Source"
);
```

### Kafka Sink

```java
KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(KafkaRecordSerializationSchema.builder()
        .setTopic("output-topic")
        .setValueSerializationSchema(new SimpleStringSchema())
        .build()
    )
    .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
    .build();

stream.sinkTo(sink);
```

### Kafka SQL è¿æ¥å™¨

```sql
CREATE TABLE kafka_source (
    id STRING,
    name STRING,
    ts TIMESTAMP(3),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'input-topic',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'my-group',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json'
);

CREATE TABLE kafka_sink (
    id STRING,
    result STRING,
    ts TIMESTAMP(3)
) WITH (
    'connector' = 'kafka',
    'topic' = 'output-topic',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
);
```

## JDBC è¿æ¥å™¨

### æ·»åŠ ä¾èµ–

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-jdbc</artifactId>
    <version>3.1.0-1.17</version>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>8.0.28</version>
</dependency>
```

### JDBC Sourceï¼ˆTable APIï¼‰

```sql
CREATE TABLE mysql_source (
    id INT,
    name STRING,
    age INT,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/mydb',
    'table-name' = 'users',
    'username' = 'root',
    'password' = 'password'
);
```

### JDBC Sink

```java
stream.addSink(JdbcSink.sink(
    "INSERT INTO users (id, name, age) VALUES (?, ?, ?)",
    (statement, user) -> {
        statement.setInt(1, user.getId());
        statement.setString(2, user.getName());
        statement.setInt(3, user.getAge());
    },
    JdbcExecutionOptions.builder()
        .withBatchSize(1000)
        .withBatchIntervalMs(200)
        .withMaxRetries(5)
        .build(),
    new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
        .withUrl("jdbc:mysql://localhost:3306/mydb")
        .withDriverName("com.mysql.cj.jdbc.Driver")
        .withUsername("root")
        .withPassword("password")
        .build()
));
```

### JDBC Lookup Join

```sql
-- ç»´è¡¨å®šä¹‰
CREATE TABLE products (
    id INT,
    name STRING,
    price DECIMAL(10, 2),
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/mydb',
    'table-name' = 'products',
    'lookup.cache.max-rows' = '5000',
    'lookup.cache.ttl' = '10min'
);

-- Lookup Join
SELECT o.order_id, p.name, p.price
FROM orders AS o
JOIN products FOR SYSTEM_TIME AS OF o.proc_time AS p
  ON o.product_id = p.id;
```

## Elasticsearch è¿æ¥å™¨

### æ·»åŠ ä¾èµ–

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch7</artifactId>
    <version>${flink.version}</version>
</dependency>
```

### Elasticsearch Sink

```java
ElasticsearchSink<Event> esSink = new Elasticsearch7SinkBuilder<Event>()
    .setHosts(new HttpHost("localhost", 9200, "http"))
    .setEmitter((element, context, indexer) -> {
        indexer.add(Requests.indexRequest()
            .index("events")
            .id(element.getId())
            .source(Map.of(
                "id", element.getId(),
                "name", element.getName(),
                "timestamp", element.getTimestamp()
            ))
        );
    })
    .setBulkFlushMaxActions(1000)
    .build();

stream.sinkTo(esSink);
```

### Elasticsearch SQL

```sql
CREATE TABLE es_sink (
    id STRING,
    name STRING,
    ts TIMESTAMP(3),
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'elasticsearch-7',
    'hosts' = 'http://localhost:9200',
    'index' = 'events'
);
```

## æ–‡ä»¶ç³»ç»Ÿè¿æ¥å™¨

### æ–‡ä»¶ Source

```java
FileSource<String> source = FileSource
    .forRecordStreamFormat(
        new TextLineInputFormat(),
        new Path("hdfs:///data/input")
    )
    .monitorContinuously(Duration.ofSeconds(10))
    .build();

DataStream<String> stream = env.fromSource(
    source,
    WatermarkStrategy.noWatermarks(),
    "File Source"
);
```

### æ–‡ä»¶ Sinkï¼ˆæµå¼å†™å…¥ï¼‰

```java
FileSink<String> sink = FileSink
    .forRowFormat(
        new Path("hdfs:///data/output"),
        new SimpleStringEncoder<String>("UTF-8")
    )
    .withRollingPolicy(
        DefaultRollingPolicy.builder()
            .withRolloverInterval(Duration.ofMinutes(15))
            .withInactivityInterval(Duration.ofMinutes(5))
            .withMaxPartSize(MemorySize.ofMebiBytes(1024))
            .build()
    )
    .withBucketAssigner(new DateTimeBucketAssigner<>("yyyy-MM-dd--HH"))
    .build();

stream.sinkTo(sink);
```

### æ–‡ä»¶ç³»ç»Ÿ SQL

```sql
CREATE TABLE file_source (
    id STRING,
    name STRING,
    dt STRING
) PARTITIONED BY (dt) WITH (
    'connector' = 'filesystem',
    'path' = 'hdfs:///data/input',
    'format' = 'parquet'
);

CREATE TABLE file_sink (
    id STRING,
    name STRING,
    dt STRING
) PARTITIONED BY (dt) WITH (
    'connector' = 'filesystem',
    'path' = 'hdfs:///data/output',
    'format' = 'parquet',
    'sink.partition-commit.policy.kind' = 'success-file'
);
```

## Redis è¿æ¥å™¨

### è‡ªå®šä¹‰ Redis Sink

```java
public class RedisSink extends RichSinkFunction<Event> {
    private transient Jedis jedis;

    @Override
    public void open(Configuration parameters) {
        jedis = new Jedis("localhost", 6379);
    }

    @Override
    public void invoke(Event event, Context context) {
        jedis.hset("events", event.getId(), event.toJson());
    }

    @Override
    public void close() {
        if (jedis != null) {
            jedis.close();
        }
    }
}
```

## è¿æ¥å™¨é…ç½®æ±‡æ€»

| è¿æ¥å™¨        | ä¾èµ–                          | ç‰¹ç‚¹                 |
| ------------- | ----------------------------- | -------------------- |
| Kafka         | flink-connector-kafka         | æœ€å¸¸ç”¨ï¼Œæ”¯æŒç²¾ç¡®ä¸€æ¬¡ |
| JDBC          | flink-connector-jdbc          | æ”¯æŒå„ç§å…³ç³»å‹æ•°æ®åº“ |
| Elasticsearch | flink-connector-elasticsearch | å…¨æ–‡æœç´¢ã€æ—¥å¿—åˆ†æ   |
| Filesystem    | flink-connector-files         | HDFS/S3/æœ¬åœ°æ–‡ä»¶     |
| Pulsar        | flink-connector-pulsar        | æ–°ä¸€ä»£æ¶ˆæ¯ç³»ç»Ÿ       |
| HBase         | flink-connector-hbase         | NoSQL æ•°æ®åº“         |

## ä¸‹ä¸€æ­¥

- ğŸ“Š [Table API & SQL](/docs/flink/table-sql) - SQL è¿æ¥å™¨ä½¿ç”¨
- ğŸš€ [éƒ¨ç½²ä¸è¿ç»´](/docs/flink/deployment) - ç”Ÿäº§éƒ¨ç½²
- ğŸ“‹ [æœ€ä½³å®è·µ](/docs/flink/best-practices) - å¼€å‘è§„èŒƒ
