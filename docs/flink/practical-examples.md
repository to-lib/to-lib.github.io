---
sidebar_position: 18
title: "å®æˆ˜æ¡ˆä¾‹"
description: "Flink ç”Ÿäº§ç¯å¢ƒå®æˆ˜æ¡ˆä¾‹ä¸æœ€ä½³å®è·µ"
---

# Flink å®æˆ˜æ¡ˆä¾‹

> é€‚ç”¨ç‰ˆæœ¬ï¼šApache Flink v2.2.0

## æ¡ˆä¾‹ä¸€ï¼šå®æ—¶ç”µå•†æ•°ä»“

### ä¸šåŠ¡åœºæ™¯

æ„å»ºå®æ—¶æ•°æ®ä»“åº“ï¼Œå®ç°è®¢å•æ•°æ®çš„å®æ—¶ç»Ÿè®¡åˆ†æã€‚

```mermaid
graph LR
    A[MySQL] -->|CDC| B[Flink]
    C[Kafka è®¢å•æµ] --> B
    B --> D[Kafka ä¸­é—´å±‚]
    B --> E[ClickHouse]
    B --> F[Redis]

    style B fill:#2e7d32,color:#fff
```

### æ ¸å¿ƒå®ç°

#### æ•°æ®æºå®šä¹‰

```sql
-- è®¢å•äº‹å®è¡¨ï¼ˆCDCï¼‰
CREATE TABLE orders (
    order_id STRING,
    user_id STRING,
    product_id STRING,
    amount DECIMAL(10, 2),
    status STRING,
    create_time TIMESTAMP(3),
    update_time TIMESTAMP(3),
    WATERMARK FOR create_time AS create_time - INTERVAL '5' SECOND,
    PRIMARY KEY (order_id) NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'mysql-host',
    'port' = '3306',
    'username' = 'flink',
    'password' = 'password',
    'database-name' = 'ecommerce',
    'table-name' = 'orders'
);

-- å•†å“ç»´è¡¨
CREATE TABLE products (
    product_id STRING,
    product_name STRING,
    category STRING,
    price DECIMAL(10, 2),
    PRIMARY KEY (product_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://mysql-host:3306/ecommerce',
    'table-name' = 'products',
    'lookup.cache.max-rows' = '5000',
    'lookup.cache.ttl' = '10min'
);
```

#### å®æ—¶èšåˆ

```sql
-- æ¯å°æ—¶é”€å”®ç»Ÿè®¡
INSERT INTO hourly_sales
SELECT
    DATE_FORMAT(create_time, 'yyyy-MM-dd HH:00:00') AS hour_time,
    p.category,
    COUNT(*) AS order_count,
    SUM(o.amount) AS total_amount,
    COUNT(DISTINCT o.user_id) AS user_count
FROM orders o
JOIN products FOR SYSTEM_TIME AS OF o.create_time AS p
    ON o.product_id = p.product_id
WHERE o.status = 'PAID'
GROUP BY
    DATE_FORMAT(create_time, 'yyyy-MM-dd HH:00:00'),
    p.category;
```

---

## æ¡ˆä¾‹äºŒï¼šå®æ—¶é£æ§ç³»ç»Ÿ

### ä¸šåŠ¡åœºæ™¯

æ£€æµ‹å¼‚å¸¸äº¤æ˜“è¡Œä¸ºï¼ŒåŒ…æ‹¬ï¼š

- çŸ­æ—¶é—´å†…å¤šæ¬¡äº¤æ˜“
- å¼‚åœ°ç™»å½•åç«‹å³äº¤æ˜“
- å¤§é¢å¼‚å¸¸äº¤æ˜“

### æ ¸å¿ƒå®ç°

#### ä½¿ç”¨ CEP æ£€æµ‹å¼‚å¸¸æ¨¡å¼

```java
// å®šä¹‰å¼‚å¸¸äº¤æ˜“æ¨¡å¼ï¼š5åˆ†é’Ÿå†…è¶…è¿‡3æ¬¡äº¤æ˜“
Pattern<Transaction, ?> pattern = Pattern.<Transaction>begin("first")
    .where(SimpleCondition.of(t -> t.getAmount() > 1000))
    .followedBy("second")
    .where(SimpleCondition.of(t -> t.getAmount() > 1000))
    .followedBy("third")
    .where(SimpleCondition.of(t -> t.getAmount() > 1000))
    .within(Time.minutes(5));

// åº”ç”¨æ¨¡å¼
PatternStream<Transaction> patternStream = CEP.pattern(
    transactions.keyBy(Transaction::getUserId),
    pattern
);

// ç”Ÿæˆå‘Šè­¦
DataStream<RiskAlert> alerts = patternStream.process(
    new PatternProcessFunction<Transaction, RiskAlert>() {
        @Override
        public void processMatch(Map<String, List<Transaction>> match,
                Context ctx, Collector<RiskAlert> out) {
            List<Transaction> txns = new ArrayList<>();
            txns.addAll(match.get("first"));
            txns.addAll(match.get("second"));
            txns.addAll(match.get("third"));

            double totalAmount = txns.stream()
                .mapToDouble(Transaction::getAmount)
                .sum();

            out.collect(new RiskAlert(
                txns.get(0).getUserId(),
                "FREQUENT_LARGE_TRANSACTION",
                totalAmount,
                txns.size()
            ));
        }
    }
);
```

#### å¼‚åœ°ç™»å½•æ£€æµ‹

```java
public class LocationCheckFunction
        extends KeyedProcessFunction<String, LoginEvent, RiskAlert> {

    private ValueState<String> lastLocationState;
    private ValueState<Long> lastLoginTimeState;

    @Override
    public void open(Configuration parameters) {
        lastLocationState = getRuntimeContext().getState(
            new ValueStateDescriptor<>("lastLocation", String.class));
        lastLoginTimeState = getRuntimeContext().getState(
            new ValueStateDescriptor<>("lastLoginTime", Long.class));
    }

    @Override
    public void processElement(LoginEvent event, Context ctx,
            Collector<RiskAlert> out) throws Exception {
        String lastLocation = lastLocationState.value();
        Long lastLoginTime = lastLoginTimeState.value();

        if (lastLocation != null && lastLoginTime != null) {
            // è®¡ç®—è·ç¦»å’Œæ—¶é—´å·®
            double distance = calculateDistance(lastLocation, event.getLocation());
            long timeDiff = event.getTimestamp() - lastLoginTime;

            // ä¸å¯èƒ½åœ¨çŸ­æ—¶é—´å†…è·¨è¶Šå¤§è·ç¦»
            if (distance > 500 && timeDiff < 3600000) { // 500km, 1å°æ—¶
                out.collect(new RiskAlert(
                    event.getUserId(),
                    "IMPOSSIBLE_TRAVEL",
                    distance,
                    timeDiff
                ));
            }
        }

        lastLocationState.update(event.getLocation());
        lastLoginTimeState.update(event.getTimestamp());
    }
}
```

---

## æ¡ˆä¾‹ä¸‰ï¼šå®æ—¶æ¨èç³»ç»Ÿ

### ä¸šåŠ¡åœºæ™¯

åŸºäºç”¨æˆ·å®æ—¶è¡Œä¸ºï¼Œç”Ÿæˆä¸ªæ€§åŒ–æ¨èã€‚

### æ¶æ„è®¾è®¡

```mermaid
graph TB
    A[ç”¨æˆ·è¡Œä¸ºæµ] --> B[Flink å®æ—¶å¤„ç†]
    B --> C[ç”¨æˆ·ç”»åƒæ›´æ–°]
    B --> D[å®æ—¶ç‰¹å¾è®¡ç®—]
    D --> E[Redis ç‰¹å¾å­˜å‚¨]
    C --> F[HBase ç”»åƒå­˜å‚¨]
    E --> G[æ¨èæœåŠ¡]
    F --> G

    style B fill:#1976d2,color:#fff
```

### å®æ—¶ç‰¹å¾è®¡ç®—

```java
public class UserFeatureFunction
        extends KeyedProcessFunction<String, UserAction, UserFeature> {

    // æœ€è¿‘æµè§ˆçš„å•†å“ç±»ç›®ï¼ˆä¿ç•™æœ€è¿‘10ä¸ªï¼‰
    private ListState<String> recentCategoriesState;
    // å„ç±»ç›®æµè§ˆæ¬¡æ•°
    private MapState<String, Long> categoryCountState;
    // ç”¨æˆ·æ´»è·ƒåº¦å¾—åˆ†
    private ValueState<Double> activityScoreState;

    @Override
    public void open(Configuration parameters) {
        recentCategoriesState = getRuntimeContext().getListState(
            new ListStateDescriptor<>("recentCategories", String.class));
        categoryCountState = getRuntimeContext().getMapState(
            new MapStateDescriptor<>("categoryCount", String.class, Long.class));
        activityScoreState = getRuntimeContext().getState(
            new ValueStateDescriptor<>("activityScore", Double.class));
    }

    @Override
    public void processElement(UserAction action, Context ctx,
            Collector<UserFeature> out) throws Exception {
        String category = action.getCategory();

        // æ›´æ–°ç±»ç›®è®¡æ•°
        Long count = categoryCountState.get(category);
        categoryCountState.put(category, count == null ? 1L : count + 1);

        // æ›´æ–°æœ€è¿‘æµè§ˆ
        List<String> recent = new ArrayList<>();
        recentCategoriesState.get().forEach(recent::add);
        recent.add(category);
        if (recent.size() > 10) {
            recent = recent.subList(recent.size() - 10, recent.size());
        }
        recentCategoriesState.update(recent);

        // è®¡ç®—æ´»è·ƒåº¦
        Double score = activityScoreState.value();
        score = (score == null ? 0 : score * 0.9) + 1.0;
        activityScoreState.update(score);

        // è¾“å‡ºç‰¹å¾
        out.collect(new UserFeature(
            action.getUserId(),
            recent,
            getTopCategories(categoryCountState, 5),
            score
        ));
    }
}
```

---

## æ¡ˆä¾‹å››ï¼šæ—¥å¿—å®æ—¶åˆ†æ

### ä¸šåŠ¡åœºæ™¯

å®æ—¶åˆ†æåº”ç”¨æ—¥å¿—ï¼Œç›‘æ§å¼‚å¸¸å’Œæ€§èƒ½æŒ‡æ ‡ã€‚

### SQL å®ç°

```sql
-- æ—¥å¿—æºè¡¨
CREATE TABLE app_logs (
    log_time TIMESTAMP(3),
    level STRING,
    service STRING,
    message STRING,
    trace_id STRING,
    duration_ms BIGINT,
    WATERMARK FOR log_time AS log_time - INTERVAL '10' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'app-logs',
    'properties.bootstrap.servers' = 'kafka:9092',
    'format' = 'json'
);

-- æ¯åˆ†é’Ÿé”™è¯¯ç»Ÿè®¡
SELECT
    TUMBLE_START(log_time, INTERVAL '1' MINUTE) AS window_start,
    service,
    COUNT(*) FILTER (WHERE level = 'ERROR') AS error_count,
    COUNT(*) FILTER (WHERE level = 'WARN') AS warn_count,
    COUNT(*) AS total_count
FROM app_logs
GROUP BY TUMBLE(log_time, INTERVAL '1' MINUTE), service
HAVING COUNT(*) FILTER (WHERE level = 'ERROR') > 10;

-- P99 å»¶è¿Ÿç›‘æ§
SELECT
    TUMBLE_START(log_time, INTERVAL '1' MINUTE) AS window_start,
    service,
    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY duration_ms) AS p99_latency,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95_latency,
    AVG(duration_ms) AS avg_latency
FROM app_logs
WHERE duration_ms IS NOT NULL
GROUP BY TUMBLE(log_time, INTERVAL '1' MINUTE), service;
```

---

## æ¡ˆä¾‹äº”ï¼šIoT æ•°æ®å¤„ç†

### ä¸šåŠ¡åœºæ™¯

å¤„ç†æµ·é‡ä¼ æ„Ÿå™¨æ•°æ®ï¼Œå®æ—¶ç›‘æ§è®¾å¤‡çŠ¶æ€ã€‚

### å®ç°ä»£ç 

```java
// ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†
DataStream<SensorReading> readings = env
    .addSource(new FlinkKafkaConsumer<>(
        "sensor-data",
        new SensorReadingSchema(),
        kafkaProps))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy
            .<SensorReading>forBoundedOutOfOrderness(Duration.ofSeconds(5))
            .withTimestampAssigner((r, ts) -> r.getTimestamp()));

// è®¾å¤‡çŠ¶æ€ç›‘æ§
readings
    .keyBy(SensorReading::getDeviceId)
    .window(SlidingEventTimeWindows.of(Time.minutes(5), Time.minutes(1)))
    .aggregate(new SensorStatsAggregate())
    .filter(stats -> stats.getMaxTemperature() > 80
                  || stats.getAvgTemperature() > 60)
    .addSink(new AlertSink());

// èšåˆå‡½æ•°
public class SensorStatsAggregate
        implements AggregateFunction<SensorReading, SensorStats, SensorStats> {

    @Override
    public SensorStats createAccumulator() {
        return new SensorStats();
    }

    @Override
    public SensorStats add(SensorReading reading, SensorStats acc) {
        acc.count++;
        acc.sumTemperature += reading.getTemperature();
        acc.maxTemperature = Math.max(acc.maxTemperature, reading.getTemperature());
        acc.minTemperature = Math.min(acc.minTemperature, reading.getTemperature());
        return acc;
    }

    @Override
    public SensorStats getResult(SensorStats acc) {
        acc.avgTemperature = acc.sumTemperature / acc.count;
        return acc;
    }

    @Override
    public SensorStats merge(SensorStats a, SensorStats b) {
        a.count += b.count;
        a.sumTemperature += b.sumTemperature;
        a.maxTemperature = Math.max(a.maxTemperature, b.maxTemperature);
        a.minTemperature = Math.min(a.minTemperature, b.minTemperature);
        return a;
    }
}
```

---

## ç”Ÿäº§ç¯å¢ƒé…ç½®å»ºè®®

### èµ„æºé…ç½®

```yaml
# é«˜åååœºæ™¯
taskmanager.memory.process.size: 16g
taskmanager.numberOfTaskSlots: 4
parallelism.default: 16

# å¤§çŠ¶æ€åœºæ™¯
state.backend: rocksdb
state.backend.incremental: true
state.backend.rocksdb.memory.managed: true
```

### æ£€æŸ¥ç‚¹é…ç½®

```java
env.enableCheckpointing(60000);
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
env.getCheckpointConfig().setMinPauseBetweenCheckpoints(30000);
env.getCheckpointConfig().setCheckpointTimeout(600000);
env.getCheckpointConfig().enableUnalignedCheckpoints();
```

## ä¸‹ä¸€æ­¥

- ğŸ”Œ [è¿æ¥å™¨](/docs/flink/connectors) - æ›´å¤šæ•°æ®æºé…ç½®
- ğŸ“Š [Flink CDC](/docs/flink/flink-cdc) - CDC è¯¦ç»†ä½¿ç”¨
- ğŸš€ [æ€§èƒ½ä¼˜åŒ–](/docs/flink/performance-optimization) - è°ƒä¼˜æŒ‡å—
