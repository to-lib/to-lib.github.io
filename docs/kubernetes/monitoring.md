---
sidebar_position: 13
title: ç›‘æ§ä¸å‘Šè­¦
description: Kubernetes é›†ç¾¤ç›‘æ§ä¸å¯è§‚æµ‹æ€§
---

# ç›‘æ§ä¸å‘Šè­¦

## å¯è§‚æµ‹æ€§æ¦‚è¿°

Kubernetes å¯è§‚æµ‹æ€§åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼š

```mermaid
graph TB
    subgraph "å¯è§‚æµ‹æ€§"
        M[ğŸ“Š Metrics<br/>æŒ‡æ ‡]
        L[ğŸ“ Logs<br/>æ—¥å¿—]
        T[ğŸ” Traces<br/>é“¾è·¯è¿½è¸ª]
    end

    subgraph "å·¥å…·æ ˆ"
        M --> P[Prometheus]
        M --> G[Grafana]
        L --> E[Elasticsearch]
        L --> F[Fluentd]
        L --> K[Kibana]
        T --> J[Jaeger]
        T --> Z[Zipkin]
    end
```

## Metrics Server

Metrics Server æä¾›é›†ç¾¤èµ„æºä½¿ç”¨æ•°æ®ï¼Œæ˜¯ HPA/VPA çš„åŸºç¡€ã€‚

### å®‰è£… Metrics Server

```bash
# å®‰è£…
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# å¦‚æœæ˜¯æœ¬åœ°é›†ç¾¤ï¼ˆè‡ªç­¾åè¯ä¹¦ï¼‰ï¼Œéœ€æ·»åŠ å‚æ•°
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[
  {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}
]'

# éªŒè¯
kubectl top nodes
kubectl top pods -A
```

### èµ„æºæŸ¥çœ‹

```bash
# æŸ¥çœ‹èŠ‚ç‚¹èµ„æº
kubectl top nodes
# NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# node-1   250m         12%    1024Mi          25%

# æŸ¥çœ‹ Pod èµ„æº
kubectl top pods -n default
kubectl top pods --containers  # æŒ‰å®¹å™¨æ˜¾ç¤º

# æŒ‰èµ„æºæ’åº
kubectl top pods --sort-by=cpu
kubectl top pods --sort-by=memory
```

## Prometheus ç›‘æ§

### ä½¿ç”¨ Helm å®‰è£… kube-prometheus-stack

```bash
# æ·»åŠ  Helm ä»“åº“
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# å®‰è£… kube-prometheus-stackï¼ˆåŒ…å« Prometheusã€Grafanaã€Alertmanagerï¼‰
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin123

# æŸ¥çœ‹å®‰è£…çš„ç»„ä»¶
kubectl get pods -n monitoring
```

### è®¿é—® Prometheus UI

```bash
# ç«¯å£è½¬å‘
kubectl port-forward svc/prometheus-kube-prometheus-prometheus -n monitoring 9090:9090

# è®¿é—® http://localhost:9090
```

### å¸¸ç”¨ PromQL æŸ¥è¯¢

```promql
# CPU ä½¿ç”¨ç‡ï¼ˆæŒ‰èŠ‚ç‚¹ï¼‰
100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# å†…å­˜ä½¿ç”¨ç‡ï¼ˆæŒ‰èŠ‚ç‚¹ï¼‰
(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100

# Pod CPU ä½¿ç”¨ç‡
sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (namespace, pod)

# Pod å†…å­˜ä½¿ç”¨
sum(container_memory_working_set_bytes{container!=""}) by (namespace, pod)

# Pod é‡å¯æ¬¡æ•°
sum(kube_pod_container_status_restarts_total) by (namespace, pod)

# API Server è¯·æ±‚å»¶è¿Ÿ
histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le, verb))
```

### è‡ªå®šä¹‰ ServiceMonitor

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app-monitor
  namespace: monitoring
  labels:
    release: prometheus # åŒ¹é… Prometheus Operator çš„ selector
spec:
  selector:
    matchLabels:
      app: my-app
  namespaceSelector:
    matchNames:
      - production
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s
```

## Grafana å¯è§†åŒ–

### è®¿é—® Grafana

```bash
# è·å– Grafana å¯†ç 
kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d

# ç«¯å£è½¬å‘
kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80

# è®¿é—® http://localhost:3000
# ç”¨æˆ·å: admin
```

### æ¨è Dashboard

| Dashboard ID | åç§°                          | ç”¨é€”            |
| ------------ | ----------------------------- | --------------- |
| 315          | Kubernetes cluster monitoring | é›†ç¾¤æ¦‚è§ˆ        |
| 13770        | Kubernetes All-in-One         | å…¨é¢ç›‘æ§        |
| 6417         | Kubernetes Pod Resources      | Pod èµ„æº        |
| 11074        | Node Exporter                 | èŠ‚ç‚¹ç›‘æ§        |
| 747          | Kubernetes Deployments        | Deployment ç›‘æ§ |

### å¯¼å…¥ Dashboard

1. è®¿é—® Grafana UI
2. å·¦ä¾§èœå• â†’ Dashboards â†’ Import
3. è¾“å…¥ Dashboard ID æˆ–ä¸Šä¼  JSON
4. é€‰æ‹©æ•°æ®æºï¼ˆPrometheusï¼‰
5. ç‚¹å‡» Import

## Alertmanager å‘Šè­¦

### å‘Šè­¦è§„åˆ™ç¤ºä¾‹

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-alerts
  namespace: monitoring
  labels:
    release: prometheus
spec:
  groups:
    - name: kubernetes-apps
      rules:
        # Pod CrashLoopBackOff å‘Šè­¦
        - alert: PodCrashLoopBackOff
          expr: |
            max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}[5m]) >= 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} CrashLoopBackOff"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is in CrashLoopBackOff state."

        # Pod å†…å­˜ä½¿ç”¨è¶…è¿‡ 80%
        - alert: PodHighMemoryUsage
          expr: |
            (container_memory_working_set_bytes / container_spec_memory_limit_bytes) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} memory usage > 80%"

        # èŠ‚ç‚¹ NotReady
        - alert: NodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Node {{ $labels.node }} is NotReady"
```

### é…ç½®å‘Šè­¦é€šçŸ¥

```yaml
# åˆ›å»º Alertmanager é…ç½®çš„ Secret
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-prometheus-kube-prometheus-alertmanager
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/xxx'

    route:
      group_by: ['alertname', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'default'
      routes:
        - match:
            severity: critical
          receiver: 'slack-critical'

    receivers:
      - name: 'default'
        webhook_configs:
          - url: 'http://alertmanager-webhook:8080/webhook'

      - name: 'slack-critical'
        slack_configs:
          - channel: '#alerts-critical'
            send_resolved: true
            title: '{{ .Status | toUpper }}: {{ .CommonAnnotations.summary }}'
            text: '{{ .CommonAnnotations.description }}'
```

## HPA è‡ªåŠ¨æ‰©ç¼©å®¹

### åŸºäº CPU çš„ HPA

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
```

### åŸºäºè‡ªå®šä¹‰æŒ‡æ ‡çš„ HPA

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 20
  metrics:
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: 1000
```

### HPA æ“ä½œ

```bash
# åˆ›å»º HPA
kubectl apply -f hpa.yaml

# æŸ¥çœ‹ HPA
kubectl get hpa
kubectl describe hpa my-app-hpa

# å‘½ä»¤è¡Œåˆ›å»º HPA
kubectl autoscale deployment my-app --cpu-percent=70 --min=2 --max=10

# æŸ¥çœ‹æ‰©ç¼©å®¹äº‹ä»¶
kubectl get events --field-selector reason=SuccessfulRescale
```

## æ—¥å¿—æ”¶é›†

### ä½¿ç”¨ Fluent Bit

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         5
        Log_Level     info
        Daemon        off

    [INPUT]
        Name              tail
        Tag               kube.*
        Path              /var/log/containers/*.log
        Parser            docker
        DB                /var/log/flb_kube.db
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  10

    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Merge_Log           On
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On

    [OUTPUT]
        Name            es
        Match           *
        Host            elasticsearch
        Port            9200
        Index           kubernetes
        Type            _doc
```

### æ—¥å¿—æŸ¥çœ‹å‘½ä»¤

```bash
# æŸ¥çœ‹ Pod æ—¥å¿—
kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>  # å¤šå®¹å™¨
kubectl logs <pod-name> --previous           # ä¸Šä¸€ä¸ªå®¹å™¨å®ä¾‹
kubectl logs <pod-name> -f                   # å®æ—¶æ—¥å¿—
kubectl logs <pod-name> --tail=100          # æœ€å 100 è¡Œ
kubectl logs <pod-name> --since=1h          # æœ€è¿‘ 1 å°æ—¶

# æŸ¥çœ‹å¤šä¸ª Pod æ—¥å¿—
kubectl logs -l app=my-app --all-containers

# ä½¿ç”¨ stern å·¥å…·
stern my-app -n production
```

## å¸¸ç”¨æ“ä½œ

```bash
# èµ„æºç›‘æ§
kubectl top nodes
kubectl top pods -A

# äº‹ä»¶æŸ¥çœ‹
kubectl get events -A --sort-by='.lastTimestamp'
kubectl get events -w  # å®æ—¶äº‹ä»¶

# ç»„ä»¶å¥åº·æ£€æŸ¥
kubectl get componentstatuses
kubectl get --raw /healthz
kubectl get --raw /readyz

# API Server æŒ‡æ ‡
kubectl get --raw /metrics | head -100
```
