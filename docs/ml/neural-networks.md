---
sidebar_position: 7
title: ğŸ”— ç¥ç»ç½‘ç»œåŸºç¡€
---

# ç¥ç»ç½‘ç»œåŸºç¡€

ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºçŸ³ï¼Œé€šè¿‡æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼æ¥å­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚

## ä»æ„ŸçŸ¥æœºåˆ°ç¥ç»ç½‘ç»œ

### æ„ŸçŸ¥æœº (Perceptron)

æœ€ç®€å•çš„ç¥ç»ç½‘ç»œå•å…ƒï¼Œå®ç°äºŒåˆ†ç±»ã€‚

$$
y = \sigma(w_1 x_1 + w_2 x_2 + \ldots + w_n x_n + b) = \sigma(\mathbf{w}^T \mathbf{x} + b)
$$

```python
import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.01, n_iters=1000):
        self.lr = learning_rate
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        # ç¡®ä¿æ ‡ç­¾ä¸º 0 æˆ– 1
        y_ = np.where(y > 0, 1, 0)

        for _ in range(self.n_iters):
            for idx, x_i in enumerate(X):
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_pred = 1 if linear_output >= 0 else 0

                # æ›´æ–°è§„åˆ™
                update = self.lr * (y_[idx] - y_pred)
                self.weights += update * x_i
                self.bias += update

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return np.where(linear_output >= 0, 1, 0)
```

### å¤šå±‚æ„ŸçŸ¥æœº (MLP)

é€šè¿‡å †å å¤šä¸ªå±‚æ¥å­¦ä¹ éçº¿æ€§å…³ç³»ã€‚

```mermaid
graph LR
    subgraph è¾“å…¥å±‚
        x1((xâ‚))
        x2((xâ‚‚))
        x3((xâ‚ƒ))
    end

    subgraph éšè—å±‚
        h1((hâ‚))
        h2((hâ‚‚))
        h3((hâ‚ƒ))
        h4((hâ‚„))
    end

    subgraph è¾“å‡ºå±‚
        y1((yâ‚))
        y2((yâ‚‚))
    end

    x1 --> h1 & h2 & h3 & h4
    x2 --> h1 & h2 & h3 & h4
    x3 --> h1 & h2 & h3 & h4
    h1 & h2 & h3 & h4 --> y1 & y2
```

## æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°ä¸ºç¥ç»ç½‘ç»œå¼•å…¥éçº¿æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚

| æ¿€æ´»å‡½æ•°   | å…¬å¼                            | ç‰¹ç‚¹                 | é€‚ç”¨åœºæ™¯         |
| ---------- | ------------------------------- | -------------------- | ---------------- |
| Sigmoid    | $\frac{1}{1+e^{-x}}$            | è¾“å‡º (0,1)ï¼Œæ¢¯åº¦æ¶ˆå¤± | äºŒåˆ†ç±»è¾“å‡ºå±‚     |
| Tanh       | $\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | è¾“å‡º (-1,1)ï¼Œé›¶ä¸­å¿ƒ  | éšè—å±‚ï¼ˆè¾ƒå°‘ç”¨ï¼‰ |
| ReLU       | $\max(0, x)$                    | ç®€å•é«˜æ•ˆï¼Œå¯èƒ½æ­»äº¡   | éšè—å±‚é¦–é€‰       |
| Leaky ReLU | $\max(0.01x, x)$                | è§£å†³ ReLU æ­»äº¡é—®é¢˜   | éšè—å±‚           |
| Softmax    | $\frac{e^{x_i}}{\sum e^{x_j}}$  | è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ         | å¤šåˆ†ç±»è¾“å‡ºå±‚     |

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # æ•°å€¼ç¨³å®šæ€§
    return exp_x / exp_x.sum()
```

```python
# å¯è§†åŒ–æ¿€æ´»å‡½æ•°
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 100)

fig, axes = plt.subplots(2, 2, figsize=(10, 8))

axes[0, 0].plot(x, sigmoid(x))
axes[0, 0].set_title('Sigmoid')
axes[0, 0].grid(True)

axes[0, 1].plot(x, tanh(x))
axes[0, 1].set_title('Tanh')
axes[0, 1].grid(True)

axes[1, 0].plot(x, relu(x))
axes[1, 0].set_title('ReLU')
axes[1, 0].grid(True)

axes[1, 1].plot(x, leaky_relu(x))
axes[1, 1].set_title('Leaky ReLU')
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
```

## å‰å‘ä¼ æ’­

æ•°æ®ä»è¾“å…¥å±‚ç»è¿‡éšè—å±‚åˆ°è¾¾è¾“å‡ºå±‚çš„è¿‡ç¨‹ã€‚

$$
\mathbf{z}^{[l]} = \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}
$$

$$
\mathbf{a}^{[l]} = g^{[l]}(\mathbf{z}^{[l]})
$$

```python
class NeuralNetwork:
    def __init__(self, layer_sizes):
        """
        layer_sizes: ä¾‹å¦‚ [784, 128, 64, 10] è¡¨ç¤ºè¾“å…¥784ï¼Œä¸¤ä¸ªéšè—å±‚ï¼Œè¾“å‡º10ç±»
        """
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        self.weights = []
        self.biases = []

        # åˆå§‹åŒ–æƒé‡ï¼ˆXavier åˆå§‹åŒ–ï¼‰
        for i in range(1, self.num_layers):
            w = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2 / layer_sizes[i-1])
            b = np.zeros((layer_sizes[i], 1))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations = [X]
        self.z_values = []

        a = X
        for i in range(self.num_layers - 1):
            z = np.dot(self.weights[i], a) + self.biases[i]
            self.z_values.append(z)

            # æœ€åä¸€å±‚ç”¨ softmaxï¼Œå…¶ä»–ç”¨ ReLU
            if i == self.num_layers - 2:
                a = self._softmax(z)
            else:
                a = self._relu(z)

            self.activations.append(a)

        return a

    def _relu(self, z):
        return np.maximum(0, z)

    def _softmax(self, z):
        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))
        return exp_z / np.sum(exp_z, axis=0, keepdims=True)
```

## åå‘ä¼ æ’­

é€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

```mermaid
graph RL
    L[æŸå¤± L] --> dA[âˆ‚L/âˆ‚a]
    dA --> dZ[âˆ‚L/âˆ‚z]
    dZ --> dW[âˆ‚L/âˆ‚W]
    dZ --> db[âˆ‚L/âˆ‚b]
    dZ --> dA_prev[âˆ‚L/âˆ‚a_prev]
```

$$
\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial z^{[l]}} \cdot (a^{[l-1]})^T
$$

$$
\frac{\partial L}{\partial b^{[l]}} = \frac{\partial L}{\partial z^{[l]}}
$$

```python
def backward(self, X, y):
    """åå‘ä¼ æ’­"""
    m = X.shape[1]
    gradients = {'dW': [], 'db': []}

    # è¾“å‡ºå±‚è¯¯å·®
    dz = self.activations[-1] - y

    for i in range(self.num_layers - 2, -1, -1):
        dw = (1/m) * np.dot(dz, self.activations[i].T)
        db = (1/m) * np.sum(dz, axis=1, keepdims=True)

        gradients['dW'].insert(0, dw)
        gradients['db'].insert(0, db)

        if i > 0:
            da = np.dot(self.weights[i].T, dz)
            dz = da * (self.z_values[i-1] > 0)  # ReLU å¯¼æ•°

    return gradients

def update_params(self, gradients, learning_rate):
    """æ›´æ–°å‚æ•°"""
    for i in range(len(self.weights)):
        self.weights[i] -= learning_rate * gradients['dW'][i]
        self.biases[i] -= learning_rate * gradients['db'][i]
```

## æŸå¤±å‡½æ•°

| æŸå¤±å‡½æ•°             | å…¬å¼                                 | é€‚ç”¨åœºæ™¯ |
| -------------------- | ------------------------------------ | -------- |
| MSE                  | $\frac{1}{n}\sum(y-\hat{y})^2$       | å›å½’     |
| Cross-Entropy        | $-\sum y_i \log \hat{y}_i$           | åˆ†ç±»     |
| Binary Cross-Entropy | $-y\log\hat{y}-(1-y)\log(1-\hat{y})$ | äºŒåˆ†ç±»   |

```python
def cross_entropy_loss(y_true, y_pred, epsilon=1e-15):
    """äº¤å‰ç†µæŸå¤±"""
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=0))
```

## æƒé‡åˆå§‹åŒ–

| æ–¹æ³•   | å…¬å¼                               | é€‚ç”¨æ¿€æ´»å‡½æ•°  |
| ------ | ---------------------------------- | ------------- |
| Xavier | $\mathcal{N}(0, \frac{1}{n_{in}})$ | Sigmoid, Tanh |
| He     | $\mathcal{N}(0, \frac{2}{n_{in}})$ | ReLU          |

```python
# Xavier åˆå§‹åŒ–
w = np.random.randn(n_out, n_in) * np.sqrt(1 / n_in)

# He åˆå§‹åŒ–
w = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in)
```

## ä½¿ç”¨ PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# å®šä¹‰ç½‘ç»œ
class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, num_classes):
        super(MLP, self).__init__()
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.2))
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, num_classes))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# åˆ›å»ºæ¨¡å‹
model = MLP(input_size=784, hidden_sizes=[256, 128], num_classes=10)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå¾ªç¯
def train(model, train_loader, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}")
```

## ä½¿ç”¨ sklearn å¿«é€Ÿå®ç°

```python
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

# åŠ è½½æ•°æ®
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(
    digits.data, digits.target, test_size=0.2, random_state=42
)

# åˆ›å»ºå¹¶è®­ç»ƒ MLP
mlp = MLPClassifier(
    hidden_layer_sizes=(128, 64),
    activation='relu',
    solver='adam',
    learning_rate_init=0.001,
    max_iter=500,
    random_state=42
)

mlp.fit(X_train, y_train)
print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {mlp.score(X_train, y_train):.2%}")
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {mlp.score(X_test, y_test):.2%}")
```

## å¸¸è§é—®é¢˜ä¸è§£å†³

| é—®é¢˜     | åŸå›                     | è§£å†³æ–¹æ¡ˆ                |
| -------- | ----------------------- | ----------------------- |
| æ¢¯åº¦æ¶ˆå¤± | Sigmoid/Tanh åœ¨æ·±å±‚ç½‘ç»œ | ä½¿ç”¨ ReLUï¼Œæ®‹å·®è¿æ¥     |
| æ¢¯åº¦çˆ†ç‚¸ | æƒé‡è¿‡å¤§ï¼Œå­¦ä¹ ç‡è¿‡é«˜    | æ¢¯åº¦è£å‰ªï¼Œæƒé‡åˆå§‹åŒ–    |
| è¿‡æ‹Ÿåˆ   | æ¨¡å‹è¿‡äºå¤æ‚            | Dropoutï¼Œæ­£åˆ™åŒ–ï¼Œæ—©åœ   |
| è®­ç»ƒç¼“æ…¢ | å­¦ä¹ ç‡ä¸å½“              | å­¦ä¹ ç‡è°ƒåº¦ï¼ŒAdam ä¼˜åŒ–å™¨ |
