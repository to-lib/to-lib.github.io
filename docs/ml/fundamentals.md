---
sidebar_position: 2
title: ğŸ¯ åŸºç¡€æ¦‚å¿µ
---

# æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ

## ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ 

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜ç¡®çš„ç¼–ç¨‹ã€‚

```python
# ä¼ ç»Ÿç¼–ç¨‹ vs æœºå™¨å­¦ä¹ 
# ä¼ ç»Ÿç¼–ç¨‹ï¼šè§„åˆ™ + æ•°æ® â†’ ç»“æœ
# æœºå™¨å­¦ä¹ ï¼šæ•°æ® + ç»“æœ â†’ è§„åˆ™ï¼ˆæ¨¡å‹ï¼‰

from sklearn.linear_model import LinearRegression

# æ•°æ®å’Œç»“æœ
X = [[1], [2], [3], [4]]  # ç‰¹å¾
y = [2, 4, 6, 8]          # æ ‡ç­¾

# æœºå™¨å­¦ä¹ ï¼šä»æ•°æ®ä¸­å­¦ä¹ è§„åˆ™
model = LinearRegression()
model.fit(X, y)

# æ¨¡å‹å­¦åˆ°çš„è§„åˆ™ï¼šy = 2x
print(f"ç³»æ•°: {model.coef_[0]}, æˆªè·: {model.intercept_}")
```

## æœºå™¨å­¦ä¹ çš„ä¸‰å¤§èŒƒå¼

### 1. ç›‘ç£å­¦ä¹  (Supervised Learning)

ä»**å¸¦æ ‡ç­¾**çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œç”¨äºé¢„æµ‹æ–°æ•°æ®çš„æ ‡ç­¾ã€‚

```mermaid
graph LR
    A[è¾“å…¥æ•°æ® X] --> B[æ¨¡å‹]
    C[æ ‡ç­¾ Y] --> B
    B --> D[å­¦ä¹ åˆ°çš„å‡½æ•° f]
    E[æ–°æ•°æ® X'] --> D
    D --> F[é¢„æµ‹ Y']
```

**å¸¸è§ä»»åŠ¡**ï¼š

| ä»»åŠ¡ç±»å‹ | æè¿°         | ç¤ºä¾‹                   | å¸¸ç”¨ç®—æ³•                |
| -------- | ------------ | ---------------------- | ----------------------- |
| å›å½’     | é¢„æµ‹è¿ç»­å€¼   | æˆ¿ä»·é¢„æµ‹ã€é”€é‡é¢„æµ‹     | çº¿æ€§å›å½’ã€å†³ç­–æ ‘å›å½’    |
| åˆ†ç±»     | é¢„æµ‹ç¦»æ•£ç±»åˆ« | åƒåœ¾é‚®ä»¶æ£€æµ‹ã€å›¾åƒåˆ†ç±» | é€»è¾‘å›å½’ã€SVMã€éšæœºæ£®æ— |

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# åŠ è½½æ•°æ®
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# è®­ç»ƒåˆ†ç±»æ¨¡å‹
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)

# é¢„æµ‹
accuracy = clf.score(X_test, y_test)
print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
```

### 2. æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)

ä»**æ— æ ‡ç­¾**çš„æ•°æ®ä¸­å‘ç°éšè—çš„ç»“æ„å’Œæ¨¡å¼ã€‚

```mermaid
graph LR
    A[è¾“å…¥æ•°æ® X] --> B[æ¨¡å‹]
    B --> C[å‘ç°çš„ç»“æ„/æ¨¡å¼]
```

**å¸¸è§ä»»åŠ¡**ï¼š

| ä»»åŠ¡ç±»å‹ | æè¿°             | ç¤ºä¾‹                 | å¸¸ç”¨ç®—æ³•        |
| -------- | ---------------- | -------------------- | --------------- |
| èšç±»     | å°†ç›¸ä¼¼æ•°æ®åˆ†ç»„   | å®¢æˆ·åˆ†ç¾¤ã€æ–‡æ¡£èšç±»   | K-Means, DBSCAN |
| é™ç»´     | å‡å°‘ç‰¹å¾ç»´åº¦     | æ•°æ®å¯è§†åŒ–ã€ç‰¹å¾å‹ç¼© | PCA, t-SNE      |
| å…³è”è§„åˆ™ | å‘ç°æ•°æ®é—´çš„å…³è” | è´­ç‰©ç¯®åˆ†æ           | Apriori         |

```python
from sklearn.cluster import KMeans
import numpy as np

# æ— æ ‡ç­¾æ•°æ®
X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])

# èšç±»
kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')
kmeans.fit(X)

print(f"èšç±»æ ‡ç­¾: {kmeans.labels_}")
print(f"èšç±»ä¸­å¿ƒ: {kmeans.cluster_centers_}")
```

### 3. å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)

æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒ**äº¤äº’**ï¼Œæ ¹æ®**å¥–åŠ±ä¿¡å·**å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

```mermaid
graph LR
    A[æ™ºèƒ½ä½“] -->|åŠ¨ä½œ Action| B[ç¯å¢ƒ]
    B -->|çŠ¶æ€ State| A
    B -->|å¥–åŠ± Reward| A
```

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

| æ¦‚å¿µ        | æè¿°                   |
| ----------- | ---------------------- |
| Agent       | å­¦ä¹ å’Œå†³ç­–çš„ä¸»ä½“       |
| Environment | æ™ºèƒ½ä½“äº¤äº’çš„å¤–éƒ¨ä¸–ç•Œ   |
| State       | ç¯å¢ƒçš„å½“å‰çŠ¶æ€         |
| Action      | æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œ     |
| Reward      | ç¯å¢ƒç»™äºˆçš„åé¦ˆä¿¡å·     |
| Policy      | ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ç­–ç•¥ |

**åº”ç”¨åœºæ™¯**ï¼šæ¸¸æˆ AIã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶

## æ¨¡å‹è¯„ä¼°çš„æ ¸å¿ƒé—®é¢˜

### è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

```mermaid
graph TB
    subgraph æ¬ æ‹Ÿåˆ ["æ¬ æ‹Ÿåˆ (Underfitting)"]
        A1[æ¨¡å‹å¤ªç®€å•]
        A2[è®­ç»ƒè¯¯å·®é«˜]
        A3[æµ‹è¯•è¯¯å·®é«˜]
    end

    subgraph é€‚åº¦ ["åˆé€‚ (Good Fit)"]
        B1[æ¨¡å‹å¤æ‚åº¦é€‚ä¸­]
        B2[è®­ç»ƒè¯¯å·®é€‚ä¸­]
        B3[æµ‹è¯•è¯¯å·®ä½]
    end

    subgraph è¿‡æ‹Ÿåˆ ["è¿‡æ‹Ÿåˆ (Overfitting)"]
        C1[æ¨¡å‹å¤ªå¤æ‚]
        C2[è®­ç»ƒè¯¯å·®å¾ˆä½]
        C3[æµ‹è¯•è¯¯å·®é«˜]
    end
```

**åˆ¤æ–­æ–¹æ³•**ï¼š

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

def plot_learning_curve(estimator, X, y):
    train_sizes, train_scores, val_scores = learning_curve(
        estimator, X, y, cv=5,
        train_sizes=np.linspace(0.1, 1.0, 10)
    )

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores.mean(axis=1), label='è®­ç»ƒåˆ†æ•°')
    plt.plot(train_sizes, val_scores.mean(axis=1), label='éªŒè¯åˆ†æ•°')
    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel('åˆ†æ•°')
    plt.legend()
    plt.title('å­¦ä¹ æ›²çº¿')
    plt.show()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

| é—®é¢˜   | åŸå›        | è§£å†³æ–¹æ¡ˆ                                  |
| ------ | ---------- | ----------------------------------------- |
| æ¬ æ‹Ÿåˆ | æ¨¡å‹å¤ªç®€å• | å¢åŠ ç‰¹å¾ã€ä½¿ç”¨æ›´å¤æ‚æ¨¡å‹ã€å‡å°‘æ­£åˆ™åŒ–      |
| è¿‡æ‹Ÿåˆ | æ¨¡å‹å¤ªå¤æ‚ | å¢åŠ æ•°æ®ã€æ­£åˆ™åŒ–ã€Dropoutã€æ—©åœã€äº¤å‰éªŒè¯ |

### åå·®ä¸æ–¹å·®

$$
\text{æ€»è¯¯å·®} = \text{åå·®}^2 + \text{æ–¹å·®} + \text{ä¸å¯çº¦è¯¯å·®}
$$

| æ¦‚å¿µ          | æè¿°                       | é«˜åå·®è¡¨ç° | é«˜æ–¹å·®è¡¨ç° |
| ------------- | -------------------------- | ---------- | ---------- |
| åå·® Bias     | æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®è·   | æ¬ æ‹Ÿåˆ     | -          |
| æ–¹å·® Variance | æ¨¡å‹å¯¹ä¸åŒè®­ç»ƒé›†çš„æ•æ„Ÿç¨‹åº¦ | -          | è¿‡æ‹Ÿåˆ     |

```mermaid
graph LR
    subgraph åå·®-æ–¹å·®æƒè¡¡
        A[ç®€å•æ¨¡å‹] -->|é«˜åå·® ä½æ–¹å·®| B[å¤æ‚æ¨¡å‹]
        B -->|ä½åå·® é«˜æ–¹å·®| A
    end
```

## æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹

```mermaid
graph TB
    A[1. é—®é¢˜å®šä¹‰] --> B[2. æ•°æ®æ”¶é›†]
    B --> C[3. æ•°æ®é¢„å¤„ç†]
    C --> D[4. ç‰¹å¾å·¥ç¨‹]
    D --> E[5. æ¨¡å‹é€‰æ‹©]
    E --> F[6. æ¨¡å‹è®­ç»ƒ]
    F --> G[7. æ¨¡å‹è¯„ä¼°]
    G -->|æ•ˆæœä¸ä½³| D
    G -->|æ•ˆæœè‰¯å¥½| H[8. æ¨¡å‹éƒ¨ç½²]
    H --> I[9. ç›‘æ§ä¸è¿­ä»£]
```

```python
# å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹ç¤ºä¾‹
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 1-2. æ•°æ®æ”¶é›†
data = load_breast_cancer()
X, y = data.data, data.target

# 3. æ•°æ®é¢„å¤„ç†ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. ç‰¹å¾å·¥ç¨‹ï¼šæ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5-6. æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒ
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# 7. æ¨¡å‹è¯„ä¼°
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
print(f"äº¤å‰éªŒè¯åˆ†æ•°: {cv_scores.mean():.2%} Â± {cv_scores.std():.2%}")

y_pred = model.predict(X_test_scaled)
print(classification_report(y_test, y_pred, target_names=data.target_names))
```

## å¸¸è§æœ¯è¯­è¡¨

| æœ¯è¯­     | è‹±æ–‡              | æè¿°                         |
| -------- | ----------------- | ---------------------------- |
| ç‰¹å¾     | Feature           | è¾“å…¥å˜é‡ï¼Œç”¨äºæè¿°æ•°æ®çš„å±æ€§ |
| æ ‡ç­¾     | Label / Target    | è¾“å‡ºå˜é‡ï¼Œéœ€è¦é¢„æµ‹çš„å€¼       |
| æ ·æœ¬     | Sample / Instance | ä¸€æ¡æ•°æ®è®°å½•                 |
| è®­ç»ƒé›†   | Training Set      | ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®           |
| éªŒè¯é›†   | Validation Set    | ç”¨äºè°ƒå‚å’Œæ¨¡å‹é€‰æ‹©çš„æ•°æ®     |
| æµ‹è¯•é›†   | Test Set          | ç”¨äºæœ€ç»ˆè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æ•°æ®   |
| è¶…å‚æ•°   | Hyperparameter    | æ¨¡å‹è®­ç»ƒå‰è®¾å®šçš„å‚æ•°         |
| æŸå¤±å‡½æ•° | Loss Function     | è¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®è·çš„å‡½æ•° |
| æ¢¯åº¦ä¸‹é™ | Gradient Descent  | æœ€å°åŒ–æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ç®—æ³•     |
