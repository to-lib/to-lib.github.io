---
sidebar_position: 6
title: ðŸ” æ— ç›‘ç£å­¦ä¹ 
---

# æ— ç›‘ç£å­¦ä¹ ç®—æ³•

æ— ç›‘ç£å­¦ä¹ ä»Žæ— æ ‡ç­¾æ•°æ®ä¸­å‘çŽ°éšè—çš„ç»“æž„å’Œæ¨¡å¼ã€‚

## èšç±»ç®—æ³•

### K-Means

å°†æ•°æ®åˆ’åˆ†ä¸º K ä¸ªç°‡ï¼Œä½¿æ¯ä¸ªæ ·æœ¬åˆ°å…¶æ‰€å±žç°‡ä¸­å¿ƒçš„è·ç¦»æœ€å°ã€‚

```mermaid
graph LR
    A[1. éšæœºåˆå§‹åŒ– K ä¸ªä¸­å¿ƒ] --> B[2. åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘ä¸­å¿ƒ]
    B --> C[3. æ›´æ–°ç°‡ä¸­å¿ƒ]
    C --> D{ä¸­å¿ƒæ˜¯å¦å˜åŒ–?}
    D -->|æ˜¯| B
    D -->|å¦| E[ç»“æŸ]
```

```python
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
from sklearn.datasets import make_blobs
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)

# K-Means èšç±»
kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')
labels = kmeans.fit_predict(X)

# å¯è§†åŒ–
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            c='red', marker='X', s=200, label='ä¸­å¿ƒ')
plt.title('K-Means èšç±»ç»“æžœ')
plt.legend()
plt.show()
```

**é€‰æ‹©æœ€ä¼˜ K å€¼**ï¼š

```python
# è‚˜éƒ¨æ³•åˆ™ (Elbow Method)
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('K å€¼')
plt.ylabel('Inertia (ç°‡å†…å¹³æ–¹å’Œ)')
plt.title('è‚˜éƒ¨æ³•åˆ™é€‰æ‹© K')
plt.show()

# è½®å»“ç³»æ•° (Silhouette Score)
from sklearn.metrics import silhouette_score

silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)
    print(f"K={k}: è½®å»“ç³»æ•°={score:.3f}")
```

### DBSCAN

åŸºäºŽå¯†åº¦çš„èšç±»ï¼Œèƒ½å‘çŽ°ä»»æ„å½¢çŠ¶çš„ç°‡å¹¶è‡ªåŠ¨è¯†åˆ«å™ªå£°ç‚¹ã€‚

```python
from sklearn.cluster import DBSCAN

# DBSCAN èšç±»
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# -1 è¡¨ç¤ºå™ªå£°ç‚¹
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)
print(f"èšç±»æ•°: {n_clusters}, å™ªå£°ç‚¹: {n_noise}")

# å¯è§†åŒ–
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.title(f'DBSCAN (clusters={n_clusters}, noise={n_noise})')
plt.show()
```

| å‚æ•°        | æè¿°             | è°ƒå‚å»ºè®®               |
| ----------- | ---------------- | ---------------------- |
| eps         | é‚»åŸŸåŠå¾„         | ä½¿ç”¨ k-distance å›¾ç¡®å®š |
| min_samples | æ ¸å¿ƒç‚¹æœ€å°é‚»å±…æ•° | é€šå¸¸ >= 2 Ã— ç»´åº¦       |

### å±‚æ¬¡èšç±» (Hierarchical Clustering)

è‡ªåº•å‘ä¸Šï¼ˆå‡èšï¼‰æˆ–è‡ªé¡¶å‘ä¸‹ï¼ˆåˆ†è£‚ï¼‰æž„å»ºèšç±»å±‚æ¬¡ç»“æž„ã€‚

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# å‡èšèšç±»
agg = AgglomerativeClustering(n_clusters=4, linkage='ward')
labels = agg.fit_predict(X)

# æ ‘çŠ¶å›¾
Z = linkage(X, method='ward')
plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.title('å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾')
plt.xlabel('æ ·æœ¬ç´¢å¼•')
plt.ylabel('è·ç¦»')
plt.show()
```

| é“¾æŽ¥æ–¹æ³• | æè¿°       | ç‰¹ç‚¹                   |
| -------- | ---------- | ---------------------- |
| ward     | æœ€å°åŒ–æ–¹å·® | å€¾å‘äºŽäº§ç”Ÿå¤§å°ç›¸ä¼¼çš„ç°‡ |
| complete | æœ€å¤§è·ç¦»   | å€¾å‘äºŽäº§ç”Ÿç´§å‡‘çš„ç°‡     |
| average  | å¹³å‡è·ç¦»   | ä»‹äºŽä¸¤è€…ä¹‹é—´           |
| single   | æœ€å°è·ç¦»   | å¯èƒ½äº§ç”Ÿé“¾çŠ¶ç°‡         |

## èšç±»ç®—æ³•å¯¹æ¯”

| ç®—æ³•     | ç°‡å½¢çŠ¶ | éœ€è¦æŒ‡å®š K | å™ªå£°å¤„ç† | æ—¶é—´å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯           |
| -------- | ------ | ---------- | -------- | ---------- | ------------------ |
| K-Means  | çƒå½¢   | æ˜¯         | æ•æ„Ÿ     | O(nKt)     | å¤§æ•°æ®ã€çƒå½¢ç°‡     |
| DBSCAN   | ä»»æ„   | å¦         | èƒ½è¯†åˆ«   | O(nÂ²)      | å™ªå£°æ•°æ®ã€ä»»æ„å½¢çŠ¶ |
| å±‚æ¬¡èšç±» | ä»»æ„   | å¯é€‰       | æ•æ„Ÿ     | O(nÂ²log n) | å°æ•°æ®ã€éœ€è¦å±‚æ¬¡   |
| GMM      | æ¤­åœ†   | æ˜¯         | æ•æ„Ÿ     | O(nKdt)    | æ¦‚çŽ‡èšç±»           |

## é™ç»´ç®—æ³•

### PCA (ä¸»æˆåˆ†åˆ†æž)

é€šè¿‡çº¿æ€§å˜æ¢å°†æ•°æ®æŠ•å½±åˆ°æ–¹å·®æœ€å¤§çš„æ–¹å‘ä¸Šï¼Œå®žçŽ°é™ç»´ã€‚

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# åŠ è½½æ•°æ®
iris = load_iris()
X = iris.data

# PCA é™ç»´
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# å¯è§†åŒ–
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
for i, target in enumerate(iris.target_names):
    mask = iris.target == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=target, alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.title('PCA é™ç»´å¯è§†åŒ–')

# æ–¹å·®è§£é‡Šæ¯”
plt.subplot(1, 2, 2)
pca_full = PCA()
pca_full.fit(X)
plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1),
        pca_full.explained_variance_ratio_)
plt.xlabel('ä¸»æˆåˆ†')
plt.ylabel('æ–¹å·®è§£é‡Šæ¯”')
plt.title('å„ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®')
plt.show()

print(f"å‰ 2 ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {sum(pca.explained_variance_ratio_):.2%}")
```

**é€‰æ‹©ä¸»æˆåˆ†æ•°é‡**ï¼š

```python
# ä¿ç•™ 95% æ–¹å·®
pca_95 = PCA(n_components=0.95)
X_reduced = pca_95.fit_transform(X)
print(f"ä¿ç•™ 95% æ–¹å·®éœ€è¦ {pca_95.n_components_} ä¸ªä¸»æˆåˆ†")
```

### t-SNE

éžçº¿æ€§é™ç»´ï¼Œæ“…é•¿å¯è§†åŒ–é«˜ç»´æ•°æ®çš„å±€éƒ¨ç»“æž„ã€‚

```python
from sklearn.manifold import TSNE

# t-SNE é™ç»´ï¼ˆé€šå¸¸ç”¨äºŽå¯è§†åŒ–ï¼‰
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(8, 6))
for i, target in enumerate(iris.target_names):
    mask = iris.target == i
    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=target, alpha=0.7)
plt.legend()
plt.title('t-SNE å¯è§†åŒ–')
plt.show()
```

| å‚æ•°          | æè¿°     | å»ºè®®          |
| ------------- | -------- | ------------- |
| perplexity    | è¿‘é‚»æ•°é‡ | 5-50ï¼Œé€šå¸¸ 30 |
| n_iter        | è¿­ä»£æ¬¡æ•° | >= 1000       |
| learning_rate | å­¦ä¹ çŽ‡   | 10-1000ï¼Œauto |

### UMAP

æ¯” t-SNE æ›´å¿«ï¼Œä¸”èƒ½æ›´å¥½åœ°ä¿ç•™å…¨å±€ç»“æž„ã€‚

```python
import umap

# UMAP é™ç»´
reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X)

plt.figure(figsize=(8, 6))
for i, target in enumerate(iris.target_names):
    mask = iris.target == i
    plt.scatter(X_umap[mask, 0], X_umap[mask, 1], label=target, alpha=0.7)
plt.legend()
plt.title('UMAP å¯è§†åŒ–')
plt.show()
```

## é™ç»´ç®—æ³•å¯¹æ¯”

| ç®—æ³•  | ç±»åž‹         | é€Ÿåº¦ | ä¿ç•™ç»“æž„  | é€‚ç”¨åœºæ™¯       |
| ----- | ------------ | ---- | --------- | -------------- |
| PCA   | çº¿æ€§         | å¿«   | å…¨å±€      | ç‰¹å¾é™ç»´ã€åŽ»å™ª |
| t-SNE | éžçº¿æ€§       | æ…¢   | å±€éƒ¨      | é«˜ç»´æ•°æ®å¯è§†åŒ– |
| UMAP  | éžçº¿æ€§       | è¾ƒå¿« | å…¨å±€+å±€éƒ¨ | å¯è§†åŒ–ã€é™ç»´   |
| LDA   | çº¿æ€§ï¼ˆç›‘ç£ï¼‰ | å¿«   | ç±»é—´      | åˆ†ç±»ç‰¹å¾æå–   |

## å¼‚å¸¸æ£€æµ‹

### Isolation Forest

é€šè¿‡éšæœºåˆ†å‰²æ¥éš”ç¦»å¼‚å¸¸ç‚¹ã€‚

```python
from sklearn.ensemble import IsolationForest

# å¼‚å¸¸æ£€æµ‹
iso_forest = IsolationForest(contamination=0.1, random_state=42)
predictions = iso_forest.fit_predict(X)  # 1: æ­£å¸¸, -1: å¼‚å¸¸

# å¯è§†åŒ–
plt.scatter(X[:, 0], X[:, 1], c=predictions, cmap='coolwarm')
plt.title('Isolation Forest å¼‚å¸¸æ£€æµ‹')
plt.show()

n_outliers = sum(predictions == -1)
print(f"æ£€æµ‹åˆ° {n_outliers} ä¸ªå¼‚å¸¸ç‚¹")
```

### One-Class SVM

å­¦ä¹ æ­£å¸¸æ•°æ®çš„è¾¹ç•Œï¼Œè¶…å‡ºè¾¹ç•Œçš„è§†ä¸ºå¼‚å¸¸ã€‚

```python
from sklearn.svm import OneClassSVM

# è®­ç»ƒ One-Class SVM
oc_svm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)
predictions = oc_svm.fit_predict(X)

# nu å‚æ•°æŽ§åˆ¶å¼‚å¸¸æ¯”ä¾‹çš„ä¸Šç•Œ
```

## å…³è”è§„åˆ™å­¦ä¹ 

å‘çŽ°æ•°æ®ä¸­çš„é¢‘ç¹æ¨¡å¼å’Œå…³è”è§„åˆ™ã€‚

```python
from mlxtend.frequent_patterns import apriori, association_rules

# å‡†å¤‡äº‹åŠ¡æ•°æ®ï¼ˆæ¯è¡Œä¸€ä¸ªè´­ç‰©ç¯®ï¼‰
transactions = pd.DataFrame({
    'ç‰›å¥¶': [1, 1, 0, 1, 0],
    'é¢åŒ…': [1, 1, 1, 0, 1],
    'é»„æ²¹': [0, 1, 0, 1, 0],
    'å•¤é…’': [0, 0, 1, 0, 1],
    'å°¿å¸ƒ': [0, 0, 1, 0, 1]
})

# æŒ–æŽ˜é¢‘ç¹é¡¹é›†
frequent_itemsets = apriori(transactions, min_support=0.4, use_colnames=True)
print("é¢‘ç¹é¡¹é›†:")
print(frequent_itemsets)

# ç”Ÿæˆå…³è”è§„åˆ™
rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)
print("\nå…³è”è§„åˆ™:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

| æŒ‡æ ‡   | å…¬å¼         | å«ä¹‰                      |
| ------ | ------------ | ------------------------- |
| æ”¯æŒåº¦ | P(Aâˆ©B)       | è§„åˆ™å‡ºçŽ°çš„é¢‘çŽ‡            |
| ç½®ä¿¡åº¦ | P(B\|A)      | A å‡ºçŽ°æ—¶ B ä¹Ÿå‡ºçŽ°çš„æ¦‚çŽ‡   |
| æå‡åº¦ | P(B\|A)/P(B) | å…³è”å¼ºåº¦ï¼ˆ>1 è¡¨ç¤ºæ­£ç›¸å…³ï¼‰ |
