---
sidebar_position: 4
title: ğŸ”§ æ•°æ®é¢„å¤„ç†
---

# æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹

> æ•°æ®å’Œç‰¹å¾å†³å®šäº†æ¨¡å‹çš„ä¸Šé™ï¼Œè€Œç®—æ³•åªæ˜¯é€¼è¿‘è¿™ä¸ªä¸Šé™ã€‚

## æ•°æ®é¢„å¤„ç†æµç¨‹

```mermaid
graph LR
    A[åŸå§‹æ•°æ®] --> B[æ•°æ®æ¸…æ´—]
    B --> C[ç‰¹å¾å·¥ç¨‹]
    C --> D[æ•°æ®è½¬æ¢]
    D --> E[ç‰¹å¾é€‰æ‹©]
    E --> F[æ¨¡å‹å°±ç»ªæ•°æ®]
```

## æ•°æ®æ¸…æ´—

### å¤„ç†ç¼ºå¤±å€¼

```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

df = pd.DataFrame({
    'age': [25, 30, np.nan, 35, 28],
    'salary': [50000, np.nan, 75000, np.nan, 60000],
    'city': ['åŒ—äº¬', 'ä¸Šæµ·', np.nan, 'æ·±åœ³', 'å¹¿å·']
})

# æŸ¥çœ‹ç¼ºå¤±å€¼
print(df.isnull().sum())

# æ–¹æ³•1: åˆ é™¤ç¼ºå¤±å€¼
df_clean = df.dropna()

# æ–¹æ³•2: å¡«å……ç¼ºå¤±å€¼
df['age'].fillna(df['age'].mean(), inplace=True)        # å‡å€¼å¡«å……
df['salary'].fillna(df['salary'].median(), inplace=True) # ä¸­ä½æ•°å¡«å……
df['city'].fillna(df['city'].mode()[0], inplace=True)   # ä¼—æ•°å¡«å……

# æ–¹æ³•3: ä½¿ç”¨ sklearn Imputer
imputer = SimpleImputer(strategy='mean')
df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])
```

| å¡«å……ç­–ç•¥ | é€‚ç”¨åœºæ™¯           | ä¼˜ç‚¹         | ç¼ºç‚¹           |
| -------- | ------------------ | ------------ | -------------- |
| å‡å€¼     | æ­£æ€åˆ†å¸ƒçš„è¿ç»­å˜é‡ | ç®€å•         | æ˜“å—å¼‚å¸¸å€¼å½±å“ |
| ä¸­ä½æ•°   | åæ€åˆ†å¸ƒçš„è¿ç»­å˜é‡ | é²æ£’         | å¯èƒ½ä¸å¤Ÿç²¾ç¡®   |
| ä¼—æ•°     | åˆ†ç±»å˜é‡           | ä¿æŒåˆ†å¸ƒ     | å¯èƒ½å¼•å…¥åå·®   |
| KNN      | ä»»æ„ç±»å‹           | åˆ©ç”¨ç›¸ä¼¼æ ·æœ¬ | è®¡ç®—å¼€é”€å¤§     |

### å¤„ç†å¼‚å¸¸å€¼

```python
import matplotlib.pyplot as plt

# æ£€æµ‹å¼‚å¸¸å€¼ - IQR æ–¹æ³•
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]

# æ£€æµ‹å¼‚å¸¸å€¼ - Z-score æ–¹æ³•
from scipy import stats

def detect_outliers_zscore(data, column, threshold=3):
    z_scores = np.abs(stats.zscore(data[column]))
    return data[z_scores > threshold]

# å¤„ç†å¼‚å¸¸å€¼ - æˆªæ–­
def clip_outliers(data, column, lower_percentile=1, upper_percentile=99):
    lower = data[column].quantile(lower_percentile / 100)
    upper = data[column].quantile(upper_percentile / 100)
    data[column] = data[column].clip(lower, upper)
    return data
```

### å¤„ç†é‡å¤å€¼

```python
# æ£€æµ‹é‡å¤å€¼
duplicates = df.duplicated()
print(f"é‡å¤è¡Œæ•°: {duplicates.sum()}")

# åˆ é™¤é‡å¤å€¼
df_unique = df.drop_duplicates()

# ä¿ç•™æœ€åä¸€æ¡
df_unique = df.drop_duplicates(keep='last')
```

## ç‰¹å¾å·¥ç¨‹

### æ•°å€¼ç‰¹å¾å¤„ç†

#### æ ‡å‡†åŒ– (Standardization)

å°†ç‰¹å¾è½¬æ¢ä¸ºå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1 çš„åˆ†å¸ƒã€‚

$$
z = \frac{x - \mu}{\sigma}
$$

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# é€†è½¬æ¢
X_original = scaler.inverse_transform(X_scaled)
```

#### å½’ä¸€åŒ– (Normalization)

å°†ç‰¹å¾ç¼©æ”¾åˆ° [0, 1] èŒƒå›´ã€‚

$$
x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)
```

| æ–¹æ³•           | å…¬å¼                | é€‚ç”¨åœºæ™¯           | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ |
| -------------- | ------------------- | ------------------ | ------------ |
| StandardScaler | $(x-\mu)/\sigma$    | æ­£æ€åˆ†å¸ƒã€æ¢¯åº¦ä¸‹é™ | æ˜¯           |
| MinMaxScaler   | $(x-min)/(max-min)$ | éœ€è¦å›ºå®šèŒƒå›´       | æ˜¯           |
| RobustScaler   | $(x-median)/IQR$    | æœ‰å¼‚å¸¸å€¼           | å¦           |

```python
from sklearn.preprocessing import RobustScaler

# å¯¹å¼‚å¸¸å€¼é²æ£’çš„ç¼©æ”¾
robust_scaler = RobustScaler()
X_robust = robust_scaler.fit_transform(X)
```

### åˆ†ç±»ç‰¹å¾ç¼–ç 

#### One-Hot ç¼–ç 

```python
from sklearn.preprocessing import OneHotEncoder

# åŸå§‹æ•°æ®
categories = [['çº¢è‰²'], ['è“è‰²'], ['ç»¿è‰²'], ['çº¢è‰²']]

encoder = OneHotEncoder(sparse_output=False)
encoded = encoder.fit_transform(categories)
print(encoded)
# [[1. 0. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]
#  [1. 0. 0.]]

# Pandas æ–¹æ³•
df = pd.DataFrame({'color': ['çº¢è‰²', 'è“è‰²', 'ç»¿è‰²', 'çº¢è‰²']})
df_encoded = pd.get_dummies(df, columns=['color'])
```

#### Label ç¼–ç 

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
labels = ['ä½', 'ä¸­', 'é«˜', 'ä¸­', 'ä½']
encoded = le.fit_transform(labels)  # [0, 2, 1, 2, 0]

# é€†è½¬æ¢
original = le.inverse_transform(encoded)
```

#### ç›®æ ‡ç¼–ç  (Target Encoding)

```python
# ç”¨ç›®æ ‡å˜é‡çš„å‡å€¼æ›¿æ¢ç±»åˆ«
def target_encode(df, column, target):
    means = df.groupby(column)[target].mean()
    return df[column].map(means)

# ç¤ºä¾‹
df['city_encoded'] = target_encode(df, 'city', 'salary')
```

| ç¼–ç æ–¹æ³• | é€‚ç”¨åœºæ™¯           | ä¼˜ç‚¹       | ç¼ºç‚¹         |
| -------- | ------------------ | ---------- | ------------ |
| One-Hot  | æ— åºç±»åˆ«ï¼Œç±»åˆ«æ•°å°‘ | ä¸å¼•å…¥é¡ºåº | ç»´åº¦çˆ†ç‚¸     |
| Label    | æœ‰åºç±»åˆ«           | ç»´åº¦ä¸å˜   | å¼•å…¥è™šå‡é¡ºåº |
| Target   | é«˜åŸºæ•°ç±»åˆ«         | ä¿ç•™ä¿¡æ¯   | æ˜“è¿‡æ‹Ÿåˆ     |

### ç‰¹å¾æ„é€ 

```python
# åˆ›å»ºæ–°ç‰¹å¾
df['age_squared'] = df['age'] ** 2
df['salary_per_age'] = df['salary'] / df['age']
df['log_salary'] = np.log1p(df['salary'])

# æ—¶é—´ç‰¹å¾
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'] >= 5

# æ–‡æœ¬ç‰¹å¾
df['text_length'] = df['text'].str.len()
df['word_count'] = df['text'].str.split().str.len()
```

### ç‰¹å¾åˆ†ç®± (Binning)

```python
from sklearn.preprocessing import KBinsDiscretizer

# ç­‰å®½åˆ†ç®±
df['age_bin'] = pd.cut(df['age'], bins=5, labels=['é’å¹´', 'é’ä¸­', 'ä¸­å¹´', 'ä¸­è€', 'è€å¹´'])

# ç­‰é¢‘åˆ†ç®±
df['salary_bin'] = pd.qcut(df['salary'], q=4, labels=['ä½', 'ä¸­ä½', 'ä¸­é«˜', 'é«˜'])

# sklearn åˆ†ç®±
binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
df['age_binned'] = binner.fit_transform(df[['age']])
```

## ç‰¹å¾é€‰æ‹©

### è¿‡æ»¤æ³• (Filter)

```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

# åŸºäºæ–¹å·®
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_selected = selector.fit_transform(X)

# åŸºäºç»Ÿè®¡æ£€éªŒ
selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X, y)

# æŸ¥çœ‹ç‰¹å¾åˆ†æ•°
scores = pd.DataFrame({
    'feature': feature_names,
    'score': selector.scores_
}).sort_values('score', ascending=False)
```

### åµŒå…¥æ³• (Embedded)

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

# ä½¿ç”¨éšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# è·å–ç‰¹å¾é‡è¦æ€§
importances = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# é€‰æ‹©é‡è¦ç‰¹å¾
selector = SelectFromModel(rf, threshold='mean')
X_selected = selector.fit_transform(X, y)
```

### åŒ…è£…æ³• (Wrapper)

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# é€’å½’ç‰¹å¾æ¶ˆé™¤
model = LogisticRegression(max_iter=1000)
rfe = RFE(estimator=model, n_features_to_select=10)
X_selected = rfe.fit_transform(X, y)

# æŸ¥çœ‹æ’å
ranking = pd.DataFrame({
    'feature': feature_names,
    'ranking': rfe.ranking_
}).sort_values('ranking')
```

## æ•°æ®å¤„ç† Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# å®šä¹‰æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾
numeric_features = ['age', 'salary']
categorical_features = ['city', 'gender']

# æ•°å€¼ç‰¹å¾å¤„ç†
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# åˆ†ç±»ç‰¹å¾å¤„ç†
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# ç»„åˆè½¬æ¢å™¨
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# å®Œæ•´ Pipelineï¼ˆåŒ…å«æ¨¡å‹ï¼‰
from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100))
])

# è®­ç»ƒå’Œé¢„æµ‹
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

## å°ç»“

| é˜¶æ®µ     | å…³é”®æ“ä½œ               | å¸¸ç”¨å·¥å…·                         |
| -------- | ---------------------- | -------------------------------- |
| æ•°æ®æ¸…æ´— | ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤å€¼ | `pandas`, `SimpleImputer`        |
| æ•°å€¼å¤„ç† | æ ‡å‡†åŒ–ã€å½’ä¸€åŒ–         | `StandardScaler`, `MinMaxScaler` |
| åˆ†ç±»ç¼–ç  | One-Hotã€Labelã€Target | `OneHotEncoder`, `LabelEncoder`  |
| ç‰¹å¾æ„é€  | äº¤äº’ã€å¤šé¡¹å¼ã€æ—¶é—´     | `pandas`, `PolynomialFeatures`   |
| ç‰¹å¾é€‰æ‹© | è¿‡æ»¤ã€åµŒå…¥ã€åŒ…è£…       | `SelectKBest`, `RFE`             |
| Pipeline | è‡ªåŠ¨åŒ–æµç¨‹             | `Pipeline`, `ColumnTransformer`  |
