---
sidebar_position: 38
title: ğŸ” å¼‚å¸¸æ£€æµ‹è¯¦è§£
---

# å¼‚å¸¸æ£€æµ‹è¯¦è§£

å¼‚å¸¸æ£€æµ‹è¯†åˆ«ä¸æ­£å¸¸æ¨¡å¼æ˜¾è‘—ä¸åŒçš„æ•°æ®ç‚¹ã€‚

## æ–¹æ³•åˆ†ç±»

```mermaid
graph TB
    A[å¼‚å¸¸æ£€æµ‹] --> B[ç»Ÿè®¡æ–¹æ³•]
    A --> C[æœºå™¨å­¦ä¹ ]
    A --> D[æ·±åº¦å­¦ä¹ ]
    B --> B1[Z-Score]
    B --> B2[IQR]
    C --> C1[Isolation Forest]
    C --> C2[One-Class SVM]
    C --> C3[LOF]
    D --> D1[AutoEncoder]
    D --> D2[VAE]
```

## ç»Ÿè®¡æ–¹æ³•

```python
import numpy as np
from scipy import stats

# Z-Score
def zscore_anomaly(data, threshold=3):
    z_scores = np.abs(stats.zscore(data))
    return z_scores > threshold

# IQR
def iqr_anomaly(data, k=1.5):
    Q1, Q3 = np.percentile(data, [25, 75])
    IQR = Q3 - Q1
    lower = Q1 - k * IQR
    upper = Q3 + k * IQR
    return (data < lower) | (data > upper)
```

## æœºå™¨å­¦ä¹ æ–¹æ³•

### Isolation Forest

```python
from sklearn.ensemble import IsolationForest

iso = IsolationForest(contamination=0.1, random_state=42)
predictions = iso.fit_predict(X)  # -1: å¼‚å¸¸, 1: æ­£å¸¸
scores = iso.decision_function(X)  # å¼‚å¸¸åˆ†æ•°
```

### Local Outlier Factor

```python
from sklearn.neighbors import LocalOutlierFactor

lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
predictions = lof.fit_predict(X)
scores = -lof.negative_outlier_factor_
```

### One-Class SVM

```python
from sklearn.svm import OneClassSVM

ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)
predictions = ocsvm.fit_predict(X)
```

## æ·±åº¦å­¦ä¹ æ–¹æ³•

### AutoEncoder

```python
import torch.nn as nn

class AnomalyAutoEncoder(nn.Module):
    def __init__(self, input_dim, latent_dim=32):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

    def anomaly_score(self, x):
        recon = self.forward(x)
        return ((x - recon) ** 2).mean(dim=1)

# é‡å»ºè¯¯å·®å¤§ â†’ å¼‚å¸¸
scores = model.anomaly_score(X)
threshold = np.percentile(scores, 95)
anomalies = scores > threshold
```

### VAE å¼‚å¸¸æ£€æµ‹

```python
class VAEAD(nn.Module):
    def anomaly_score(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        recon = self.decode(z)

        # é‡å»ºè¯¯å·® + KL æ•£åº¦
        recon_loss = ((x - recon) ** 2).sum(dim=1)
        kl_loss = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp()).sum(dim=1)

        return recon_loss + kl_loss
```

## æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹

```python
from pyod.models.knn import KNN
from pyod.models.copod import COPOD

# æ»‘åŠ¨çª—å£ç‰¹å¾
def create_features(series, window=10):
    features = []
    for i in range(window, len(series)):
        features.append([
            series[i-window:i].mean(),
            series[i-window:i].std(),
            series[i] - series[i-1]
        ])
    return np.array(features)

# æ£€æµ‹
detector = COPOD()
detector.fit(features)
scores = detector.decision_scores_
```

## è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import precision_recall_curve, auc

# ç”±äºå¼‚å¸¸ç¨€å°‘ï¼Œä½¿ç”¨ PR-AUC
precision, recall, _ = precision_recall_curve(y_true, scores)
pr_auc = auc(recall, precision)
```

| æŒ‡æ ‡     | æè¿°                  |
| -------- | --------------------- |
| PR-AUC   | ç²¾ç¡®ç‡-å¬å›ç‡ AUC     |
| F1@K     | å‰ K ä¸ªé¢„æµ‹çš„ F1      |
| Hit Rate | çœŸå®å¼‚å¸¸åœ¨å‰ K çš„æ¯”ä¾‹ |
