---
sidebar_position: 8
title: ğŸš€ æ·±åº¦å­¦ä¹ å…¥é—¨
---

# æ·±åº¦å­¦ä¹ å…¥é—¨

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚

## å·ç§¯ç¥ç»ç½‘ç»œ (CNN)

CNN ç‰¹åˆ«æ“…é•¿å¤„ç†å…·æœ‰ç½‘æ ¼ç»“æ„çš„æ•°æ®ï¼Œå¦‚å›¾åƒã€‚

### æ ¸å¿ƒç»„ä»¶

```mermaid
graph LR
    A[è¾“å…¥å›¾åƒ] --> B[å·ç§¯å±‚]
    B --> C[æ¿€æ´»å‡½æ•°]
    C --> D[æ± åŒ–å±‚]
    D --> E[...]
    E --> F[å…¨è¿æ¥å±‚]
    F --> G[è¾“å‡º]
```

**å·ç§¯å±‚**ï¼šæå–å±€éƒ¨ç‰¹å¾

```python
import torch
import torch.nn as nn

# å·ç§¯æ“ä½œ
conv = nn.Conv2d(
    in_channels=3,      # è¾“å…¥é€šé“æ•°ï¼ˆRGB=3ï¼‰
    out_channels=16,    # è¾“å‡ºé€šé“æ•°ï¼ˆå·ç§¯æ ¸æ•°é‡ï¼‰
    kernel_size=3,      # å·ç§¯æ ¸å¤§å°
    stride=1,           # æ­¥å¹…
    padding=1           # å¡«å……
)

# è¾“å‡ºå°ºå¯¸è®¡ç®—
# output_size = (input_size - kernel_size + 2*padding) / stride + 1
```

**æ± åŒ–å±‚**ï¼šä¸‹é‡‡æ ·ï¼Œå‡å°‘å‚æ•°

```python
# æœ€å¤§æ± åŒ–
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

# å¹³å‡æ± åŒ–
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)
```

### ç»å…¸ CNN æ¶æ„

```python
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()

        # å·ç§¯å±‚
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )

        # å…¨è¿æ¥å±‚
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x
```

### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
from torchvision import models

# åŠ è½½é¢„è®­ç»ƒçš„ ResNet
resnet = models.resnet18(pretrained=True)

# å†»ç»“æ‰€æœ‰å±‚
for param in resnet.parameters():
    param.requires_grad = False

# æ›¿æ¢æœ€åçš„åˆ†ç±»å±‚
resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)

# åªè®­ç»ƒæœ€åä¸€å±‚
optimizer = torch.optim.Adam(resnet.fc.parameters(), lr=0.001)
```

## å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)

RNN æ“…é•¿å¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€æ—¶é—´åºåˆ—ã€‚

### åŸºæœ¬ RNN

```mermaid
graph LR
    x1[xâ‚] --> h1((hâ‚))
    x2[xâ‚‚] --> h2((hâ‚‚))
    x3[xâ‚ƒ] --> h3((hâ‚ƒ))
    h1 --> h2
    h2 --> h3
    h3 --> y[è¾“å‡º]
```

$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b)
$$

```python
# åŸºæœ¬ RNN
rnn = nn.RNN(
    input_size=100,     # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_size=256,    # éšè—çŠ¶æ€ç»´åº¦
    num_layers=2,       # RNN å±‚æ•°
    batch_first=True    # è¾“å…¥å½¢çŠ¶ (batch, seq, feature)
)

# å‰å‘ä¼ æ’­
output, hidden = rnn(x)  # x: (batch, seq_len, input_size)
```

### LSTM (é•¿çŸ­æœŸè®°å¿†)

è§£å†³ RNN çš„é•¿æœŸä¾èµ–é—®é¢˜ã€‚

```python
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super(LSTMClassifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 å› ä¸ºåŒå‘

    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)

        lstm_out, (hidden, cell) = self.lstm(embedded)

        # å–æœ€åæ—¶åˆ»çš„è¾“å‡º
        # åŒå‘ LSTMï¼šæ‹¼æ¥ä¸¤ä¸ªæ–¹å‘çš„æœ€åéšè—çŠ¶æ€
        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)

        return self.fc(hidden_cat)
```

### GRU (é—¨æ§å¾ªç¯å•å…ƒ)

LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå‚æ•°æ›´å°‘ã€‚

```python
gru = nn.GRU(
    input_size=100,
    hidden_size=256,
    num_layers=2,
    batch_first=True,
    bidirectional=True
)
```

| æ¨¡å‹ | å‚æ•°é‡ | é•¿æœŸä¾èµ– | è®­ç»ƒé€Ÿåº¦ |
| ---- | ------ | -------- | -------- |
| RNN  | å°‘     | å·®       | å¿«       |
| LSTM | å¤š     | å¥½       | è¾ƒæ…¢     |
| GRU  | ä¸­     | è¾ƒå¥½     | è¾ƒå¿«     |

## Transformer

Transformer åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¯ç°ä»£ NLP å’Œè§†è§‰æ¨¡å‹çš„åŸºç¡€ã€‚

### è‡ªæ³¨æ„åŠ›æœºåˆ¶

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

```python
class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        self.queries = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.values = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)

    def forward(self, x):
        N, seq_len, _ = x.shape

        # çº¿æ€§å˜æ¢
        Q = self.queries(x)
        K = self.keys(x)
        V = self.values(x)

        # é‡å¡‘ä¸ºå¤šå¤´
        Q = Q.view(N, seq_len, self.heads, self.head_dim).transpose(1, 2)
        K = K.view(N, seq_len, self.heads, self.head_dim).transpose(1, 2)
        V = V.view(N, seq_len, self.heads, self.head_dim).transpose(1, 2)

        # æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention = torch.softmax(scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attention, V)
        out = out.transpose(1, 2).contiguous().view(N, seq_len, self.embed_size)

        return self.fc_out(out)
```

### ä½¿ç”¨ PyTorch å†…ç½® Transformer

```python
# Transformer ç¼–ç å™¨å±‚
encoder_layer = nn.TransformerEncoderLayer(
    d_model=512,
    nhead=8,
    dim_feedforward=2048,
    dropout=0.1,
    batch_first=True
)

# å †å å¤šä¸ªç¼–ç å™¨å±‚
transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)

# ä½¿ç”¨
x = torch.randn(32, 100, 512)  # (batch, seq_len, d_model)
output = transformer_encoder(x)
```

## æ·±åº¦å­¦ä¹ å®è·µè¦ç‚¹

### æ•°æ®å¢å¼º

```python
from torchvision import transforms

# å›¾åƒæ•°æ®å¢å¼º
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])
```

### å­¦ä¹ ç‡è°ƒåº¦

```python
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau

# é˜¶æ¢¯ä¸‹é™
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=100)

# è‡ªé€‚åº”ä¸‹é™
scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)

# è®­ç»ƒä¸­ä½¿ç”¨
for epoch in range(epochs):
    train(...)
    val_loss = validate(...)
    scheduler.step(val_loss)  # ReduceLROnPlateau
    # scheduler.step()         # å…¶ä»–è°ƒåº¦å™¨
```

### æ—©åœ (Early Stopping)

```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0
```

### æ¨¡å‹ä¿å­˜ä¸åŠ è½½

```python
# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'model.pth')

# ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}, 'checkpoint.pth')

# åŠ è½½æ¨¡å‹
model.load_state_dict(torch.load('model.pth'))

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

## æ¡†æ¶å¯¹æ¯”

| æ¡†æ¶       | ä¼˜ç‚¹                       | ç¼ºç‚¹         | é€‚ç”¨åœºæ™¯       |
| ---------- | -------------------------- | ------------ | -------------- |
| PyTorch    | åŠ¨æ€å›¾ã€è°ƒè¯•æ–¹ä¾¿ã€ç ”ç©¶å‹å¥½ | éƒ¨ç½²ç¨å¤æ‚   | ç ”ç©¶ã€åŸå‹å¼€å‘ |
| TensorFlow | ç”Ÿäº§éƒ¨ç½²æˆç†Ÿã€TFLite       | å­¦ä¹ æ›²çº¿é™¡å³­ | å¤§è§„æ¨¡ç”Ÿäº§éƒ¨ç½² |
| JAX        | é«˜æ€§èƒ½ã€å‡½æ•°å¼             | ç”Ÿæ€è¾ƒæ–°     | ç§‘å­¦è®¡ç®—ã€ç ”ç©¶ |
| Keras      | ç®€å•æ˜“ç”¨                   | çµæ´»æ€§è¾ƒä½   | å¿«é€ŸåŸå‹ã€æ•™å­¦ |
