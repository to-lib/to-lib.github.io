---
sidebar_position: 27
title: ğŸ¯ å¤šä»»åŠ¡å­¦ä¹ 
---

# å¤šä»»åŠ¡å­¦ä¹ 

å¤šä»»åŠ¡å­¦ä¹ åŒæ—¶å­¦ä¹ å¤šä¸ªç›¸å…³ä»»åŠ¡ï¼Œé€šè¿‡å…±äº«è¡¨ç¤ºæå‡æ³›åŒ–èƒ½åŠ›ã€‚

## æ ¸å¿ƒæ€æƒ³

```mermaid
graph TB
    A[è¾“å…¥] --> B[å…±äº«å±‚]
    B --> C[ä»»åŠ¡1å¤´]
    B --> D[ä»»åŠ¡2å¤´]
    B --> E[ä»»åŠ¡3å¤´]
    C --> F[è¾“å‡º1]
    D --> G[è¾“å‡º2]
    E --> H[è¾“å‡º3]
```

## ç¡¬å‚æ•°å…±äº«

```python
import torch.nn as nn

class HardSharingMTL(nn.Module):
    def __init__(self, input_dim, shared_dim, task_dims):
        super().__init__()
        # å…±äº«å±‚
        self.shared = nn.Sequential(
            nn.Linear(input_dim, shared_dim),
            nn.ReLU(),
            nn.Linear(shared_dim, shared_dim),
            nn.ReLU()
        )

        # ä»»åŠ¡ç‰¹å®šå±‚
        self.task_heads = nn.ModuleList([
            nn.Linear(shared_dim, dim) for dim in task_dims
        ])

    def forward(self, x):
        shared_repr = self.shared(x)
        outputs = [head(shared_repr) for head in self.task_heads]
        return outputs
```

## è½¯å‚æ•°å…±äº«

```python
class SoftSharingMTL(nn.Module):
    def __init__(self, input_dim, hidden_dim, task_dims):
        super().__init__()
        # æ¯ä¸ªä»»åŠ¡æœ‰ç‹¬ç«‹çš„ç½‘ç»œ
        self.task_networks = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, dim)
            ) for dim in task_dims
        ])

    def forward(self, x):
        return [net(x) for net in self.task_networks]

    def regularization_loss(self):
        # é¼“åŠ±å‚æ•°ç›¸ä¼¼
        reg = 0
        for i in range(len(self.task_networks)):
            for j in range(i + 1, len(self.task_networks)):
                for p1, p2 in zip(self.task_networks[i].parameters(),
                                  self.task_networks[j].parameters()):
                    reg += torch.norm(p1 - p2, p=2)
        return reg
```

## æŸå¤±å‡½æ•°æƒé‡

```python
# é™æ€æƒé‡
def static_weighted_loss(losses, weights):
    return sum(w * l for w, l in zip(weights, losses))

# ä¸ç¡®å®šæ€§æƒé‡ (Uncertainty Weighting)
class UncertaintyWeighting(nn.Module):
    def __init__(self, num_tasks):
        super().__init__()
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))

    def forward(self, losses):
        weighted = 0
        for i, loss in enumerate(losses):
            precision = torch.exp(-self.log_vars[i])
            weighted += precision * loss + self.log_vars[i]
        return weighted

# GradNorm
class GradNorm:
    def __init__(self, model, alpha=1.5):
        self.weights = nn.Parameter(torch.ones(num_tasks))
        self.alpha = alpha

    def update_weights(self, losses, shared_params):
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æ¢¯åº¦èŒƒæ•°
        grads = []
        for loss in losses:
            grad = torch.autograd.grad(loss, shared_params, retain_graph=True)
            grads.append(torch.norm(torch.cat([g.flatten() for g in grad])))
        # æ ¹æ®è®­ç»ƒé€Ÿåº¦è°ƒæ•´æƒé‡
        ...
```

## åº”ç”¨åœºæ™¯

| åœºæ™¯     | ä»»åŠ¡ç»„åˆ                       |
| -------- | ------------------------------ |
| è‡ªåŠ¨é©¾é©¶ | ç›®æ ‡æ£€æµ‹ + è¯­ä¹‰åˆ†å‰² + æ·±åº¦ä¼°è®¡ |
| æ¨èç³»ç»Ÿ | ç‚¹å‡»é¢„æµ‹ + è½¬åŒ–é¢„æµ‹ + æ—¶é•¿é¢„æµ‹ |
| NLP      | æƒ…æ„Ÿåˆ†æ + å®ä½“è¯†åˆ« + å…³ç³»æŠ½å– |
| CV       | äººè„¸æ£€æµ‹ + å…³é”®ç‚¹ + å±æ€§è¯†åˆ«   |
