---
sidebar_position: 26
title: ğŸ”„ å¯¹æ¯”å­¦ä¹ 
---

# å¯¹æ¯”å­¦ä¹ ä¸è‡ªç›‘ç£å­¦ä¹ 

å¯¹æ¯”å­¦ä¹ é€šè¿‡æ¯”è¾ƒç›¸ä¼¼å’Œä¸ç›¸ä¼¼çš„æ ·æœ¬æ¥å­¦ä¹ è¡¨ç¤ºï¼Œæ— éœ€äººå·¥æ ‡ç­¾ã€‚

## æ ¸å¿ƒæ€æƒ³

```mermaid
graph LR
    A[åŸå§‹æ ·æœ¬] --> B[å¢å¼º1]
    A --> C[å¢å¼º2]
    B --> D[ç¼–ç å™¨]
    C --> D
    D --> E[æ­£æ ·æœ¬å¯¹æ¥è¿‘]
    D --> F[è´Ÿæ ·æœ¬å¯¹è¿œç¦»]
```

## SimCLR

```python
import torch
import torch.nn as nn
import torchvision.transforms as T

# æ•°æ®å¢å¼º
augmentation = T.Compose([
    T.RandomResizedCrop(224),
    T.RandomHorizontalFlip(),
    T.ColorJitter(0.4, 0.4, 0.4, 0.1),
    T.RandomGrayscale(p=0.2),
    T.GaussianBlur(kernel_size=23),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

class SimCLR(nn.Module):
    def __init__(self, encoder, projection_dim=128):
        super().__init__()
        self.encoder = encoder
        self.projector = nn.Sequential(
            nn.Linear(encoder.output_dim, 512),
            nn.ReLU(),
            nn.Linear(512, projection_dim)
        )

    def forward(self, x):
        h = self.encoder(x)
        z = self.projector(h)
        return nn.functional.normalize(z, dim=1)

# NT-Xent æŸå¤±
def nt_xent_loss(z1, z2, temperature=0.5):
    batch_size = z1.size(0)
    z = torch.cat([z1, z2], dim=0)
    sim = torch.mm(z, z.t()) / temperature

    # æ­£æ ·æœ¬å¯¹çš„ä½ç½®
    pos_mask = torch.zeros(2 * batch_size, 2 * batch_size, dtype=torch.bool)
    pos_mask[:batch_size, batch_size:] = torch.eye(batch_size, dtype=torch.bool)
    pos_mask[batch_size:, :batch_size] = torch.eye(batch_size, dtype=torch.bool)

    # æ’é™¤è‡ªèº«
    self_mask = torch.eye(2 * batch_size, dtype=torch.bool)
    sim.masked_fill_(self_mask, float('-inf'))

    loss = -torch.log(torch.exp(sim[pos_mask]) / torch.exp(sim).sum(dim=1))
    return loss.mean()
```

## MoCo

```python
class MoCo(nn.Module):
    def __init__(self, encoder, dim=128, K=65536, m=0.999, T=0.07):
        super().__init__()
        self.K = K
        self.m = m
        self.T = T

        self.encoder_q = encoder
        self.encoder_k = copy.deepcopy(encoder)

        # å†»ç»“åŠ¨é‡ç¼–ç å™¨
        for param in self.encoder_k.parameters():
            param.requires_grad = False

        # é˜Ÿåˆ—
        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = nn.functional.normalize(self.queue, dim=0)

    @torch.no_grad()
    def momentum_update(self):
        for p_q, p_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            p_k.data = self.m * p_k.data + (1 - self.m) * p_q.data
```

## CLIP

```python
class CLIP(nn.Module):
    def __init__(self, image_encoder, text_encoder, embed_dim):
        super().__init__()
        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        self.temperature = nn.Parameter(torch.ones([]) * 0.07)

    def forward(self, images, texts):
        image_features = self.image_encoder(images)
        text_features = self.text_encoder(texts)

        # å½’ä¸€åŒ–
        image_features = nn.functional.normalize(image_features, dim=-1)
        text_features = nn.functional.normalize(text_features, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        logits = image_features @ text_features.t() / self.temperature
        return logits
```

## åº”ç”¨åœºæ™¯

| æ–¹æ³•   | ç‰¹ç‚¹         | åº”ç”¨                 |
| ------ | ------------ | -------------------- |
| SimCLR | ç®€å•æœ‰æ•ˆ     | å›¾åƒè¡¨ç¤ºé¢„è®­ç»ƒ       |
| MoCo   | å¤§è§„æ¨¡è´Ÿæ ·æœ¬ | è§†è§‰è¡¨ç¤ºå­¦ä¹          |
| CLIP   | å›¾æ–‡å¯¹é½     | é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒæ£€ç´¢ |
| BERT   | æ©ç è¯­è¨€æ¨¡å‹ | NLP é¢„è®­ç»ƒ           |

## ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ

```python
# åŠ è½½é¢„è®­ç»ƒçš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹
pretrained_encoder = load_pretrained()

# æ·»åŠ åˆ†ç±»å¤´
classifier = nn.Sequential(
    pretrained_encoder,
    nn.Linear(encoder_dim, num_classes)
)

# å¾®è°ƒ
for param in pretrained_encoder.parameters():
    param.requires_grad = False  # å†»ç»“ç¼–ç å™¨ï¼Œæˆ–è®¾ä¸º True è¿›è¡Œå…¨é‡å¾®è°ƒ
```
