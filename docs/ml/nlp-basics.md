---
sidebar_position: 21
title: ğŸ“ NLP åŸºç¡€
---

# è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€

NLP è®©è®¡ç®—æœºç†è§£å’Œå¤„ç†äººç±»è¯­è¨€ã€‚

## æ–‡æœ¬é¢„å¤„ç†

```python
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

def preprocess_text(text):
    # å°å†™åŒ–
    text = text.lower()
    # å»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # åˆ†è¯
    tokens = word_tokenize(text)
    # å»åœç”¨è¯
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    # è¯å½¢è¿˜åŸ
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return tokens
```

## æ–‡æœ¬è¡¨ç¤º

### TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(texts)
```

### Word2Vec

```python
from gensim.models import Word2Vec

# è®­ç»ƒè¯å‘é‡
sentences = [['I', 'love', 'NLP'], ['NLP', 'is', 'fun']]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# è·å–è¯å‘é‡
vector = model.wv['NLP']

# ç›¸ä¼¼è¯
similar = model.wv.most_similar('NLP', topn=5)
```

### é¢„è®­ç»ƒè¯å‘é‡

```python
import gensim.downloader as api

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
glove = api.load('glove-wiki-gigaword-100')
vector = glove['king']
```

## æ–‡æœ¬åˆ†ç±»

```python
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

text_clf = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', LogisticRegression())
])

text_clf.fit(X_train, y_train)
```

### ä½¿ç”¨ BERT

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

inputs = tokenizer("I love this movie!", return_tensors='pt', padding=True, truncation=True)
outputs = model(**inputs)
predictions = torch.argmax(outputs.logits, dim=1)
```

## å‘½åå®ä½“è¯†åˆ« (NER)

```python
from transformers import pipeline

ner = pipeline('ner', grouped_entities=True)
result = ner("Apple was founded by Steve Jobs in California.")
# [{'entity_group': 'ORG', 'word': 'Apple'},
#  {'entity_group': 'PER', 'word': 'Steve Jobs'},
#  {'entity_group': 'LOC', 'word': 'California'}]
```

## æ–‡æœ¬ç›¸ä¼¼åº¦

```python
from sklearn.metrics.pairwise import cosine_similarity

# TF-IDF ç›¸ä¼¼åº¦
tfidf = TfidfVectorizer()
vectors = tfidf.fit_transform(texts)
similarity = cosine_similarity(vectors[0:1], vectors[1:2])

# Sentence Transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(['text1', 'text2'])
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])
```

## å¸¸ç”¨ä»»åŠ¡

| ä»»åŠ¡     | æè¿°               | æ¨¡å‹          |
| -------- | ------------------ | ------------- |
| æ–‡æœ¬åˆ†ç±» | æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±» | BERT, RoBERTa |
| NER      | è¯†åˆ«å®ä½“           | BERT-NER      |
| é—®ç­”     | é˜…è¯»ç†è§£           | BERT-QA       |
| æ–‡æœ¬ç”Ÿæˆ | å†™ä½œã€å¯¹è¯         | GPT           |
| ç¿»è¯‘     | æœºå™¨ç¿»è¯‘           | MarianMT      |
| æ‘˜è¦     | æ–‡æœ¬æ‘˜è¦           | BART, T5      |
