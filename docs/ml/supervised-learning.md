---
sidebar_position: 5
title: ğŸ“Š ç›‘ç£å­¦ä¹ 
---

# ç›‘ç£å­¦ä¹ ç®—æ³•

ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¸¸è§çš„èŒƒå¼ï¼Œä»å¸¦æ ‡ç­¾çš„æ•°æ®ä¸­å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚

## å›å½’ç®—æ³•

### çº¿æ€§å›å½’ (Linear Regression)

å¯»æ‰¾æœ€ä½³æ‹Ÿåˆç›´çº¿/è¶…å¹³é¢ï¼Œä½¿é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„è¯¯å·®å¹³æ–¹å’Œæœ€å°ã€‚

$$
\hat{y} = \mathbf{w}^T \mathbf{x} + b
$$

$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X = np.random.randn(100, 1) * 10
y = 3 * X.flatten() + 5 + np.random.randn(100) * 2

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
y_pred = model.predict(X_test)
print(f"ç³»æ•°: {model.coef_[0]:.2f}, æˆªè·: {model.intercept_:.2f}")
print(f"MSE: {mean_squared_error(y_test, y_pred):.2f}")
print(f"RÂ²: {r2_score(y_test, y_pred):.2f}")
```

**æ­£åˆ™åŒ–**ï¼š

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge (L2 æ­£åˆ™åŒ–)
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso (L1 æ­£åˆ™åŒ–) - å¯äº§ç”Ÿç¨€ç–è§£
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# ElasticNet (L1 + L2)
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)
```

| æ–¹æ³•             | æ­£åˆ™åŒ–  | ç‰¹ç‚¹           | é€‚ç”¨åœºæ™¯     |
| ---------------- | ------- | -------------- | ------------ |
| LinearRegression | æ—       | ç®€å•å¿«é€Ÿ       | å°æ•°æ®é›†     |
| Ridge            | L2      | ç¼“è§£å¤šé‡å…±çº¿æ€§ | ç‰¹å¾ç›¸å…³æ€§é«˜ |
| Lasso            | L1      | ç‰¹å¾é€‰æ‹©       | é«˜ç»´ç¨€ç–ç‰¹å¾ |
| ElasticNet       | L1 + L2 | ç»“åˆä¸¤è€…ä¼˜ç‚¹   | ç‰¹å¾åˆ†ç»„     |

### å¤šé¡¹å¼å›å½’

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# äºŒæ¬¡å¤šé¡¹å¼å›å½’
poly_model = make_pipeline(
    PolynomialFeatures(degree=2),
    LinearRegression()
)
poly_model.fit(X_train, y_train)
```

## åˆ†ç±»ç®—æ³•

### é€»è¾‘å›å½’ (Logistic Regression)

è™½ç„¶åå­—å«"å›å½’"ï¼Œä½†å®é™…ç”¨äºåˆ†ç±»ã€‚ä½¿ç”¨ Sigmoid å‡½æ•°å°†è¾“å‡ºæ˜ å°„åˆ° [0, 1] åŒºé—´ã€‚

$$
P(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}
$$

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score, classification_report

# åŠ è½½æ•°æ®
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# è®­ç»ƒæ¨¡å‹
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

# é¢„æµ‹
y_pred = lr.predict(X_test)
y_prob = lr.predict_proba(X_test)[:, 1]

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.2%}")
print(classification_report(y_test, y_pred, target_names=data.target_names))
```

### å†³ç­–æ ‘ (Decision Tree)

é€šè¿‡é€’å½’åœ°é€‰æ‹©æœ€ä¼˜ç‰¹å¾è¿›è¡Œåˆ†è£‚ï¼Œæ„å»ºæ ‘å½¢ç»“æ„ã€‚

```mermaid
graph TB
    A[æ ¹èŠ‚ç‚¹: å¹´é¾„ < 30?] -->|æ˜¯| B[æ”¶å…¥ > 50k?]
    A -->|å¦| C[ä¿¡ç”¨è¯„åˆ† > 700?]
    B -->|æ˜¯| D[æ‰¹å‡†è´·æ¬¾]
    B -->|å¦| E[æ‹’ç»è´·æ¬¾]
    C -->|æ˜¯| F[æ‰¹å‡†è´·æ¬¾]
    C -->|å¦| G[æ‹’ç»è´·æ¬¾]
```

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# è®­ç»ƒå†³ç­–æ ‘
dt = DecisionTreeClassifier(
    max_depth=5,           # æœ€å¤§æ·±åº¦
    min_samples_split=10,  # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=5,    # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    random_state=42
)
dt.fit(X_train, y_train)

# å¯è§†åŒ–
plt.figure(figsize=(20, 10))
plot_tree(dt, feature_names=data.feature_names, class_names=data.target_names, filled=True)
plt.show()

# ç‰¹å¾é‡è¦æ€§
importance = pd.DataFrame({
    'feature': data.feature_names,
    'importance': dt.feature_importances_
}).sort_values('importance', ascending=False)
```

**åˆ†è£‚æ ‡å‡†**ï¼š

| æ ‡å‡†        | å…¬å¼                             | é€‚ç”¨åœºæ™¯     |
| ----------- | -------------------------------- | ------------ |
| Gini ä¸çº¯åº¦ | $1 - \sum p_i^2$                 | åˆ†ç±»ï¼ˆé»˜è®¤ï¼‰ |
| ä¿¡æ¯ç†µ      | $-\sum p_i \log p_i$             | åˆ†ç±»         |
| MSE         | $\frac{1}{n}\sum(y_i-\bar{y})^2$ | å›å½’         |

### éšæœºæ£®æ— (Random Forest)

å¤šæ£µå†³ç­–æ ‘çš„é›†æˆï¼Œé€šè¿‡æŠ•ç¥¨æˆ–å¹³å‡å¾—å‡ºæœ€ç»ˆç»“æœã€‚

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=10,          # å•æ£µæ ‘æœ€å¤§æ·±åº¦
    min_samples_split=5,
    max_features='sqrt',   # æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„ç‰¹å¾æ•°
    n_jobs=-1,             # å¹¶è¡Œè®¡ç®—
    random_state=42
)
rf.fit(X_train, y_train)

# è¯„ä¼°
print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {rf.score(X_train, y_train):.2%}")
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {rf.score(X_test, y_test):.2%}")

# OOB (Out-of-Bag) åˆ†æ•°
rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)
rf_oob.fit(X_train, y_train)
print(f"OOB åˆ†æ•°: {rf_oob.oob_score_:.2%}")
```

### æ”¯æŒå‘é‡æœº (SVM)

å¯»æ‰¾æœ€å¤§é—´éš”è¶…å¹³é¢æ¥åˆ†å‰²æ•°æ®ã€‚

```python
from sklearn.svm import SVC, SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# åˆ†ç±»
svc = make_pipeline(
    StandardScaler(),
    SVC(kernel='rbf', C=1.0, gamma='scale')
)
svc.fit(X_train, y_train)
print(f"SVM å‡†ç¡®ç‡: {svc.score(X_test, y_test):.2%}")

# å›å½’
svr = make_pipeline(
    StandardScaler(),
    SVR(kernel='rbf', C=1.0, epsilon=0.1)
)
```

**æ ¸å‡½æ•°**ï¼š

| æ ¸å‡½æ•°     | é€‚ç”¨åœºæ™¯     | å‚æ•°         |
| ---------- | ------------ | ------------ |
| linear     | çº¿æ€§å¯åˆ†     | -            |
| poly       | å¤šé¡¹å¼è¾¹ç•Œ   | degree       |
| rbf (é»˜è®¤) | éçº¿æ€§ï¼Œé€šç”¨ | gamma        |
| sigmoid    | ç±»ä¼¼ç¥ç»ç½‘ç»œ | gamma, coef0 |

### K è¿‘é‚» (KNN)

åŸºäº"ç‰©ä»¥ç±»èš"çš„åŸç†ï¼Œæ ¹æ®æœ€è¿‘çš„ K ä¸ªé‚»å±…è¿›è¡Œé¢„æµ‹ã€‚

```python
from sklearn.neighbors import KNeighborsClassifier

# é€‰æ‹©æœ€ä¼˜ K å€¼
from sklearn.model_selection import cross_val_score

k_range = range(1, 31)
k_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    k_scores.append(scores.mean())

best_k = k_range[np.argmax(k_scores)]
print(f"æœ€ä¼˜ K å€¼: {best_k}")

# ä½¿ç”¨æœ€ä¼˜ K è®­ç»ƒ
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)
print(f"KNN å‡†ç¡®ç‡: {knn.score(X_test, y_test):.2%}")
```

### æ¢¯åº¦æå‡ (Gradient Boosting)

```python
from sklearn.ensemble import GradientBoostingClassifier

# sklearn å®ç°
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gb.fit(X_train, y_train)

# XGBoost (æ¨è)
import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    use_label_encoder=False,
    eval_metric='logloss'
)
xgb_model.fit(X_train, y_train)

# LightGBM (å¤§è§„æ¨¡æ•°æ®æ¨è)
import lightgbm as lgb

lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
lgb_model.fit(X_train, y_train)
```

## ç®—æ³•é€‰æ‹©æŒ‡å—

```mermaid
graph TD
    A[å¼€å§‹] --> B{æ•°æ®é‡}
    B -->|å° < 10K| C{ç‰¹å¾ç±»å‹}
    B -->|å¤§ > 10K| D[XGBoost/LightGBM]
    C -->|çº¿æ€§å¯åˆ†| E[é€»è¾‘å›å½’/SVM]
    C -->|éçº¿æ€§| F[éšæœºæ£®æ—/SVM-RBF]
    D --> G{éœ€è¦è§£é‡Šæ€§?}
    G -->|æ˜¯| H[LightGBM + SHAP]
    G -->|å¦| I[XGBoost]
```

| ç®—æ³•              | ä¼˜ç‚¹                 | ç¼ºç‚¹             | é€‚ç”¨åœºæ™¯                 |
| ----------------- | -------------------- | ---------------- | ------------------------ |
| çº¿æ€§å›å½’/é€»è¾‘å›å½’ | ç®€å•ã€å¯è§£é‡Šã€å¿«é€Ÿ   | åªèƒ½æ‹Ÿåˆçº¿æ€§å…³ç³» | åŸºçº¿æ¨¡å‹ã€å¯è§£é‡Šæ€§è¦æ±‚é«˜ |
| å†³ç­–æ ‘            | å¯è§£é‡Šã€æ— éœ€ç‰¹å¾ç¼©æ”¾ | æ˜“è¿‡æ‹Ÿåˆ         | ç†è§£æ•°æ®ã€ç‰¹å¾é€‰æ‹©       |
| éšæœºæ£®æ—          | ä¸æ˜“è¿‡æ‹Ÿåˆã€å¹¶è¡Œ     | å†…å­˜å ç”¨å¤§       | é€šç”¨åˆ†ç±»/å›å½’            |
| SVM               | é«˜ç»´æœ‰æ•ˆã€æ³›åŒ–å¥½     | å¤§æ•°æ®æ…¢ã€éœ€è°ƒå‚ | å°ä¸­å‹æ•°æ®ã€æ–‡æœ¬åˆ†ç±»     |
| KNN               | ç®€å•ã€æ— è®­ç»ƒ         | é¢„æµ‹æ…¢ã€ç»´åº¦ç¾éš¾ | å°æ•°æ®é›†ã€æ¨èç³»ç»Ÿ       |
| XGBoost/LightGBM  | æ€§èƒ½å¼ºã€é€Ÿåº¦å¿«       | éœ€è°ƒå‚           | ç«èµ›ã€ç”Ÿäº§ç¯å¢ƒ           |
