---
sidebar_position: 19
title: ğŸ® å¼ºåŒ–å­¦ä¹ 
---

# å¼ºåŒ–å­¦ä¹ 

å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ç¬¬ä¸‰å¤§èŒƒå¼ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

```mermaid
graph LR
    A[Agent] -->|Action a| B[Environment]
    B -->|State s| A
    B -->|Reward r| A
```

| æ¦‚å¿µ     | ç¬¦å·    | æè¿°               |
| -------- | ------- | ------------------ |
| çŠ¶æ€     | s       | ç¯å¢ƒçš„å½“å‰æƒ…å†µ     |
| åŠ¨ä½œ     | a       | æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œ |
| å¥–åŠ±     | r       | ç¯å¢ƒåé¦ˆçš„å³æ—¶ä¿¡å· |
| ç­–ç•¥     | Ï€(a\|s) | ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ |
| ä»·å€¼å‡½æ•° | V(s)    | çŠ¶æ€çš„é•¿æœŸä»·å€¼     |
| Q å‡½æ•°   | Q(s,a)  | çŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼  |

## é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

$$
V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s\right]
$$

- **Î³ (gamma)**: æŠ˜æ‰£å› å­ï¼Œ0 < Î³ â‰¤ 1

## ç»å…¸ç®—æ³•

### Q-Learning

```python
import numpy as np

class QLearning:
    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.99, epsilon=0.1):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon

    def choose_action(self, state):
        if np.random.random() < self.epsilon:
            return np.random.randint(len(self.q_table[state]))
        return np.argmax(self.q_table[state])

    def update(self, state, action, reward, next_state):
        # Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
        best_next = np.max(self.q_table[next_state])
        td_target = reward + self.gamma * best_next
        td_error = td_target - self.q_table[state, action]
        self.q_table[state, action] += self.lr * td_error
```

### SARSA

```python
def sarsa_update(self, s, a, r, s_next, a_next):
    # Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
    td_target = r + self.gamma * self.q_table[s_next, a_next]
    self.q_table[s, a] += self.lr * (td_target - self.q_table[s, a])
```

| ç®—æ³•       | ç±»å‹       | ç‰¹ç‚¹         |
| ---------- | ---------- | ------------ |
| Q-Learning | Off-policy | å­¦ä¹ æœ€ä¼˜ç­–ç•¥ |
| SARSA      | On-policy  | å­¦ä¹ å½“å‰ç­–ç•¥ |

## æ·±åº¦å¼ºåŒ–å­¦ä¹ 

### DQN

```python
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.net(x)

# ç»éªŒå›æ”¾
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = []
        self.capacity = capacity

    def push(self, transition):
        if len(self.buffer) >= self.capacity:
            self.buffer.pop(0)
        self.buffer.append(transition)

    def sample(self, batch_size):
        indices = np.random.choice(len(self.buffer), batch_size)
        return [self.buffer[i] for i in indices]
```

### Policy Gradient

```python
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.net(x)

# REINFORCE ç®—æ³•
def reinforce_loss(log_probs, rewards, gamma=0.99):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-8)
    return -torch.sum(log_probs * returns)
```

### Actor-Critic

```python
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.shared = nn.Linear(state_dim, 128)
        self.actor = nn.Linear(128, action_dim)
        self.critic = nn.Linear(128, 1)

    def forward(self, x):
        x = torch.relu(self.shared(x))
        policy = torch.softmax(self.actor(x), dim=-1)
        value = self.critic(x)
        return policy, value
```

## å¸¸ç”¨åº“

```python
import gymnasium as gym

# åˆ›å»ºç¯å¢ƒ
env = gym.make('CartPole-v1')

state, _ = env.reset()
for _ in range(1000):
    action = env.action_space.sample()  # éšæœºåŠ¨ä½œ
    next_state, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        break
```

## ç®—æ³•åˆ†ç±»

| ç±»å‹         | ç®—æ³•            | ç‰¹ç‚¹         |
| ------------ | --------------- | ------------ |
| Value-based  | DQN, Double DQN | å­¦ä¹  Q å‡½æ•°  |
| Policy-based | REINFORCE, A2C  | ç›´æ¥å­¦ä¹ ç­–ç•¥ |
| Actor-Critic | PPO, SAC, TD3   | ç»“åˆä¸¤è€…     |
| Model-based  | MuZero, Dreamer | å­¦ä¹ ç¯å¢ƒæ¨¡å‹ |
