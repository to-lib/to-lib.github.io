---
sidebar_position: 17
title: ğŸš€ æ¨¡å‹éƒ¨ç½²
---

# æ¨¡å‹éƒ¨ç½² (MLOps)

å°†è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæä¾›å®æ—¶æ¨ç†æœåŠ¡ã€‚

## æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### scikit-learn

```python
import joblib

# ä¿å­˜
joblib.dump(model, 'model.joblib')

# åŠ è½½
model = joblib.load('model.joblib')
```

### PyTorch

```python
import torch

# ä¿å­˜æ¨¡å‹æƒé‡
torch.save(model.state_dict(), 'model.pth')

# åŠ è½½
model = MyModel()
model.load_state_dict(torch.load('model.pth'))
model.eval()

# TorchScript (ç”Ÿäº§æ¨è)
scripted = torch.jit.script(model)
scripted.save('model_scripted.pt')
```

### ONNX (è·¨æ¡†æ¶)

```python
import torch.onnx

# PyTorch â†’ ONNX
dummy_input = torch.randn(1, 10)
torch.onnx.export(model, dummy_input, 'model.onnx')

# ä½¿ç”¨ ONNX Runtime
import onnxruntime as ort
session = ort.InferenceSession('model.onnx')
result = session.run(None, {'input': input_data})
```

## FastAPI æœåŠ¡

```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib

app = FastAPI()
model = joblib.load('model.joblib')

class PredictRequest(BaseModel):
    features: list[float]

class PredictResponse(BaseModel):
    prediction: float
    probability: list[float] = None

@app.post('/predict', response_model=PredictResponse)
def predict(request: PredictRequest):
    X = [request.features]
    pred = model.predict(X)[0]
    proba = model.predict_proba(X)[0].tolist() if hasattr(model, 'predict_proba') else None
    return PredictResponse(prediction=pred, probability=proba)

# è¿è¡Œ: uvicorn app:app --host 0.0.0.0 --port 8000
```

## Docker å®¹å™¨åŒ–

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY model.joblib .
COPY app.py .

EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# æ„å»ºå¹¶è¿è¡Œ
docker build -t ml-service .
docker run -p 8000:8000 ml-service
```

## æ‰¹é‡é¢„æµ‹

```python
import pandas as pd
from multiprocessing import Pool

def predict_batch(df, model_path, batch_size=1000):
    model = joblib.load(model_path)
    results = []

    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        preds = model.predict(batch)
        results.extend(preds)

    return results

# å¹¶è¡Œæ‰¹å¤„ç†
def parallel_predict(df, n_workers=4):
    chunks = np.array_split(df, n_workers)
    with Pool(n_workers) as p:
        results = p.map(predict_chunk, chunks)
    return np.concatenate(results)
```

## æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹é‡åŒ–

```python
# PyTorch åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

### æ¨¡å‹è’¸é¦

```python
# ç”¨å¤§æ¨¡å‹æŒ‡å¯¼è®­ç»ƒå°æ¨¡å‹
def distillation_loss(student_logits, teacher_logits, labels, T=3, alpha=0.5):
    soft_loss = nn.KLDivLoss()(
        F.log_softmax(student_logits / T, dim=1),
        F.softmax(teacher_logits / T, dim=1)
    ) * T * T
    hard_loss = nn.CrossEntropyLoss()(student_logits, labels)
    return alpha * soft_loss + (1 - alpha) * hard_loss
```

## ç›‘æ§ä¸å‘Šè­¦

```python
from prometheus_client import Counter, Histogram, start_http_server

# å®šä¹‰æŒ‡æ ‡
prediction_counter = Counter('predictions_total', 'é¢„æµ‹æ€»æ•°')
latency_histogram = Histogram('prediction_latency_seconds', 'é¢„æµ‹å»¶è¿Ÿ')

@app.post('/predict')
def predict(request: PredictRequest):
    with latency_histogram.time():
        result = model.predict([request.features])
    prediction_counter.inc()
    return {'prediction': result[0]}

# å¯åŠ¨ Prometheus æŒ‡æ ‡ç«¯ç‚¹
start_http_server(8001)
```

## A/B æµ‹è¯•

```python
import random

models = {
    'A': joblib.load('model_a.joblib'),
    'B': joblib.load('model_b.joblib')
}

@app.post('/predict')
def predict(request: PredictRequest, user_id: str):
    # æ ¹æ®ç”¨æˆ· ID åˆ†æµ
    variant = 'A' if hash(user_id) % 100 < 50 else 'B'
    model = models[variant]
    result = model.predict([request.features])[0]

    # è®°å½•å®éªŒæ•°æ®
    log_experiment(user_id, variant, result)

    return {'prediction': result, 'variant': variant}
```

## MLOps å·¥å…·

| å·¥å…·             | ç”¨é€”               |
| ---------------- | ------------------ |
| MLflow           | å®éªŒè¿½è¸ªã€æ¨¡å‹æ³¨å†Œ |
| DVC              | æ•°æ®ç‰ˆæœ¬æ§åˆ¶       |
| Kubeflow         | K8s ä¸Šçš„ ML æµæ°´çº¿ |
| BentoML          | æ¨¡å‹æ‰“åŒ…éƒ¨ç½²       |
| Seldon           | K8s æ¨¡å‹æœåŠ¡       |
| Weights & Biases | å®éªŒè¿½è¸ªå¯è§†åŒ–     |
