---
sidebar_position: 31
title: ğŸ­ å¤šæ¨¡æ€å­¦ä¹ 
---

# å¤šæ¨¡æ€å­¦ä¹ 

å¤šæ¨¡æ€å­¦ä¹ å¤„ç†å’Œèåˆæ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰ï¼‰çš„æ•°æ®ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

```mermaid
graph TB
    A[å›¾åƒ] --> D[ç‰¹å¾æå–]
    B[æ–‡æœ¬] --> E[ç‰¹å¾æå–]
    C[éŸ³é¢‘] --> F[ç‰¹å¾æå–]
    D --> G[èåˆ]
    E --> G
    F --> G
    G --> H[è¾“å‡º]
```

## èåˆç­–ç•¥

### æ—©æœŸèåˆ

```python
import torch
import torch.nn as nn

class EarlyFusion(nn.Module):
    def __init__(self, img_dim, text_dim, hidden_dim, num_classes):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(img_dim + text_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, img_feat, text_feat):
        combined = torch.cat([img_feat, text_feat], dim=1)
        return self.fc(combined)
```

### æ™šæœŸèåˆ

```python
class LateFusion(nn.Module):
    def __init__(self, img_dim, text_dim, hidden_dim, num_classes):
        super().__init__()
        self.img_branch = nn.Linear(img_dim, num_classes)
        self.text_branch = nn.Linear(text_dim, num_classes)

    def forward(self, img_feat, text_feat):
        img_out = self.img_branch(img_feat)
        text_out = self.text_branch(text_feat)
        return (img_out + text_out) / 2
```

### æ³¨æ„åŠ›èåˆ

```python
class AttentionFusion(nn.Module):
    def __init__(self, img_dim, text_dim, hidden_dim):
        super().__init__()
        self.img_proj = nn.Linear(img_dim, hidden_dim)
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8)

    def forward(self, img_feat, text_feat):
        img_proj = self.img_proj(img_feat)
        text_proj = self.text_proj(text_feat)

        # äº¤å‰æ³¨æ„åŠ›
        attn_out, _ = self.attention(img_proj, text_proj, text_proj)
        return attn_out
```

## è§†è§‰-è¯­è¨€æ¨¡å‹

### CLIP ä½¿ç”¨

```python
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å›¾æ–‡åŒ¹é…
inputs = processor(text=["a cat", "a dog"], images=image, return_tensors="pt")
outputs = model(**inputs)
similarity = outputs.logits_per_image  # å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦
```

### BLIP (å›¾åƒæè¿°)

```python
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

inputs = processor(image, return_tensors="pt")
caption = model.generate(**inputs)
```

## åº”ç”¨åœºæ™¯

| ä»»åŠ¡           | æ¨¡æ€             | æ¨¡å‹          |
| -------------- | ---------------- | ------------- |
| å›¾åƒæè¿°       | å›¾åƒ â†’ æ–‡æœ¬      | BLIP, GIT     |
| è§†è§‰é—®ç­” (VQA) | å›¾åƒ+æ–‡æœ¬ â†’ æ–‡æœ¬ | BLIP-2, LLaVA |
| å›¾æ–‡æ£€ç´¢       | å›¾åƒ â†” æ–‡æœ¬      | CLIP          |
| è§†é¢‘ç†è§£       | è§†é¢‘+éŸ³é¢‘ â†’ æ–‡æœ¬ | VideoMAE      |
| è¯­éŸ³è¯†åˆ«       | éŸ³é¢‘ â†’ æ–‡æœ¬      | Whisper       |
