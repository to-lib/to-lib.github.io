---
sidebar_position: 46
title: ğŸŒ é¢†åŸŸè‡ªé€‚åº”
---

# é¢†åŸŸè‡ªé€‚åº”

é¢†åŸŸè‡ªé€‚åº”è§£å†³æºåŸŸå’Œç›®æ ‡åŸŸåˆ†å¸ƒä¸åŒçš„é—®é¢˜ã€‚

## é—®é¢˜å®šä¹‰

```mermaid
graph LR
    A[æºåŸŸ Labeled] --> B[æ¨¡å‹]
    C[ç›®æ ‡åŸŸ Unlabeled] --> B
    B --> D[ç›®æ ‡åŸŸé¢„æµ‹]
```

åŸŸåç§»ç±»å‹ï¼š

- **åå˜é‡åç§»**: P(X) ä¸åŒ
- **æ ‡ç­¾åç§»**: P(Y) ä¸åŒ
- **æ¦‚å¿µåç§»**: P(Y|X) ä¸åŒ

## æ–¹æ³•åˆ†ç±»

### åŸºäºå·®å¼‚çš„æ–¹æ³• (MMD)

```python
import torch

def mmd_loss(source_features, target_features):
    """Maximum Mean Discrepancy"""
    def rbf_kernel(x, y, sigma=1.0):
        diff = x.unsqueeze(1) - y.unsqueeze(0)
        return torch.exp(-torch.sum(diff ** 2, dim=-1) / (2 * sigma ** 2))

    K_ss = rbf_kernel(source_features, source_features)
    K_tt = rbf_kernel(target_features, target_features)
    K_st = rbf_kernel(source_features, target_features)

    return K_ss.mean() + K_tt.mean() - 2 * K_st.mean()
```

### å¯¹æŠ—åŸŸè‡ªé€‚åº” (DANN)

```python
from torch.autograd import Function

class GradientReversal(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.alpha * grad_output, None

class DANN(nn.Module):
    def __init__(self, feature_extractor, classifier, domain_classifier):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.classifier = classifier
        self.domain_classifier = domain_classifier

    def forward(self, x, alpha=1.0):
        features = self.feature_extractor(x)
        class_output = self.classifier(features)

        # æ¢¯åº¦åè½¬
        reversed_features = GradientReversal.apply(features, alpha)
        domain_output = self.domain_classifier(reversed_features)

        return class_output, domain_output
```

### CORAL

```python
def coral_loss(source, target):
    """ç›¸å…³æ€§å¯¹é½"""
    d = source.size(1)

    source_cov = torch.mm(source.t(), source) / (source.size(0) - 1)
    target_cov = torch.mm(target.t(), target) / (target.size(0) - 1)

    loss = torch.norm(source_cov - target_cov, p='fro') ** 2
    return loss / (4 * d * d)
```

## è®­ç»ƒæµç¨‹

```python
def train_dann(model, source_loader, target_loader, epochs):
    for epoch in range(epochs):
        for (src_data, src_labels), (tgt_data, _) in zip(source_loader, target_loader):
            # åŠ¨æ€è°ƒæ•´ alpha
            alpha = 2.0 / (1 + np.exp(-10 * epoch / epochs)) - 1

            # æºåŸŸ
            class_out, domain_out = model(src_data, alpha)
            class_loss = F.cross_entropy(class_out, src_labels)
            domain_loss_src = F.binary_cross_entropy(domain_out, torch.zeros(len(src_data)))

            # ç›®æ ‡åŸŸ
            _, domain_out = model(tgt_data, alpha)
            domain_loss_tgt = F.binary_cross_entropy(domain_out, torch.ones(len(tgt_data)))

            loss = class_loss + domain_loss_src + domain_loss_tgt
```

## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³•  | æ€æƒ³           | ä¼˜ç‚¹     |
| ----- | -------------- | -------- |
| MMD   | æœ€å°åŒ–åˆ†å¸ƒè·ç¦» | ç®€å•ç¨³å®š |
| DANN  | å¯¹æŠ—å­¦ä¹        | æ•ˆæœå¥½   |
| CORAL | å¯¹é½åæ–¹å·®     | è®¡ç®—é«˜æ•ˆ |
