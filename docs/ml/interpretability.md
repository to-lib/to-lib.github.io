---
sidebar_position: 18
title: ğŸ” å¯è§£é‡Šæ€§
---

# æ¨¡å‹å¯è§£é‡Šæ€§

ç†è§£æ¨¡å‹ä¸ºä»€ä¹ˆåšå‡ºè¿™æ ·çš„é¢„æµ‹ï¼Œå¯¹äºæ¨¡å‹è°ƒè¯•å’Œä¸šåŠ¡ä¿¡ä»»è‡³å…³é‡è¦ã€‚

## ç‰¹å¾é‡è¦æ€§

### æ ‘æ¨¡å‹å†…ç½®é‡è¦æ€§

```python
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# ç‰¹å¾é‡è¦æ€§
importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# å¯è§†åŒ–
plt.barh(importance['feature'][:10], importance['importance'][:10])
plt.xlabel('é‡è¦æ€§')
plt.title('ç‰¹å¾é‡è¦æ€§ Top 10')
```

### æ’åˆ—é‡è¦æ€§

```python
from sklearn.inspection import permutation_importance

result = permutation_importance(model, X_test, y_test, n_repeats=10)

importance = pd.DataFrame({
    'feature': feature_names,
    'importance': result.importances_mean,
    'std': result.importances_std
}).sort_values('importance', ascending=False)
```

## SHAP

SHAP (SHapley Additive exPlanations) åŸºäºåšå¼ˆè®ºï¼Œä¸ºæ¯ä¸ªç‰¹å¾åˆ†é…è´¡çŒ®å€¼ã€‚

```python
import shap

# åˆ›å»ºè§£é‡Šå™¨
explainer = shap.TreeExplainer(model)  # æ ‘æ¨¡å‹
# explainer = shap.KernelExplainer(model.predict, X_train[:100])  # é€šç”¨

# è®¡ç®— SHAP å€¼
shap_values = explainer.shap_values(X_test)
```

### å…¨å±€è§£é‡Š

```python
# ç‰¹å¾é‡è¦æ€§æ±‡æ€»å›¾
shap.summary_plot(shap_values, X_test, feature_names=feature_names)

# æ¡å½¢å›¾
shap.summary_plot(shap_values, X_test, plot_type='bar')
```

### å•æ ·æœ¬è§£é‡Š

```python
# ç€‘å¸ƒå›¾ - è§£é‡Šå•ä¸ªé¢„æµ‹
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=X_test.iloc[0],
    feature_names=feature_names
))

# åŠ›å›¾
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
```

### ç‰¹å¾äº¤äº’

```python
# ä¾èµ–å›¾
shap.dependence_plot('feature_name', shap_values, X_test)
```

## LIME

LIME (Local Interpretable Model-agnostic Explanations) ç”¨ç®€å•æ¨¡å‹å±€éƒ¨è¿‘ä¼¼å¤æ‚æ¨¡å‹ã€‚

```python
from lime.lime_tabular import LimeTabularExplainer

explainer = LimeTabularExplainer(
    X_train.values,
    feature_names=feature_names,
    class_names=['è´Ÿ', 'æ­£'],
    mode='classification'
)

# è§£é‡Šå•ä¸ªé¢„æµ‹
exp = explainer.explain_instance(
    X_test.iloc[0].values,
    model.predict_proba,
    num_features=10
)

exp.show_in_notebook()
# æˆ–ä¿å­˜ä¸º HTML
exp.save_to_file('explanation.html')
```

## éƒ¨åˆ†ä¾èµ–å›¾ (PDP)

å±•ç¤ºç‰¹å¾ä¸é¢„æµ‹çš„è¾¹é™…æ•ˆåº”ã€‚

```python
from sklearn.inspection import PartialDependenceDisplay

# å•ç‰¹å¾ PDP
PartialDependenceDisplay.from_estimator(model, X_train, ['feature1', 'feature2'])
plt.show()

# åŒç‰¹å¾äº¤äº’ PDP
PartialDependenceDisplay.from_estimator(
    model, X_train, [('feature1', 'feature2')]
)
```

## ICE å›¾

Individual Conditional Expectation - æ¯ä¸ªæ ·æœ¬çš„ PDPã€‚

```python
from sklearn.inspection import PartialDependenceDisplay

PartialDependenceDisplay.from_estimator(
    model, X_train, ['feature1'],
    kind='both'  # åŒæ—¶æ˜¾ç¤º PDP å’Œ ICE
)
```

## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³•       | ä½œç”¨èŒƒå›´  | æ¨¡å‹æ— å…³    | è®¡ç®—é€Ÿåº¦ |
| ---------- | --------- | ----------- | -------- |
| ç‰¹å¾é‡è¦æ€§ | å…¨å±€      | å¦ (æ ‘æ¨¡å‹) | å¿«       |
| æ’åˆ—é‡è¦æ€§ | å…¨å±€      | æ˜¯          | ä¸­       |
| SHAP       | å…¨å±€+å±€éƒ¨ | æ˜¯          | æ…¢       |
| LIME       | å±€éƒ¨      | æ˜¯          | ä¸­       |
| PDP        | å…¨å±€      | æ˜¯          | ä¸­       |

## æœ€ä½³å®è·µ

1. **ä»ç‰¹å¾é‡è¦æ€§å¼€å§‹**ï¼šå¿«é€Ÿäº†è§£å“ªäº›ç‰¹å¾é‡è¦
2. **ç”¨ SHAP æ·±å…¥åˆ†æ**ï¼šç†è§£ç‰¹å¾å¦‚ä½•å½±å“é¢„æµ‹
3. **ç”¨ LIME è§£é‡Šä¸ªä¾‹**ï¼šå‘ä¸šåŠ¡æ–¹è§£é‡Šå…·ä½“é¢„æµ‹
4. **äº¤å‰éªŒè¯è§£é‡Šç»“æœ**ï¼šä¸åŒæ–¹æ³•ç»“æœåº”è¯¥ä¸€è‡´
5. **è­¦æƒ•ç›¸å…³ç‰¹å¾**ï¼šé«˜åº¦ç›¸å…³çš„ç‰¹å¾ä¼šåˆ†æ•£é‡è¦æ€§
