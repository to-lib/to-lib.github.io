---
sidebar_position: 35
title: ğŸ“¦ æ¨¡å‹å‹ç¼©
---

# æ¨¡å‹å‹ç¼©

æ¨¡å‹å‹ç¼©å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†å»¶è¿Ÿï¼Œä¾¿äºè¾¹ç¼˜éƒ¨ç½²ã€‚

## æŠ€æœ¯æ¦‚è§ˆ

```mermaid
graph TB
    A[æ¨¡å‹å‹ç¼©] --> B[é‡åŒ–]
    A --> C[å‰ªæ]
    A --> D[çŸ¥è¯†è’¸é¦]
    A --> E[ä½ç§©åˆ†è§£]
```

## é‡åŒ–

### è®­ç»ƒåé‡åŒ– (PTQ)

```python
import torch

# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# é™æ€é‡åŒ–
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
# æ ¡å‡†
with torch.no_grad():
    for x in calibration_data:
        model(x)
torch.quantization.convert(model, inplace=True)
```

### é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)

```python
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
model_prepared = torch.quantization.prepare_qat(model)

# æ­£å¸¸è®­ç»ƒ
for epoch in range(epochs):
    train(model_prepared)

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
quantized = torch.quantization.convert(model_prepared)
```

## å‰ªæ

### éç»“æ„åŒ–å‰ªæ

```python
import torch.nn.utils.prune as prune

# å‰ªæ‰ 30% æƒé‡
prune.l1_unstructured(model.fc, name='weight', amount=0.3)

# æ°¸ä¹…åŒ–
prune.remove(model.fc, 'weight')
```

### ç»“æ„åŒ–å‰ªæ

```python
# å‰ªæ‰æ•´ä¸ªé€šé“
prune.ln_structured(model.conv, name='weight', amount=0.3, n=2, dim=0)
```

### è¿­ä»£å‰ªæ

```python
def iterative_pruning(model, target_sparsity, steps=5):
    current_sparsity = 0
    step_sparsity = target_sparsity / steps

    for step in range(steps):
        current_sparsity += step_sparsity
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, 'weight', amount=step_sparsity)

        # å¾®è°ƒ
        finetune(model)
```

## çŸ¥è¯†è’¸é¦

```python
def distillation_loss(student_logits, teacher_logits, labels, T=4, alpha=0.5):
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = nn.KLDivLoss(reduction='batchmean')(
        nn.functional.log_softmax(student_logits / T, dim=1),
        nn.functional.softmax(teacher_logits / T, dim=1)
    ) * T * T

    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = nn.functional.cross_entropy(student_logits, labels)

    return alpha * soft_loss + (1 - alpha) * hard_loss

# è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
for x, y in dataloader:
    with torch.no_grad():
        teacher_logits = teacher_model(x)
    student_logits = student_model(x)
    loss = distillation_loss(student_logits, teacher_logits, y)
```

## æ•ˆæœå¯¹æ¯”

| æ–¹æ³•      | å‹ç¼©æ¯” | ç²¾åº¦æŸå¤± | éœ€è¦é‡è®­ç»ƒ |
| --------- | ------ | -------- | ---------- |
| INT8 é‡åŒ– | 4x     | < 1%     | å¦/å¯é€‰    |
| å‰ªæ 50%  | 2x     | 1-2%     | æ˜¯         |
| è’¸é¦      | å¯å˜   | 1-3%     | æ˜¯         |
| ç»„åˆ      | 10x+   | 2-5%     | æ˜¯         |
