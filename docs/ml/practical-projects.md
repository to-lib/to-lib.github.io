---
sidebar_position: 11
title: ğŸ’¡ å®æˆ˜é¡¹ç›®
---

# æœºå™¨å­¦ä¹ å®æˆ˜é¡¹ç›®

## é¡¹ç›®ä¸€ï¼šæ‰‹å†™æ•°å­—è¯†åˆ« (MNIST)

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# åŠ è½½æ•°æ®
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(
    digits.data, digits.target, test_size=0.2, random_state=42
)

# è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# è¯„ä¼°
print(classification_report(y_test, y_pred))
```

### PyTorch CNN ç‰ˆæœ¬

```python
import torch.nn as nn

class DigitCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
        )
        self.fc = nn.Sequential(
            nn.Flatten(), nn.Linear(64*7*7, 128), nn.ReLU(), nn.Linear(128, 10)
        )

    def forward(self, x):
        return self.fc(self.conv(x))
```

## é¡¹ç›®äºŒï¼šæˆ¿ä»·é¢„æµ‹

```python
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor

# åŠ è½½æ•°æ®
housing = fetch_california_housing()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)

# æ„å»º Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', XGBRegressor(n_estimators=100, learning_rate=0.1))
])

pipeline.fit(X_train, y_train)
print(f"RÂ² Score: {pipeline.score(X_test, y_test):.4f}")
```

## é¡¹ç›®ä¸‰ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# ç¤ºä¾‹æ•°æ®
texts = ["è¿™ä¸ªäº§å“å¾ˆå¥½", "æœåŠ¡å¤ªå·®äº†", "éå¸¸æ»¡æ„", "ä¸æ¨èè´­ä¹°"]
labels = [1, 0, 1, 0]  # 1: æ­£é¢, 0: è´Ÿé¢

# Pipeline
text_clf = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

text_clf.fit(texts, labels)
print(text_clf.predict(["è´¨é‡ä¸é”™"]))  # [1]
```

## é¡¹ç›®æ¸…å•

| é¡¹ç›®           | ç±»å‹     | æŠ€æœ¯æ ˆ       | éš¾åº¦     |
| -------------- | -------- | ------------ | -------- |
| MNIST æ‰‹å†™æ•°å­— | å›¾åƒåˆ†ç±» | CNN          | â­â­     |
| æˆ¿ä»·é¢„æµ‹       | å›å½’     | XGBoost      | â­â­     |
| æƒ…æ„Ÿåˆ†æ       | NLP      | TF-IDF, LSTM | â­â­â­   |
| å®¢æˆ·æµå¤±é¢„æµ‹   | åˆ†ç±»     | éšæœºæ£®æ—     | â­â­     |
| æ¨èç³»ç»Ÿ       | ååŒè¿‡æ»¤ | SVD          | â­â­â­   |
| å›¾åƒé£æ ¼è¿ç§»   | ç”Ÿæˆ     | CNN          | â­â­â­â­ |
