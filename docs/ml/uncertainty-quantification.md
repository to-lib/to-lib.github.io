---
sidebar_position: 37
title: ğŸ“ ä¸ç¡®å®šæ€§é‡åŒ–
---

# ä¸ç¡®å®šæ€§é‡åŒ–

ä¸ç¡®å®šæ€§é‡åŒ–ä¼°è®¡æ¨¡å‹é¢„æµ‹çš„ç½®ä¿¡åº¦ï¼Œå¯¹äºå®‰å…¨å…³é”®åº”ç”¨è‡³å…³é‡è¦ã€‚

## ä¸ç¡®å®šæ€§ç±»å‹

| ç±»å‹                     | æè¿°         | æ¥æº               |
| ------------------------ | ------------ | ------------------ |
| è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic) | æ¨¡å‹ä¸ç¡®å®šæ€§ | æ•°æ®ä¸è¶³ã€æ¨¡å‹å±€é™ |
| å¶ç„¶ä¸ç¡®å®šæ€§ (Aleatoric) | æ•°æ®å›ºæœ‰å™ªå£° | æ— æ³•æ¶ˆé™¤           |

## æ–¹æ³•æ¦‚è§ˆ

### MC Dropout

```python
import torch
import torch.nn as nn

class MCDropoutModel(nn.Module):
    def __init__(self, model, dropout_rate=0.1):
        super().__init__()
        self.model = model
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x, n_samples=100):
        self.train()  # ä¿æŒ dropout å¼€å¯
        predictions = []

        for _ in range(n_samples):
            pred = self.model(self.dropout(x))
            predictions.append(pred)

        predictions = torch.stack(predictions)
        mean = predictions.mean(dim=0)
        std = predictions.std(dim=0)  # ä¸ç¡®å®šæ€§

        return mean, std
```

### Deep Ensemble

```python
class DeepEnsemble:
    def __init__(self, model_class, n_models=5):
        self.models = [model_class() for _ in range(n_models)]

    def fit(self, X, y):
        for model in self.models:
            # æ¯ä¸ªæ¨¡å‹ç”¨ä¸åŒéšæœºç§å­è®­ç»ƒ
            model.fit(X, y)

    def predict(self, X):
        predictions = [model.predict(X) for model in self.models]
        predictions = np.stack(predictions)

        mean = predictions.mean(axis=0)
        std = predictions.std(axis=0)

        return mean, std
```

### è´å¶æ–¯ç¥ç»ç½‘ç»œ

```python
import torch.nn as nn
import torch.nn.functional as F

class BayesianLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight_mu = nn.Parameter(torch.randn(out_features, in_features) * 0.1)
        self.weight_rho = nn.Parameter(torch.randn(out_features, in_features) - 3)
        self.bias_mu = nn.Parameter(torch.zeros(out_features))
        self.bias_rho = nn.Parameter(torch.ones(out_features) * -3)

    def forward(self, x):
        weight_sigma = F.softplus(self.weight_rho)
        bias_sigma = F.softplus(self.bias_rho)

        weight = self.weight_mu + weight_sigma * torch.randn_like(weight_sigma)
        bias = self.bias_mu + bias_sigma * torch.randn_like(bias_sigma)

        return F.linear(x, weight, bias)
```

### æ¸©åº¦ç¼©æ”¾ (Calibration)

```python
class TemperatureScaling:
    def __init__(self, model):
        self.model = model
        self.temperature = nn.Parameter(torch.ones(1))

    def calibrate(self, val_loader):
        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01)

        def eval():
            loss = 0
            for x, y in val_loader:
                logits = self.model(x) / self.temperature
                loss += nn.functional.cross_entropy(logits, y)
            return loss

        optimizer.step(eval)

    def predict(self, x):
        logits = self.model(x) / self.temperature
        return torch.softmax(logits, dim=1)
```

## è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.calibration import calibration_curve

# å¯é æ€§å›¾
prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)

# ECE (Expected Calibration Error)
def expected_calibration_error(y_true, y_prob, n_bins=10):
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0
    for i in range(n_bins):
        mask = (y_prob >= bin_boundaries[i]) & (y_prob < bin_boundaries[i + 1])
        if mask.sum() > 0:
            acc = y_true[mask].mean()
            conf = y_prob[mask].mean()
            ece += mask.sum() * abs(acc - conf)
    return ece / len(y_true)
```

## åº”ç”¨åœºæ™¯

| é¢†åŸŸ     | åº”ç”¨                   |
| -------- | ---------------------- |
| åŒ»ç–—è¯Šæ–­ | è¯†åˆ«éœ€è¦äººå·¥å¤æ ¸çš„ç—…ä¾‹ |
| è‡ªåŠ¨é©¾é©¶ | ä½ç½®ä¿¡åº¦æ—¶äº¤è¿˜æ§åˆ¶æƒ   |
| ä¸»åŠ¨å­¦ä¹  | é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬æ ‡æ³¨ |
| å¼‚å¸¸æ£€æµ‹ | é«˜ä¸ç¡®å®šæ€§å¯èƒ½æ˜¯å¼‚å¸¸   |
