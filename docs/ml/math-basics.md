---
sidebar_position: 3
title: ğŸ“ æ•°å­¦åŸºç¡€
---

# æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€

æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒå»ºç«‹åœ¨ä¸‰å¤§æ•°å­¦æ”¯æŸ±ä¹‹ä¸Šï¼š**çº¿æ€§ä»£æ•°**ã€**æ¦‚ç‡ç»Ÿè®¡**å’Œ**å¾®ç§¯åˆ†**ã€‚

## çº¿æ€§ä»£æ•°

### å‘é‡ä¸çŸ©é˜µ

```python
import numpy as np

# å‘é‡ï¼šä¸€ç»´æ•°ç»„
v = np.array([1, 2, 3])

# çŸ©é˜µï¼šäºŒç»´æ•°ç»„
A = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])

print(f"å‘é‡å½¢çŠ¶: {v.shape}")  # (3,)
print(f"çŸ©é˜µå½¢çŠ¶: {A.shape}")  # (3, 3)
```

### å¸¸ç”¨è¿ç®—

| è¿ç®—       | ç¬¦å·                          | NumPy å®ç°                | åº”ç”¨åœºæ™¯         |
| ---------- | ----------------------------- | ------------------------- | ---------------- |
| ç‚¹ç§¯       | $\mathbf{a} \cdot \mathbf{b}$ | `np.dot(a, b)` æˆ– `a @ b` | ç›¸ä¼¼åº¦è®¡ç®—       |
| çŸ©é˜µä¹˜æ³•   | $AB$                          | `A @ B`                   | ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­ |
| è½¬ç½®       | $A^T$                         | `A.T`                     | æ•°æ®å˜æ¢         |
| é€†çŸ©é˜µ     | $A^{-1}$                      | `np.linalg.inv(A)`        | æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„   |
| è¡Œåˆ—å¼     | $\det(A)$                     | `np.linalg.det(A)`        | åˆ¤æ–­çŸ©é˜µå¯é€†æ€§   |
| ç‰¹å¾å€¼åˆ†è§£ | $A = Q\Lambda Q^{-1}$         | `np.linalg.eig(A)`        | PCA é™ç»´         |

```python
# å‘é‡ç‚¹ç§¯
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
dot_product = a @ b  # 1*4 + 2*5 + 3*6 = 32

# çŸ©é˜µä¹˜æ³•
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A @ B

# ç‰¹å¾å€¼åˆ†è§£
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"ç‰¹å¾å€¼: {eigenvalues}")
print(f"ç‰¹å¾å‘é‡:\n{eigenvectors}")
```

### èŒƒæ•° (Norm)

èŒƒæ•°è¡¡é‡å‘é‡çš„"å¤§å°"ï¼Œåœ¨æ­£åˆ™åŒ–ä¸­å¹¿æ³›ä½¿ç”¨ã€‚

$$
\|\mathbf{x}\|_p = \left( \sum_{i=1}^{n} |x_i|^p \right)^{1/p}
$$

| èŒƒæ•°    | å…¬å¼                | å«ä¹‰         | åº”ç”¨         |
| ------- | ------------------- | ------------ | ------------ | ---------- | ------------ |
| L1 èŒƒæ•° | $\sum               | x_i          | $            | æ›¼å“ˆé¡¿è·ç¦» | Lasso æ­£åˆ™åŒ– |
| L2 èŒƒæ•° | $\sqrt{\sum x_i^2}$ | æ¬§å‡ é‡Œå¾—è·ç¦» | Ridge æ­£åˆ™åŒ– |
| Lâˆ èŒƒæ•° | $\max               | x_i          | $            | æœ€å¤§ç»å¯¹å€¼ | é²æ£’ä¼˜åŒ–     |

```python
x = np.array([3, 4])

l1_norm = np.linalg.norm(x, ord=1)  # 7
l2_norm = np.linalg.norm(x, ord=2)  # 5
linf_norm = np.linalg.norm(x, ord=np.inf)  # 4
```

## æ¦‚ç‡ç»Ÿè®¡

### æ¦‚ç‡åŸºç¡€

**æ¡ä»¶æ¦‚ç‡**ï¼š

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

**è´å¶æ–¯å®šç†**ï¼š

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

```python
# æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç¤ºä¾‹
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target

nb = GaussianNB()
nb.fit(X, y)

# é¢„æµ‹æ–°æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ
proba = nb.predict_proba([[5.0, 3.4, 1.5, 0.2]])
print(f"å„ç±»åˆ«æ¦‚ç‡: {proba}")
```

### å¸¸è§æ¦‚ç‡åˆ†å¸ƒ

| åˆ†å¸ƒ       | å…¬å¼/ç‰¹ç‚¹                     | Python å®ç°                   | åº”ç”¨         |
| ---------- | ----------------------------- | ----------------------------- | ------------ |
| æ­£æ€åˆ†å¸ƒ   | $\mu$ï¼šå‡å€¼ï¼Œ$\sigma$ï¼šæ ‡å‡†å·® | `np.random.normal(mu, sigma)` | è¿ç»­ç‰¹å¾å»ºæ¨¡ |
| ä¼¯åŠªåˆ©åˆ†å¸ƒ | äºŒå…ƒç»“æœï¼ŒæˆåŠŸæ¦‚ç‡ $p$        | `np.random.binomial(1, p)`    | äºŒåˆ†ç±»       |
| å¤šé¡¹åˆ†å¸ƒ   | å¤šä¸ªç¦»æ•£ç»“æœ                  | `np.random.multinomial`       | å¤šåˆ†ç±»       |
| å‡åŒ€åˆ†å¸ƒ   | åŒºé—´å†…ç­‰æ¦‚ç‡                  | `np.random.uniform(a, b)`     | éšæœºåˆå§‹åŒ–   |

```python
import matplotlib.pyplot as plt
from scipy import stats

# æ­£æ€åˆ†å¸ƒ
x = np.linspace(-4, 4, 100)
y = stats.norm.pdf(x, loc=0, scale=1)

plt.figure(figsize=(10, 4))
plt.plot(x, y, label='æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0,1)')
plt.fill_between(x, y, alpha=0.3)
plt.legend()
plt.title('æ­£æ€åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å‡½æ•°')
plt.show()
```

### ç»Ÿè®¡æŒ‡æ ‡

```python
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# é›†ä¸­è¶‹åŠ¿
mean = np.mean(data)       # å‡å€¼: 5.5
median = np.median(data)   # ä¸­ä½æ•°: 5.5

# ç¦»æ•£ç¨‹åº¦
variance = np.var(data)    # æ–¹å·®: 8.25
std = np.std(data)         # æ ‡å‡†å·®: 2.87

# ç›¸å…³æ€§
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])
correlation = np.corrcoef(x, y)[0, 1]  # ç›¸å…³ç³»æ•°
print(f"ç›¸å…³ç³»æ•°: {correlation:.2f}")
```

### æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)

å¯»æ‰¾ä½¿è§‚æµ‹æ•°æ®å‡ºç°æ¦‚ç‡æœ€å¤§çš„å‚æ•°å€¼ã€‚

$$
\hat{\theta} = \arg\max_\theta \prod_{i=1}^{n} P(x_i | \theta)
$$

```python
# ç”¨ MLE ä¼°è®¡æ­£æ€åˆ†å¸ƒå‚æ•°
from scipy.stats import norm

# è§‚æµ‹æ•°æ®
data = np.random.normal(loc=5, scale=2, size=1000)

# MLE ä¼°è®¡
mu_mle = np.mean(data)      # å‡å€¼ä¼°è®¡
sigma_mle = np.std(data)    # æ ‡å‡†å·®ä¼°è®¡

print(f"ä¼°è®¡çš„å‡å€¼: {mu_mle:.2f} (çœŸå®å€¼: 5)")
print(f"ä¼°è®¡çš„æ ‡å‡†å·®: {sigma_mle:.2f} (çœŸå®å€¼: 2)")
```

## å¾®ç§¯åˆ†

### å¯¼æ•°ä¸æ¢¯åº¦

**å¯¼æ•°**ï¼šå‡½æ•°åœ¨æŸç‚¹çš„å˜åŒ–ç‡

$$
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

**æ¢¯åº¦**ï¼šå¤šå…ƒå‡½æ•°å¯¹å„å˜é‡çš„åå¯¼æ•°ç»„æˆçš„å‘é‡

$$
\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)
$$

```python
# æ•°å€¼æ±‚å¯¼
def numerical_gradient(f, x, h=1e-5):
    grad = np.zeros_like(x)
    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()
        x_plus[i] += h
        x_minus[i] -= h
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)
    return grad

# ç¤ºä¾‹ï¼šf(x, y) = x^2 + y^2
def f(x):
    return x[0]**2 + x[1]**2

point = np.array([3.0, 4.0])
grad = numerical_gradient(f, point)
print(f"åœ¨ç‚¹ (3, 4) å¤„çš„æ¢¯åº¦: {grad}")  # [6, 8]
```

### æ¢¯åº¦ä¸‹é™

æœºå™¨å­¦ä¹ ä¸­æœ€æ ¸å¿ƒçš„ä¼˜åŒ–ç®—æ³•ã€‚

$$
\theta_{new} = \theta_{old} - \eta \cdot \nabla L(\theta)
$$

å…¶ä¸­ $\eta$ æ˜¯**å­¦ä¹ ç‡**ï¼Œ$L$ æ˜¯**æŸå¤±å‡½æ•°**ã€‚

```python
def gradient_descent(f, grad_f, x0, learning_rate=0.1, max_iter=100):
    """æ¢¯åº¦ä¸‹é™ä¼˜åŒ–"""
    x = x0.copy()
    history = [x.copy()]

    for _ in range(max_iter):
        grad = grad_f(x)
        x = x - learning_rate * grad
        history.append(x.copy())

        if np.linalg.norm(grad) < 1e-6:
            break

    return x, history

# ç¤ºä¾‹ï¼šæœ€å°åŒ– f(x, y) = x^2 + y^2
def f(x):
    return x[0]**2 + x[1]**2

def grad_f(x):
    return np.array([2*x[0], 2*x[1]])

x0 = np.array([5.0, 5.0])
x_min, history = gradient_descent(f, grad_f, x0, learning_rate=0.1)
print(f"æœ€å°å€¼ç‚¹: {x_min}")  # æ¥è¿‘ [0, 0]
```

### å¸¸è§æŸå¤±å‡½æ•°çš„å¯¼æ•°

| æŸå¤±å‡½æ•°      | å…¬å¼                           | å¯¼æ•°                      | åº”ç”¨ |
| ------------- | ------------------------------ | ------------------------- | ---- |
| MSE           | $\frac{1}{n}\sum(y-\hat{y})^2$ | $-\frac{2}{n}(y-\hat{y})$ | å›å½’ |
| Cross-Entropy | $-\sum y\log\hat{y}$           | $\hat{y} - y$             | åˆ†ç±» |
| Hinge Loss    | $\max(0, 1-y\cdot\hat{y})$     | $-y$ if $y\hat{y} < 1$    | SVM  |

```python
# MSE æŸå¤±åŠå…¶æ¢¯åº¦
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def mse_gradient(y_true, y_pred):
    n = len(y_true)
    return -2 * (y_true - y_pred) / n
```

## æ ¸å¿ƒå…¬å¼é€ŸæŸ¥è¡¨

### çº¿æ€§ä»£æ•°

| å…¬å¼                                                                             | æè¿°       |
| -------------------------------------------------------------------------------- | ---------- |
| $\|\mathbf{x}\|_2 = \sqrt{\sum x_i^2}$                                           | L2 èŒƒæ•°    |
| $\cos\theta = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}$ | ä½™å¼¦ç›¸ä¼¼åº¦ |
| $A^T A$                                                                          | Gram çŸ©é˜µ  |

### æ¦‚ç‡ç»Ÿè®¡

| å…¬å¼                                 | æè¿°       |
| ------------------------------------ | ---------- |
| $P(A\|B) = \frac{P(B\|A)P(A)}{P(B)}$ | è´å¶æ–¯å®šç† |
| $\text{Var}(X) = E[(X-\mu)^2]$       | æ–¹å·®       |
| $\sigma = \sqrt{\text{Var}(X)}$      | æ ‡å‡†å·®     |

### å¾®ç§¯åˆ†

| å…¬å¼                                          | æè¿°         |
| --------------------------------------------- | ------------ |
| $\theta := \theta - \eta \nabla L$            | æ¢¯åº¦ä¸‹é™æ›´æ–° |
| $\frac{\partial}{\partial x}(x^n) = nx^{n-1}$ | å¹‚å‡½æ•°å¯¼æ•°   |
| $\frac{d}{dx}\ln(x) = \frac{1}{x}$            | å¯¹æ•°å¯¼æ•°     |
