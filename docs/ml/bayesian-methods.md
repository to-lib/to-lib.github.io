---
sidebar_position: 20
title: ğŸ“Š è´å¶æ–¯æ–¹æ³•
---

# è´å¶æ–¯æ–¹æ³•

è´å¶æ–¯æ–¹æ³•åŸºäºæ¦‚ç‡è®ºï¼Œå°†ä¸ç¡®å®šæ€§èå…¥æ¨¡å‹ä¸­ã€‚

## è´å¶æ–¯å®šç†

$$
P(\theta|D) = \frac{P(D|\theta) \cdot P(\theta)}{P(D)}
$$

| æœ¯è¯­     | ç¬¦å·    | æè¿°                   |
| -------- | ------- | ---------------------- |
| åéªŒæ¦‚ç‡ | P(Î¸\|D) | è§‚æµ‹æ•°æ®åå¯¹å‚æ•°çš„ä¿¡å¿µ |
| ä¼¼ç„¶     | P(D\|Î¸) | ç»™å®šå‚æ•°ä¸‹æ•°æ®çš„æ¦‚ç‡   |
| å…ˆéªŒæ¦‚ç‡ | P(Î¸)    | è§‚æµ‹å‰å¯¹å‚æ•°çš„ä¿¡å¿µ     |
| è¾¹é™…ä¼¼ç„¶ | P(D)    | å½’ä¸€åŒ–å› å­             |

## æœ´ç´ è´å¶æ–¯

å‡è®¾ç‰¹å¾ä¹‹é—´æ¡ä»¶ç‹¬ç«‹ã€‚

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

# é«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼ˆè¿ç»­ç‰¹å¾ï¼‰
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ï¼ˆè®¡æ•°ç‰¹å¾ï¼Œå¦‚è¯é¢‘ï¼‰
mnb = MultinomialNB()

# ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼ˆäºŒå…ƒç‰¹å¾ï¼‰
bnb = BernoulliNB()
```

### æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

text_clf = Pipeline([
    ('vect', CountVectorizer()),
    ('clf', MultinomialNB())
])

text_clf.fit(texts, labels)
predictions = text_clf.predict(new_texts)
```

## é«˜æ–¯è¿‡ç¨‹

éå‚æ•°è´å¶æ–¯æ–¹æ³•ï¼Œç”¨äºå›å½’å’Œåˆ†ç±»ã€‚

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel

kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1)
gpr.fit(X_train, y_train)

# é¢„æµ‹å‡å€¼å’Œæ ‡å‡†å·®
mean, std = gpr.predict(X_test, return_std=True)
```

## è´å¶æ–¯ä¼˜åŒ–

ç”¨äºè¶…å‚æ•°è°ƒä¼˜ï¼Œé€‚åˆæ˜‚è´µçš„ç›®æ ‡å‡½æ•°ã€‚

```python
from skopt import BayesSearchCV
from sklearn.ensemble import RandomForestClassifier

opt = BayesSearchCV(
    RandomForestClassifier(),
    {
        'n_estimators': (50, 500),
        'max_depth': (3, 20),
        'min_samples_split': (2, 20)
    },
    n_iter=50,
    cv=5
)
opt.fit(X_train, y_train)
print(f"æœ€ä½³å‚æ•°: {opt.best_params_}")
```

### Optuna

```python
import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
    }
    model = XGBClassifier(**params)
    return cross_val_score(model, X, y, cv=5).mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

## è´å¶æ–¯ç¥ç»ç½‘ç»œ

```python
import torch
import torch.nn as nn

class BayesianLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight_mu = nn.Parameter(torch.randn(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.randn(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.randn(out_features))
        self.bias_sigma = nn.Parameter(torch.randn(out_features))

    def forward(self, x):
        weight = self.weight_mu + torch.exp(self.weight_sigma) * torch.randn_like(self.weight_mu)
        bias = self.bias_mu + torch.exp(self.bias_sigma) * torch.randn_like(self.bias_mu)
        return torch.nn.functional.linear(x, weight, bias)
```

## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³•       | é€‚ç”¨åœºæ™¯   | ä¼˜ç‚¹         | ç¼ºç‚¹         |
| ---------- | ---------- | ------------ | ------------ |
| æœ´ç´ è´å¶æ–¯ | æ–‡æœ¬åˆ†ç±»   | å¿«é€Ÿã€ç®€å•   | ç‰¹å¾ç‹¬ç«‹å‡è®¾ |
| é«˜æ–¯è¿‡ç¨‹   | å°æ•°æ®å›å½’ | ä¸ç¡®å®šæ€§ä¼°è®¡ | è®¡ç®—å¤æ‚åº¦é«˜ |
| è´å¶æ–¯ä¼˜åŒ– | è¶…å‚æ•°è°ƒä¼˜ | æ ·æœ¬æ•ˆç‡é«˜   | ç»´åº¦ç¾éš¾     |
