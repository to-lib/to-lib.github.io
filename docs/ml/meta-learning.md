---
sidebar_position: 28
title: ğŸ§¬ å…ƒå­¦ä¹ 
---

# å…ƒå­¦ä¹  (Meta-Learning)

å…ƒå­¦ä¹ æ˜¯"å­¦ä¹ å¦‚ä½•å­¦ä¹ "ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚

## æ ¸å¿ƒæ€æƒ³

```mermaid
graph LR
    A[ä»»åŠ¡1] --> B[Meta-Learner]
    C[ä»»åŠ¡2] --> B
    D[ä»»åŠ¡3] --> B
    B --> E[å­¦ä¹ ç­–ç•¥]
    E --> F[å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡]
```

## Few-Shot Learning

ç»™å®šå°‘é‡æ ·æœ¬å­¦ä¹ æ–°ç±»åˆ«ã€‚

| æœ¯è¯­        | æè¿°               |
| ----------- | ------------------ |
| N-way       | N ä¸ªç±»åˆ«           |
| K-shot      | æ¯ç±» K ä¸ªæ ·æœ¬      |
| Support Set | ç”¨äºå­¦ä¹ çš„å°‘é‡æ ·æœ¬ |
| Query Set   | ç”¨äºè¯„ä¼°çš„æ ·æœ¬     |

## MAML (Model-Agnostic Meta-Learning)

```python
import torch
import torch.nn as nn
from copy import deepcopy

class MAML:
    def __init__(self, model, lr_inner=0.01, lr_outer=0.001):
        self.model = model
        self.lr_inner = lr_inner
        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr_outer)

    def inner_loop(self, support_x, support_y, num_steps=5):
        # å¤åˆ¶æ¨¡å‹å‚æ•°
        fast_weights = {name: param.clone() for name, param in self.model.named_parameters()}

        for _ in range(num_steps):
            # å‰å‘ä¼ æ’­
            logits = self.model(support_x)
            loss = nn.functional.cross_entropy(logits, support_y)

            # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å¿«å‚æ•°
            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)
            fast_weights = {name: param - self.lr_inner * grad
                           for (name, param), grad in zip(fast_weights.items(), grads)}

        return fast_weights

    def outer_loop(self, tasks):
        meta_loss = 0

        for support_x, support_y, query_x, query_y in tasks:
            # å†…å¾ªç¯ï¼šåœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”
            fast_weights = self.inner_loop(support_x, support_y)

            # å¤–å¾ªç¯ï¼šåœ¨æŸ¥è¯¢é›†ä¸Šè®¡ç®—æŸå¤±
            with torch.no_grad():
                # ä¸´æ—¶ä½¿ç”¨ fast_weights
                pass
            logits = self.model(query_x)  # ä½¿ç”¨ fast_weights
            meta_loss += nn.functional.cross_entropy(logits, query_y)

        # æ›´æ–°å…ƒå‚æ•°
        self.optimizer.zero_grad()
        meta_loss.backward()
        self.optimizer.step()
```

## Prototypical Networks

```python
class PrototypicalNetwork(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, support_x, support_y, query_x, n_way, k_shot):
        # ç¼–ç æ‰€æœ‰æ ·æœ¬
        support_emb = self.encoder(support_x)  # (n_way * k_shot, dim)
        query_emb = self.encoder(query_x)

        # è®¡ç®—æ¯ç±»çš„åŸå‹
        support_emb = support_emb.view(n_way, k_shot, -1)
        prototypes = support_emb.mean(dim=1)  # (n_way, dim)

        # è®¡ç®—æŸ¥è¯¢æ ·æœ¬åˆ°å„åŸå‹çš„è·ç¦»
        distances = torch.cdist(query_emb, prototypes)  # (n_query, n_way)

        return -distances  # è´Ÿè·ç¦»ä½œä¸º logits
```

## Matching Networks

```python
class MatchingNetwork(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, support_x, support_y, query_x):
        support_emb = self.encoder(support_x)
        query_emb = self.encoder(query_x)

        # ä½™å¼¦ç›¸ä¼¼åº¦
        support_emb = nn.functional.normalize(support_emb, dim=1)
        query_emb = nn.functional.normalize(query_emb, dim=1)
        similarity = torch.mm(query_emb, support_emb.t())

        # æ³¨æ„åŠ›æƒé‡
        attention = nn.functional.softmax(similarity, dim=1)

        # åŠ æƒé¢„æµ‹
        support_labels_onehot = nn.functional.one_hot(support_y)
        predictions = torch.mm(attention, support_labels_onehot.float())

        return predictions
```

## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³•         | ç±»å‹ | ç‰¹ç‚¹               |
| ------------ | ---- | ------------------ |
| MAML         | ä¼˜åŒ– | æ¨¡å‹æ— å…³ï¼ŒäºŒé˜¶å¯¼æ•° |
| Prototypical | åº¦é‡ | ç®€å•é«˜æ•ˆ           |
| Matching     | åº¦é‡ | åŸºäºæ³¨æ„åŠ›         |
| Reptile      | ä¼˜åŒ– | MAML çš„ç®€åŒ–ç‰ˆ      |
