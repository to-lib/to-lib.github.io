---
sidebar_position: 20
title: ğŸ—„ï¸ å‘é‡æ•°æ®åº“å®æˆ˜
---

# å‘é‡æ•°æ®åº“å®æˆ˜

å‘é‡æ•°æ®åº“æ˜¯ RAG å’Œè¯­ä¹‰æœç´¢çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡ã€‚æœ¬æ–‡ä»‹ç»ä¸»æµå‘é‡æ•°æ®åº“çš„ä½¿ç”¨æ–¹æ³•ã€‚

## å‘é‡æ•°æ®åº“å¯¹æ¯”

| æ•°æ®åº“       | ç±»å‹   | ç‰¹ç‚¹                     | é€‚ç”¨åœºæ™¯         |
| ------------ | ------ | ------------------------ | ---------------- |
| **Chroma**   | åµŒå…¥å¼ | è½»é‡ã€æ˜“ç”¨ã€Python åŸç”Ÿ  | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡ |
| **Milvus**   | åˆ†å¸ƒå¼ | é«˜æ€§èƒ½ã€å¯æ‰©å±•           | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡ |
| **Pinecone** | æ‰˜ç®¡   | å…¨æ‰˜ç®¡ã€å…è¿ç»´           | å¿«é€Ÿä¸Šçº¿ã€ä¼ä¸šçº§ |
| **Qdrant**   | å¼€æº   | Rust å®ç°ã€é«˜æ€§èƒ½        | ç”Ÿäº§ç¯å¢ƒ         |
| **pgvector** | æ‰©å±•   | PostgreSQL æ’ä»¶          | å·²æœ‰ PG åŸºç¡€è®¾æ–½ |
| **Weaviate** | å¼€æº   | GraphQL APIã€å¤šæ¨¡æ€      | å¤æ‚æŸ¥è¯¢éœ€æ±‚     |

## Chroma

Chroma æ˜¯æœ€ç®€å•çš„å‘é‡æ•°æ®åº“ï¼Œé€‚åˆå¿«é€Ÿå¼€å‘å’ŒåŸå‹éªŒè¯ã€‚

### å®‰è£…

```bash
pip install chromadb
```

### åŸºç¡€ä½¿ç”¨

```python
import chromadb
from chromadb.utils import embedding_functions

# åˆ›å»ºå®¢æˆ·ç«¯
client = chromadb.Client()  # å†…å­˜æ¨¡å¼
# client = chromadb.PersistentClient(path="./chroma_db")  # æŒä¹…åŒ–

# ä½¿ç”¨ OpenAI Embedding
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

# åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="documents",
    embedding_function=openai_ef,
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)

# æ·»åŠ æ–‡æ¡£
collection.add(
    documents=["æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯", "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ", "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†æ–‡æœ¬æ•°æ®"],
    metadatas=[{"source": "wiki"}, {"source": "book"}, {"source": "paper"}],
    ids=["doc1", "doc2", "doc3"]
)

# æŸ¥è¯¢
results = collection.query(
    query_texts=["ä»€ä¹ˆæ˜¯ AIï¼Ÿ"],
    n_results=2,
    where={"source": "wiki"}  # å…ƒæ•°æ®è¿‡æ»¤
)

print(results["documents"])
print(results["distances"])
```

### æ›´æ–°å’Œåˆ é™¤

```python
# æ›´æ–°
collection.update(
    ids=["doc1"],
    documents=["æ›´æ–°åçš„å†…å®¹"],
    metadatas=[{"source": "updated"}]
)

# åˆ é™¤
collection.delete(ids=["doc1"])

# æŒ‰æ¡ä»¶åˆ é™¤
collection.delete(where={"source": "wiki"})
```

### ä¸ LangChain é›†æˆ

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ä»æ–‡æ¡£åˆ›å»º
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# æ£€ç´¢
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
docs = retriever.invoke("æŸ¥è¯¢å†…å®¹")
```

## Milvus

Milvus æ˜¯é«˜æ€§èƒ½çš„åˆ†å¸ƒå¼å‘é‡æ•°æ®åº“ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

### å®‰è£…

```bash
# ä½¿ç”¨ Docker
docker run -d --name milvus \
    -p 19530:19530 \
    -p 9091:9091 \
    milvusdb/milvus:latest

# Python SDK
pip install pymilvus
```

### åŸºç¡€ä½¿ç”¨

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility

# è¿æ¥
connections.connect("default", host="localhost", port="19530")

# å®šä¹‰ Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536),
    FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=256)
]

schema = CollectionSchema(fields, description="æ–‡æ¡£é›†åˆ")

# åˆ›å»ºé›†åˆ
collection = Collection("documents", schema)

# åˆ›å»ºç´¢å¼•
index_params = {
    "metric_type": "COSINE",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024}
}
collection.create_index("embedding", index_params)

# æ’å…¥æ•°æ®
data = [
    ["æ–‡æ¡£1å†…å®¹", "æ–‡æ¡£2å†…å®¹"],  # text
    [[0.1] * 1536, [0.2] * 1536],  # embedding
    ["wiki", "book"]  # source
]
collection.insert(data)

# åŠ è½½åˆ°å†…å­˜
collection.load()

# æœç´¢
search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
results = collection.search(
    data=[[0.1] * 1536],  # æŸ¥è¯¢å‘é‡
    anns_field="embedding",
    param=search_params,
    limit=5,
    expr="source == 'wiki'",  # è¿‡æ»¤æ¡ä»¶
    output_fields=["text", "source"]
)

for hits in results:
    for hit in hits:
        print(f"ID: {hit.id}, Distance: {hit.distance}")
        print(f"Text: {hit.entity.get('text')}")
```

### é«˜çº§åŠŸèƒ½

```python
# æ··åˆæœç´¢ï¼ˆå‘é‡ + æ ‡é‡ï¼‰
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    param=search_params,
    limit=10,
    expr="source in ['wiki', 'book'] and length(text) > 100",
    output_fields=["text", "source"]
)

# æ‰¹é‡æ’å…¥
from pymilvus import utility

# åˆ†æ‰¹æ’å…¥å¤§é‡æ•°æ®
batch_size = 1000
for i in range(0, len(all_data), batch_size):
    batch = all_data[i:i+batch_size]
    collection.insert(batch)
    collection.flush()

# åˆ é™¤
collection.delete(expr="id in [1, 2, 3]")
```

### ä¸ LangChain é›†æˆ

```python
from langchain_community.vectorstores import Milvus
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

vectorstore = Milvus.from_documents(
    documents=docs,
    embedding=embeddings,
    connection_args={"host": "localhost", "port": "19530"},
    collection_name="langchain_docs"
)

# æ£€ç´¢
docs = vectorstore.similarity_search("æŸ¥è¯¢", k=3)
```

## Pinecone

Pinecone æ˜¯å…¨æ‰˜ç®¡çš„å‘é‡æ•°æ®åº“æœåŠ¡ã€‚

### å®‰è£…

```bash
pip install pinecone-client
```

### åŸºç¡€ä½¿ç”¨

```python
from pinecone import Pinecone, ServerlessSpec

# åˆå§‹åŒ–
pc = Pinecone(api_key="your-api-key")

# åˆ›å»ºç´¢å¼•
pc.create_index(
    name="documents",
    dimension=1536,
    metric="cosine",
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)

# è·å–ç´¢å¼•
index = pc.Index("documents")

# æ’å…¥å‘é‡
index.upsert(
    vectors=[
        {
            "id": "doc1",
            "values": [0.1] * 1536,
            "metadata": {"text": "æ–‡æ¡£å†…å®¹", "source": "wiki"}
        }
    ],
    namespace="default"
)

# æŸ¥è¯¢
results = index.query(
    vector=[0.1] * 1536,
    top_k=5,
    include_metadata=True,
    filter={"source": {"$eq": "wiki"}}
)

for match in results["matches"]:
    print(f"ID: {match['id']}, Score: {match['score']}")
    print(f"Metadata: {match['metadata']}")
```

### æ‰¹é‡æ“ä½œ

```python
# æ‰¹é‡æ’å…¥
vectors = [
    {"id": f"doc{i}", "values": embeddings[i], "metadata": {"text": texts[i]}}
    for i in range(len(texts))
]

# åˆ†æ‰¹ä¸Šä¼ 
batch_size = 100
for i in range(0, len(vectors), batch_size):
    batch = vectors[i:i+batch_size]
    index.upsert(vectors=batch)

# åˆ é™¤
index.delete(ids=["doc1", "doc2"])
index.delete(filter={"source": {"$eq": "wiki"}})
```

## pgvector

pgvector æ˜¯ PostgreSQL çš„å‘é‡æ‰©å±•ï¼Œé€‚åˆå·²æœ‰ PostgreSQL åŸºç¡€è®¾æ–½çš„åœºæ™¯ã€‚

### å®‰è£…

```sql
-- PostgreSQL ä¸­å®‰è£…æ‰©å±•
CREATE EXTENSION vector;
```

```bash
pip install pgvector psycopg2-binary
```

### åŸºç¡€ä½¿ç”¨

```python
import psycopg2
from pgvector.psycopg2 import register_vector

# è¿æ¥
conn = psycopg2.connect("postgresql://user:pass@localhost/db")
register_vector(conn)

cur = conn.cursor()

# åˆ›å»ºè¡¨
cur.execute("""
    CREATE TABLE IF NOT EXISTS documents (
        id SERIAL PRIMARY KEY,
        text TEXT,
        embedding vector(1536),
        source VARCHAR(256),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
""")

# åˆ›å»ºç´¢å¼•
cur.execute("""
    CREATE INDEX ON documents 
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 100)
""")

# æ’å…¥
cur.execute(
    "INSERT INTO documents (text, embedding, source) VALUES (%s, %s, %s)",
    ("æ–‡æ¡£å†…å®¹", [0.1] * 1536, "wiki")
)

# æŸ¥è¯¢ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
cur.execute("""
    SELECT id, text, source, 1 - (embedding <=> %s) as similarity
    FROM documents
    WHERE source = %s
    ORDER BY embedding <=> %s
    LIMIT 5
""", ([0.1] * 1536, "wiki", [0.1] * 1536))

results = cur.fetchall()
for row in results:
    print(f"ID: {row[0]}, Similarity: {row[3]:.4f}")

conn.commit()
```

### ä½¿ç”¨ SQLAlchemy

```python
from sqlalchemy import create_engine, Column, Integer, String, Text
from sqlalchemy.orm import declarative_base, sessionmaker
from pgvector.sqlalchemy import Vector

Base = declarative_base()

class Document(Base):
    __tablename__ = "documents"
    
    id = Column(Integer, primary_key=True)
    text = Column(Text)
    embedding = Column(Vector(1536))
    source = Column(String(256))

engine = create_engine("postgresql://user:pass@localhost/db")
Base.metadata.create_all(engine)

Session = sessionmaker(bind=engine)
session = Session()

# æ’å…¥
doc = Document(text="å†…å®¹", embedding=[0.1] * 1536, source="wiki")
session.add(doc)
session.commit()

# æŸ¥è¯¢
from sqlalchemy import select

query_embedding = [0.1] * 1536
results = session.scalars(
    select(Document)
    .order_by(Document.embedding.cosine_distance(query_embedding))
    .limit(5)
).all()
```

## Qdrant

Qdrant æ˜¯ç”¨ Rust ç¼–å†™çš„é«˜æ€§èƒ½å‘é‡æ•°æ®åº“ã€‚

### å®‰è£…

```bash
# Docker
docker run -p 6333:6333 qdrant/qdrant

# Python SDK
pip install qdrant-client
```

### åŸºç¡€ä½¿ç”¨

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# è¿æ¥
client = QdrantClient(host="localhost", port=6333)

# åˆ›å»ºé›†åˆ
client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
)

# æ’å…¥
client.upsert(
    collection_name="documents",
    points=[
        PointStruct(
            id=1,
            vector=[0.1] * 1536,
            payload={"text": "æ–‡æ¡£å†…å®¹", "source": "wiki"}
        )
    ]
)

# æœç´¢
results = client.search(
    collection_name="documents",
    query_vector=[0.1] * 1536,
    query_filter={
        "must": [{"key": "source", "match": {"value": "wiki"}}]
    },
    limit=5
)

for result in results:
    print(f"ID: {result.id}, Score: {result.score}")
    print(f"Payload: {result.payload}")
```

## æœ€ä½³å®è·µ

### 1. ç´¢å¼•é€‰æ‹©

| ç´¢å¼•ç±»å‹   | ç‰¹ç‚¹                 | é€‚ç”¨åœºæ™¯       |
| ---------- | -------------------- | -------------- |
| Flat       | ç²¾ç¡®æœç´¢ï¼Œæ…¢         | å°æ•°æ®é›†       |
| IVF        | è¿‘ä¼¼æœç´¢ï¼Œå¿«         | ä¸­ç­‰æ•°æ®é›†     |
| HNSW       | é«˜å¬å›ç‡ï¼Œå†…å­˜å ç”¨å¤§ | é«˜ç²¾åº¦éœ€æ±‚     |
| PQ         | å‹ç¼©å­˜å‚¨ï¼Œé€Ÿåº¦å¿«     | å¤§è§„æ¨¡æ•°æ®     |

### 2. å…ƒæ•°æ®è®¾è®¡

```python
# å¥½çš„å…ƒæ•°æ®è®¾è®¡
metadata = {
    "source": "wiki",           # æ¥æº
    "category": "technology",   # åˆ†ç±»
    "created_at": "2024-01-01", # æ—¶é—´
    "author": "å¼ ä¸‰",           # ä½œè€…
    "access_level": "public",   # æƒé™
    "chunk_index": 0,           # åˆ†å—ç´¢å¼•
    "doc_id": "doc_123"         # åŸæ–‡æ¡£ ID
}
```

### 3. åˆ†ç‰‡ç­–ç•¥

```python
# æŒ‰ç§Ÿæˆ·åˆ†ç‰‡
collection_name = f"tenant_{tenant_id}_documents"

# æŒ‰æ—¶é—´åˆ†ç‰‡
collection_name = f"documents_{year}_{month}"
```

### 4. æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹é‡æ“ä½œ
batch_size = 1000
for i in range(0, len(vectors), batch_size):
    batch = vectors[i:i+batch_size]
    collection.insert(batch)

# é¢„çƒ­ç´¢å¼•
collection.load()

# è°ƒæ•´ç´¢å¼•å‚æ•°
index_params = {
    "index_type": "IVF_PQ",
    "params": {
        "nlist": 2048,
        "m": 16,
        "nbits": 8
    }
}
```

## å»¶ä¼¸é˜…è¯»

- [Chroma æ–‡æ¡£](https://docs.trychroma.com/)
- [Milvus æ–‡æ¡£](https://milvus.io/docs)
- [Pinecone æ–‡æ¡£](https://docs.pinecone.io/)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [Qdrant æ–‡æ¡£](https://qdrant.tech/documentation/)
