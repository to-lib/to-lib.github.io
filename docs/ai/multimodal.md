---
sidebar_position: 15
title: ğŸ–¼ï¸ å¤šæ¨¡æ€ AI
---

# å¤šæ¨¡æ€ AI (Multimodal AI)

å¤šæ¨¡æ€ AI æ˜¯æŒ‡èƒ½å¤Ÿå¤„ç†å’Œç†è§£å¤šç§ç±»å‹æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚ç°ä»£ LLM å¦‚ GPT-4oã€Claude 3.5ã€Gemini éƒ½å…·å¤‡å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚

## å¤šæ¨¡æ€èƒ½åŠ›æ¦‚è§ˆ

| æ¨¡æ€       | è¾“å…¥ | è¾“å‡º | å…¸å‹åº”ç”¨                   |
| ---------- | ---- | ---- | -------------------------- |
| **æ–‡æœ¬**   | âœ…   | âœ…   | å¯¹è¯ã€å†™ä½œã€ç¿»è¯‘           |
| **å›¾åƒ**   | âœ…   | âœ…   | å›¾åƒç†è§£ã€å›¾åƒç”Ÿæˆ         |
| **éŸ³é¢‘**   | âœ…   | âœ…   | è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆ         |
| **è§†é¢‘**   | âœ…   | âš ï¸   | è§†é¢‘ç†è§£ï¼ˆç”Ÿæˆèƒ½åŠ›æœ‰é™ï¼‰   |

## Visionï¼ˆå›¾åƒç†è§£ï¼‰

### OpenAI Vision API

```python
from openai import OpenAI
import base64

client = OpenAI()

def encode_image(image_path: str) -> str:
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    with open(image_path, "rb") as f:
        return base64.standard_b64encode(f.read()).decode("utf-8")

def analyze_image(image_path: str, prompt: str = "æè¿°è¿™å¼ å›¾ç‰‡") -> str:
    """åˆ†æå›¾ç‰‡å†…å®¹"""
    base64_image = encode_image(image_path)
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"  # low, high, auto
                        }
                    }
                ]
            }
        ],
        max_tokens=1000
    )
    
    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
result = analyze_image("screenshot.png", "è¿™ä¸ªç•Œé¢æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿè¯·ç»™å‡ºæ”¹è¿›å»ºè®®ã€‚")
print(result)
```

### ä½¿ç”¨ URL å›¾ç‰‡

```python
def analyze_image_url(image_url: str, prompt: str) -> str:
    """åˆ†æç½‘ç»œå›¾ç‰‡"""
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url}
                    }
                ]
            }
        ]
    )
    
    return response.choices[0].message.content

# åˆ†æç½‘ç»œå›¾ç‰‡
result = analyze_image_url(
    "https://example.com/chart.png",
    "è¯·åˆ†æè¿™ä¸ªå›¾è¡¨çš„æ•°æ®è¶‹åŠ¿"
)
```

### å¤šå›¾ç‰‡åˆ†æ

```python
def compare_images(image_paths: list[str], prompt: str) -> str:
    """æ¯”è¾ƒå¤šå¼ å›¾ç‰‡"""
    content = [{"type": "text", "text": prompt}]
    
    for path in image_paths:
        base64_image = encode_image(path)
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
        })
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=1500
    )
    
    return response.choices[0].message.content

# æ¯”è¾ƒä¸¤ä¸ªè®¾è®¡ç¨¿
result = compare_images(
    ["design_v1.png", "design_v2.png"],
    "æ¯”è¾ƒè¿™ä¸¤ä¸ªè®¾è®¡ç¨¿çš„å·®å¼‚ï¼Œå“ªä¸ªæ›´å¥½ï¼Ÿ"
)
```

### Anthropic Vision

```python
import anthropic
import base64

client = anthropic.Anthropic()

def analyze_with_claude(image_path: str, prompt: str) -> str:
    """ä½¿ç”¨ Claude åˆ†æå›¾ç‰‡"""
    with open(image_path, "rb") as f:
        image_data = base64.standard_b64encode(f.read()).decode("utf-8")
    
    # è·å–å›¾ç‰‡ç±»å‹
    import mimetypes
    media_type = mimetypes.guess_type(image_path)[0] or "image/jpeg"
    
    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": image_data
                        }
                    },
                    {"type": "text", "text": prompt}
                ]
            }
        ]
    )
    
    return message.content[0].text
```

## å›¾åƒç”Ÿæˆ

### DALL-E 3

```python
def generate_image(prompt: str, size: str = "1024x1024", quality: str = "standard") -> str:
    """ä½¿ç”¨ DALL-E 3 ç”Ÿæˆå›¾ç‰‡"""
    response = client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size=size,  # 1024x1024, 1792x1024, 1024x1792
        quality=quality,  # standard, hd
        n=1
    )
    
    return response.data[0].url

# ç”Ÿæˆå›¾ç‰‡
image_url = generate_image(
    "ä¸€åªå¯çˆ±çš„æœºå™¨äººåœ¨èŠ±å›­é‡Œæµ‡èŠ±ï¼Œçš®å…‹æ–¯é£æ ¼ï¼Œæ¸©æš–çš„é˜³å…‰",
    size="1024x1024",
    quality="hd"
)
print(f"ç”Ÿæˆçš„å›¾ç‰‡: {image_url}")
```

### å›¾ç‰‡ç¼–è¾‘

```python
def edit_image(image_path: str, mask_path: str, prompt: str) -> str:
    """ç¼–è¾‘å›¾ç‰‡ï¼ˆéœ€è¦ DALL-E 2ï¼‰"""
    response = client.images.edit(
        model="dall-e-2",
        image=open(image_path, "rb"),
        mask=open(mask_path, "rb"),  # é€æ˜åŒºåŸŸè¡¨ç¤ºè¦ç¼–è¾‘çš„éƒ¨åˆ†
        prompt=prompt,
        n=1,
        size="1024x1024"
    )
    
    return response.data[0].url
```

### å›¾ç‰‡å˜ä½“

```python
def create_variation(image_path: str) -> str:
    """åˆ›å»ºå›¾ç‰‡å˜ä½“"""
    response = client.images.create_variation(
        model="dall-e-2",
        image=open(image_path, "rb"),
        n=1,
        size="1024x1024"
    )
    
    return response.data[0].url
```

## è¯­éŸ³å¤„ç†

### è¯­éŸ³è½¬æ–‡å­— (STT)

```python
def transcribe_audio(audio_path: str, language: str = None) -> dict:
    """è¯­éŸ³è½¬æ–‡å­—"""
    with open(audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language=language,  # å¯é€‰ï¼Œå¦‚ "zh", "en"
            response_format="verbose_json",  # åŒ…å«æ—¶é—´æˆ³
            timestamp_granularities=["word", "segment"]
        )
    
    return {
        "text": transcript.text,
        "segments": transcript.segments,
        "words": transcript.words
    }

# è½¬å½•éŸ³é¢‘
result = transcribe_audio("meeting.mp3", language="zh")
print(result["text"])

# å¸¦æ—¶é—´æˆ³çš„è½¬å½•
for segment in result["segments"]:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] {segment['text']}")
```

### è¯­éŸ³ç¿»è¯‘

```python
def translate_audio(audio_path: str) -> str:
    """å°†éŸ³é¢‘ç¿»è¯‘ä¸ºè‹±æ–‡"""
    with open(audio_path, "rb") as audio_file:
        translation = client.audio.translations.create(
            model="whisper-1",
            file=audio_file
        )
    
    return translation.text

# å°†ä¸­æ–‡éŸ³é¢‘ç¿»è¯‘ä¸ºè‹±æ–‡
english_text = translate_audio("chinese_speech.mp3")
```

### æ–‡å­—è½¬è¯­éŸ³ (TTS)

```python
from pathlib import Path

def text_to_speech(
    text: str, 
    output_path: str,
    voice: str = "alloy",
    model: str = "tts-1"
) -> str:
    """æ–‡å­—è½¬è¯­éŸ³"""
    # å¯ç”¨å£°éŸ³: alloy, echo, fable, onyx, nova, shimmer
    # æ¨¡å‹: tts-1 (å¿«é€Ÿ), tts-1-hd (é«˜è´¨é‡)
    
    response = client.audio.speech.create(
        model=model,
        voice=voice,
        input=text
    )
    
    response.stream_to_file(output_path)
    return output_path

# ç”Ÿæˆè¯­éŸ³
text_to_speech(
    "ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„æœåŠ¡ï¼",
    "welcome.mp3",
    voice="nova"
)
```

### æµå¼è¯­éŸ³ç”Ÿæˆ

```python
def stream_speech(text: str):
    """æµå¼ç”Ÿæˆè¯­éŸ³"""
    with client.audio.speech.with_streaming_response.create(
        model="tts-1",
        voice="alloy",
        input=text
    ) as response:
        for chunk in response.iter_bytes(chunk_size=1024):
            # å®æ—¶æ’­æ”¾æˆ–å¤„ç†éŸ³é¢‘å—
            yield chunk
```

## å®æˆ˜åº”ç”¨

### 1. æ–‡æ¡£ OCR ä¸ç†è§£

```python
def extract_document_info(image_path: str) -> dict:
    """ä»æ–‡æ¡£å›¾ç‰‡ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
    
    prompt = """åˆ†æè¿™å¼ æ–‡æ¡£å›¾ç‰‡ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥ JSON æ ¼å¼è¿”å›ï¼š
    - document_type: æ–‡æ¡£ç±»å‹ï¼ˆå‘ç¥¨/åˆåŒ/èº«ä»½è¯/å…¶ä»–ï¼‰
    - key_fields: å…³é”®å­—æ®µåŠå…¶å€¼
    - summary: æ–‡æ¡£æ‘˜è¦
    
    åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
    """
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{encode_image(image_path)}",
                            "detail": "high"
                        }
                    }
                ]
            }
        ],
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)

# æå–å‘ç¥¨ä¿¡æ¯
invoice_info = extract_document_info("invoice.jpg")
print(invoice_info)
```

### 2. ä¼šè®®å½•éŸ³æ€»ç»“

```python
async def summarize_meeting(audio_path: str) -> dict:
    """ä¼šè®®å½•éŸ³è½¬å½•ä¸æ€»ç»“"""
    
    # 1. è½¬å½•éŸ³é¢‘
    transcript = transcribe_audio(audio_path, language="zh")
    
    # 2. ä½¿ç”¨ LLM æ€»ç»“
    summary_prompt = f"""è¯·æ€»ç»“ä»¥ä¸‹ä¼šè®®å†…å®¹ï¼š

{transcript['text']}

è¯·æä¾›ï¼š
1. ä¼šè®®ä¸»é¢˜
2. ä¸»è¦è®¨è®ºç‚¹ï¼ˆ3-5 ä¸ªï¼‰
3. å†³ç­–äº‹é¡¹
4. å¾…åŠäº‹é¡¹ï¼ˆåŒ…æ‹¬è´Ÿè´£äººå’Œæˆªæ­¢æ—¥æœŸï¼Œå¦‚æœæåˆ°çš„è¯ï¼‰
5. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚
"""
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": summary_prompt}],
        response_format={"type": "json_object"}
    )
    
    return {
        "transcript": transcript,
        "summary": json.loads(response.choices[0].message.content)
    }
```

### 3. äº§å“å›¾ç‰‡åˆ†æ

```python
def analyze_product_image(image_path: str) -> dict:
    """åˆ†æäº§å“å›¾ç‰‡ï¼Œç”Ÿæˆæè¿°å’Œæ ‡ç­¾"""
    
    prompt = """åˆ†æè¿™å¼ äº§å“å›¾ç‰‡ï¼Œæä¾›ï¼š
    1. äº§å“æè¿°ï¼ˆé€‚åˆç”µå•†ä½¿ç”¨ï¼Œ100å­—ä»¥å†…ï¼‰
    2. äº§å“ç‰¹ç‚¹ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
    3. å»ºè®®æ ‡ç­¾ï¼ˆ5-10ä¸ªå…³é”®è¯ï¼‰
    4. ç›®æ ‡å—ä¼—
    5. å»ºè®®å®šä»·åŒºé—´ï¼ˆå¦‚æœèƒ½åˆ¤æ–­çš„è¯ï¼‰
    
    ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚
    """
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{encode_image(image_path)}"}
                    }
                ]
            }
        ],
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)
```

### 4. æ™ºèƒ½å®¢æœï¼ˆå¤šæ¨¡æ€ï¼‰

```python
class MultimodalAssistant:
    """å¤šæ¨¡æ€æ™ºèƒ½åŠ©æ‰‹"""
    
    def __init__(self):
        self.client = OpenAI()
        self.conversation_history = []
    
    def process_message(self, text: str = None, image_path: str = None, audio_path: str = None) -> str:
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        content = []
        
        # å¤„ç†éŸ³é¢‘è¾“å…¥
        if audio_path:
            transcript = transcribe_audio(audio_path)
            text = transcript["text"]
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        if text:
            content.append({"type": "text", "text": text})
        
        if image_path:
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{encode_image(image_path)}"}
            })
        
        if not content:
            return "è¯·æä¾›æ–‡å­—ã€å›¾ç‰‡æˆ–è¯­éŸ³è¾“å…¥"
        
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        self.conversation_history.append({"role": "user", "content": content})
        
        # è°ƒç”¨æ¨¡å‹
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¤šæ¨¡æ€åŠ©æ‰‹ï¼Œå¯ä»¥ç†è§£æ–‡å­—ã€å›¾ç‰‡å’Œè¯­éŸ³ã€‚"},
                *self.conversation_history
            ]
        )
        
        assistant_message = response.choices[0].message.content
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        
        return assistant_message

# ä½¿ç”¨ç¤ºä¾‹
assistant = MultimodalAssistant()

# æ–‡å­—å¯¹è¯
print(assistant.process_message(text="ä½ å¥½ï¼"))

# å›¾ç‰‡ç†è§£
print(assistant.process_message(
    text="è¿™æ˜¯ä»€ä¹ˆäº§å“ï¼Ÿ",
    image_path="product.jpg"
))

# è¯­éŸ³è¾“å…¥
print(assistant.process_message(audio_path="question.mp3"))
```

## æˆæœ¬ä¸é™åˆ¶

### Vision æˆæœ¬

| å›¾ç‰‡å¤§å°   | ä½ç»†èŠ‚ (low) | é«˜ç»†èŠ‚ (high) |
| ---------- | ------------ | ------------- |
| 512x512    | 85 tokens    | 85 tokens     |
| 1024x1024  | 85 tokens    | 765 tokens    |
| 2048x2048  | 85 tokens    | 1105 tokens   |

### éŸ³é¢‘æˆæœ¬

| æœåŠ¡      | ä»·æ ¼              |
| --------- | ----------------- |
| Whisper   | $0.006 / åˆ†é’Ÿ     |
| TTS       | $15.00 / 1M å­—ç¬¦  |
| TTS-HD    | $30.00 / 1M å­—ç¬¦  |

### å›¾åƒç”Ÿæˆæˆæœ¬

| æ¨¡å‹     | åˆ†è¾¨ç‡    | è´¨é‡     | ä»·æ ¼      |
| -------- | --------- | -------- | --------- |
| DALL-E 3 | 1024x1024 | standard | $0.040    |
| DALL-E 3 | 1024x1024 | hd       | $0.080    |
| DALL-E 3 | 1792x1024 | standard | $0.080    |
| DALL-E 3 | 1792x1024 | hd       | $0.120    |

## å»¶ä¼¸é˜…è¯»

- [OpenAI Vision æ–‡æ¡£](https://platform.openai.com/docs/guides/vision)
- [OpenAI Audio æ–‡æ¡£](https://platform.openai.com/docs/guides/speech-to-text)
- [DALL-E æ–‡æ¡£](https://platform.openai.com/docs/guides/images)
- [Anthropic Vision æ–‡æ¡£](https://docs.anthropic.com/claude/docs/vision)
