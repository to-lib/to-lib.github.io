---
sidebar_position: 11
title: ğŸš€ Productionï¼ˆç”Ÿäº§åŒ–ä¸éƒ¨ç½²ï¼‰
---

# Productionï¼ˆç”Ÿäº§åŒ–ä¸éƒ¨ç½²ï¼‰

æŠŠ Demo å˜æˆå¯ç”¨çš„ç”Ÿäº§ç³»ç»Ÿï¼Œå…³é”®æ˜¯"ç¨³å®šã€å¯è§‚æµ‹ã€å¯æ§æˆæœ¬"ã€‚æœ¬é¡µä»å»¶è¿Ÿã€ç¨³å®šæ€§ã€ç¼“å­˜ã€è§‚æµ‹ä¸å‘å¸ƒç­–ç•¥æ€»ç»“å¸¸ç”¨åšæ³•ã€‚

## å…¸å‹ç”Ÿäº§æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        API Gateway                               â”‚
â”‚              (é‰´æƒã€é™æµã€é…é¢ã€è´Ÿè½½å‡è¡¡)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ LLM      â”‚    â”‚ RAG      â”‚    â”‚ Agent    â”‚
       â”‚ Service  â”‚    â”‚ Service  â”‚    â”‚ Service  â”‚
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚               â”‚               â”‚
            â–¼               â–¼               â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚           Observability Layer            â”‚
       â”‚     (æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ªã€è¯„ä¼°å›æ”¾)           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å»¶è¿Ÿä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
class ModelRouter:
    """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
    
    def __init__(self):
        self.models = {
            "simple": "gpt-4o-mini",      # ç®€å•ä»»åŠ¡
            "complex": "gpt-4o",           # å¤æ‚ä»»åŠ¡
            "code": "gpt-4o",              # ä»£ç ç”Ÿæˆ
        }
    
    def classify_task(self, prompt: str) -> str:
        """ç®€å•çš„ä»»åŠ¡åˆ†ç±»"""
        if len(prompt) < 100:
            return "simple"
        if any(kw in prompt.lower() for kw in ["ä»£ç ", "code", "function", "class"]):
            return "code"
        return "complex"
    
    def get_model(self, prompt: str) -> str:
        task_type = self.classify_task(prompt)
        return self.models[task_type]

router = ModelRouter()
model = router.get_model(user_prompt)
```

### 2. æµå¼è¾“å‡º

```python
from openai import OpenAI

client = OpenAI()

def stream_response(prompt: str):
    """æµå¼è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒ"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# FastAPI æµå¼å“åº”
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.post("/chat/stream")
async def chat_stream(prompt: str):
    return StreamingResponse(
        stream_response(prompt),
        media_type="text/event-stream"
    )
```

### 3. å¹¶è¡ŒåŒ–å¤„ç†

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def parallel_requests(prompts: list[str]) -> list[str]:
    """å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚"""
    tasks = [
        client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": p}]
        )
        for p in prompts
    ]
    
    responses = await asyncio.gather(*tasks)
    return [r.choices[0].message.content for r in responses]

# RAG åœºæ™¯ï¼šå¹¶è¡Œæ£€ç´¢å’Œç”Ÿæˆ
async def rag_parallel(query: str):
    """å¹¶è¡Œæ‰§è¡Œæ£€ç´¢å’ŒæŸ¥è¯¢é‡å†™"""
    rewrite_task = rewrite_query(query)
    retrieve_task = retrieve_documents(query)
    
    rewritten, docs = await asyncio.gather(rewrite_task, retrieve_task)
    
    # ä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢å†æ¬¡æ£€ç´¢
    more_docs = await retrieve_documents(rewritten)
    
    return generate_answer(query, docs + more_docs)
```

### 4. ä¸Šä¸‹æ–‡å‹ç¼©

```python
def compress_context(messages: list[dict], max_tokens: int = 4000) -> list[dict]:
    """å‹ç¼©å¯¹è¯å†å²"""
    import tiktoken
    enc = tiktoken.encoding_for_model("gpt-4o")
    
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    system_msg = [m for m in messages if m["role"] == "system"]
    other_msgs = [m for m in messages if m["role"] != "system"]
    
    # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹ä¿ç•™
    compressed = []
    total_tokens = sum(len(enc.encode(m["content"])) for m in system_msg)
    
    for msg in reversed(other_msgs):
        msg_tokens = len(enc.encode(msg["content"]))
        if total_tokens + msg_tokens > max_tokens:
            break
        compressed.insert(0, msg)
        total_tokens += msg_tokens
    
    return system_msg + compressed

def summarize_history(messages: list[dict]) -> str:
    """å°†å†å²å¯¹è¯æ€»ç»“ä¸ºæ‘˜è¦"""
    history_text = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"è¯·ç”¨ 100 å­—ä»¥å†…æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„è¦ç‚¹ï¼š\n{history_text}"
        }],
        max_tokens=150
    )
    
    return response.choices[0].message.content
```

## ç¨³å®šæ€§ä¸å®¹é”™

### 1. é‡è¯•ç­–ç•¥

```python
import time
from functools import wraps
from openai import RateLimitError, APIError

def retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):
    """æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except RateLimitError as e:
                    if attempt == max_retries - 1:
                        raise
                    delay = base_delay * (2 ** attempt)
                    print(f"Rate limited, retrying in {delay}s...")
                    time.sleep(delay)
                except APIError as e:
                    if e.status_code >= 500:  # æœåŠ¡ç«¯é”™è¯¯æ‰é‡è¯•
                        if attempt == max_retries - 1:
                            raise
                        time.sleep(base_delay)
                    else:
                        raise
            return None
        return wrapper
    return decorator

@retry_with_backoff(max_retries=3)
def call_llm(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
```

### 2. è¶…æ—¶ä¸å–æ¶ˆ

```python
import asyncio
from contextlib import asynccontextmanager

@asynccontextmanager
async def timeout_context(seconds: float):
    """è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield asyncio.wait_for(asyncio.sleep(0), timeout=seconds)
    except asyncio.TimeoutError:
        raise TimeoutError(f"Operation timed out after {seconds}s")

async def call_with_timeout(prompt: str, timeout: float = 30.0) -> str:
    """å¸¦è¶…æ—¶çš„ LLM è°ƒç”¨"""
    try:
        response = await asyncio.wait_for(
            client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}]
            ),
            timeout=timeout
        )
        return response.choices[0].message.content
    except asyncio.TimeoutError:
        return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
```

### 3. é™çº§ç­–ç•¥

```python
class LLMService:
    """å¸¦é™çº§ç­–ç•¥çš„ LLM æœåŠ¡"""
    
    def __init__(self):
        self.primary_model = "gpt-4o"
        self.fallback_model = "gpt-4o-mini"
        self.error_count = 0
        self.error_threshold = 5
        self.circuit_open = False
    
    async def call(self, prompt: str) -> str:
        # ç†”æ–­å™¨æ‰“å¼€æ—¶ç›´æ¥ä½¿ç”¨é™çº§æ¨¡å‹
        if self.circuit_open:
            return await self._call_fallback(prompt)
        
        try:
            response = await self._call_primary(prompt)
            self.error_count = 0  # æˆåŠŸåˆ™é‡ç½®é”™è¯¯è®¡æ•°
            return response
        except Exception as e:
            self.error_count += 1
            if self.error_count >= self.error_threshold:
                self.circuit_open = True
                print("âš ï¸ ç†”æ–­å™¨æ‰“å¼€ï¼Œåˆ‡æ¢åˆ°é™çº§æ¨¡å‹")
            return await self._call_fallback(prompt)
    
    async def _call_primary(self, prompt: str) -> str:
        response = await client.chat.completions.create(
            model=self.primary_model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    
    async def _call_fallback(self, prompt: str) -> str:
        response = await client.chat.completions.create(
            model=self.fallback_model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

## ç¼“å­˜ç­–ç•¥

### 1. è¯­ä¹‰ç¼“å­˜

```python
import hashlib
from typing import Optional
import numpy as np

class SemanticCache:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„ç¼“å­˜"""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.cache = {}  # {embedding_hash: (embedding, response)}
        self.threshold = similarity_threshold
    
    def _get_embedding(self, text: str) -> np.ndarray:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def get(self, query: str) -> Optional[str]:
        """æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„ç¼“å­˜"""
        query_embedding = self._get_embedding(query)
        
        for key, (cached_embedding, response) in self.cache.items():
            similarity = self._cosine_similarity(query_embedding, cached_embedding)
            if similarity >= self.threshold:
                return response
        
        return None
    
    def set(self, query: str, response: str):
        """è®¾ç½®ç¼“å­˜"""
        embedding = self._get_embedding(query)
        key = hashlib.md5(query.encode()).hexdigest()
        self.cache[key] = (embedding, response)

# ä½¿ç”¨ç¤ºä¾‹
cache = SemanticCache(similarity_threshold=0.95)

def cached_llm_call(prompt: str) -> str:
    # å…ˆæŸ¥ç¼“å­˜
    cached = cache.get(prompt)
    if cached:
        return cached
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ LLM
    response = call_llm(prompt)
    cache.set(prompt, response)
    return response
```

### 2. Redis ç¼“å­˜

```python
import redis
import json
import hashlib

class RedisCache:
    """Redis ç¼“å­˜"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, ttl: int = 3600):
        self.client = redis.Redis(host=host, port=port, decode_responses=True)
        self.ttl = ttl
    
    def _make_key(self, prompt: str, model: str) -> str:
        content = f"{model}:{prompt}"
        return f"llm_cache:{hashlib.sha256(content.encode()).hexdigest()}"
    
    def get(self, prompt: str, model: str) -> Optional[str]:
        key = self._make_key(prompt, model)
        return self.client.get(key)
    
    def set(self, prompt: str, model: str, response: str):
        key = self._make_key(prompt, model)
        self.client.setex(key, self.ttl, response)
```

## å¯è§‚æµ‹æ€§ï¼ˆObservabilityï¼‰

### 1. ç»“æ„åŒ–æ—¥å¿—

```python
import logging
import json
from datetime import datetime

class LLMLogger:
    """LLM è°ƒç”¨æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger("llm")
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log_request(self, request_id: str, model: str, prompt: str, 
                    response: str, latency_ms: float, tokens: dict):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
            "model": model,
            "prompt_preview": prompt[:100] + "..." if len(prompt) > 100 else prompt,
            "response_preview": response[:100] + "..." if len(response) > 100 else response,
            "latency_ms": latency_ms,
            "prompt_tokens": tokens.get("prompt_tokens", 0),
            "completion_tokens": tokens.get("completion_tokens", 0),
            "total_tokens": tokens.get("total_tokens", 0)
        }
        self.logger.info(json.dumps(log_entry, ensure_ascii=False))

llm_logger = LLMLogger()
```

### 2. æŒ‡æ ‡æ”¶é›†

```python
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
llm_requests_total = Counter(
    'llm_requests_total',
    'Total LLM requests',
    ['model', 'status']
)

llm_latency_seconds = Histogram(
    'llm_latency_seconds',
    'LLM request latency',
    ['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

llm_tokens_total = Counter(
    'llm_tokens_total',
    'Total tokens used',
    ['model', 'type']  # type: prompt/completion
)

# ä½¿ç”¨ç¤ºä¾‹
def call_llm_with_metrics(prompt: str, model: str) -> str:
    import time
    start = time.time()
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        latency = time.time() - start
        llm_requests_total.labels(model=model, status="success").inc()
        llm_latency_seconds.labels(model=model).observe(latency)
        llm_tokens_total.labels(model=model, type="prompt").inc(response.usage.prompt_tokens)
        llm_tokens_total.labels(model=model, type="completion").inc(response.usage.completion_tokens)
        
        return response.choices[0].message.content
    except Exception as e:
        llm_requests_total.labels(model=model, status="error").inc()
        raise
```

### 3. åˆ†å¸ƒå¼è¿½è¸ª

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# åˆå§‹åŒ–è¿½è¸ª
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# æ·»åŠ å¯¼å‡ºå™¨
otlp_exporter = OTLPSpanExporter(endpoint="http://localhost:4317")
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(otlp_exporter))

# ä½¿ç”¨è¿½è¸ª
@tracer.start_as_current_span("llm_call")
def traced_llm_call(prompt: str) -> str:
    span = trace.get_current_span()
    span.set_attribute("llm.model", "gpt-4o")
    span.set_attribute("llm.prompt_length", len(prompt))
    
    response = call_llm(prompt)
    
    span.set_attribute("llm.response_length", len(response))
    return response
```

## å‘å¸ƒä¸å›æ»š

### 1. ç°åº¦å‘å¸ƒ

```python
class GradualRollout:
    """ç°åº¦å‘å¸ƒç®¡ç†"""
    
    def __init__(self):
        self.rollout_percentage = 0.0
        self.new_config = None
        self.old_config = None
    
    def start_rollout(self, new_config: dict, initial_percentage: float = 0.05):
        """å¼€å§‹ç°åº¦å‘å¸ƒ"""
        self.old_config = self.get_current_config()
        self.new_config = new_config
        self.rollout_percentage = initial_percentage
    
    def increase_rollout(self, percentage: float):
        """å¢åŠ ç°åº¦æ¯”ä¾‹"""
        self.rollout_percentage = min(1.0, percentage)
    
    def get_config_for_request(self, user_id: str) -> dict:
        """è·å–è¯·æ±‚åº”ä½¿ç”¨çš„é…ç½®"""
        import hashlib
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        if (hash_value % 100) / 100 < self.rollout_percentage:
            return self.new_config
        return self.old_config
    
    def rollback(self):
        """å›æ»šåˆ°æ—§é…ç½®"""
        self.rollout_percentage = 0.0
        self.new_config = None
```

### 2. Feature Flag

```python
class FeatureFlags:
    """åŠŸèƒ½å¼€å…³ç®¡ç†"""
    
    def __init__(self):
        self.flags = {
            "use_new_prompt": False,
            "enable_rag": True,
            "use_gpt4": False,
            "enable_caching": True
        }
    
    def is_enabled(self, flag_name: str, user_id: str = None) -> bool:
        """æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        if flag_name not in self.flags:
            return False
        
        flag_value = self.flags[flag_name]
        
        # æ”¯æŒç™¾åˆ†æ¯”ç°åº¦
        if isinstance(flag_value, float):
            if user_id:
                import hashlib
                hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
                return (hash_value % 100) / 100 < flag_value
            return False
        
        return flag_value
    
    def set_flag(self, flag_name: str, value):
        """è®¾ç½®åŠŸèƒ½å¼€å…³"""
        self.flags[flag_name] = value

# ä½¿ç”¨ç¤ºä¾‹
flags = FeatureFlags()

def process_request(prompt: str, user_id: str) -> str:
    if flags.is_enabled("use_new_prompt", user_id):
        prompt = apply_new_prompt_template(prompt)
    
    if flags.is_enabled("enable_rag", user_id):
        context = retrieve_context(prompt)
        prompt = f"Context: {context}\n\nQuestion: {prompt}"
    
    model = "gpt-4o" if flags.is_enabled("use_gpt4", user_id) else "gpt-4o-mini"
    
    return call_llm(prompt, model)
```

## æˆæœ¬æ§åˆ¶

### é¢„ç®—ç®¡ç†

```python
from datetime import datetime, timedelta
from collections import defaultdict

class BudgetManager:
    """é¢„ç®—ç®¡ç†å™¨"""
    
    # æ¨¡å‹ä»·æ ¼ (USD per 1M tokens)
    PRICES = {
        "gpt-4o": {"input": 2.5, "output": 10.0},
        "gpt-4o-mini": {"input": 0.15, "output": 0.6},
    }
    
    def __init__(self, daily_budget_usd: float = 100.0):
        self.daily_budget = daily_budget_usd
        self.usage = defaultdict(float)  # {date: cost}
    
    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—å•æ¬¡è°ƒç”¨æˆæœ¬"""
        prices = self.PRICES.get(model, {"input": 0, "output": 0})
        input_cost = (input_tokens / 1_000_000) * prices["input"]
        output_cost = (output_tokens / 1_000_000) * prices["output"]
        return input_cost + output_cost
    
    def record_usage(self, model: str, input_tokens: int, output_tokens: int):
        """è®°å½•ä½¿ç”¨é‡"""
        cost = self.calculate_cost(model, input_tokens, output_tokens)
        today = datetime.now().strftime("%Y-%m-%d")
        self.usage[today] += cost
    
    def can_proceed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…å‡ºé¢„ç®—"""
        today = datetime.now().strftime("%Y-%m-%d")
        return self.usage[today] < self.daily_budget
    
    def get_remaining_budget(self) -> float:
        """è·å–å‰©ä½™é¢„ç®—"""
        today = datetime.now().strftime("%Y-%m-%d")
        return max(0, self.daily_budget - self.usage[today])

budget = BudgetManager(daily_budget_usd=50.0)

def call_with_budget_check(prompt: str) -> str:
    if not budget.can_proceed():
        raise Exception("Daily budget exceeded")
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    
    budget.record_usage(
        "gpt-4o",
        response.usage.prompt_tokens,
        response.usage.completion_tokens
    )
    
    return response.choices[0].message.content
```

## å»¶ä¼¸é˜…è¯»

- [OpenAI å»¶è¿Ÿä¼˜åŒ–æŒ‡å—](https://platform.openai.com/docs/guides/latency-optimization)
- [LangChain ç”Ÿäº§åŒ–æŒ‡å—](https://python.langchain.com/docs/guides/productionization/)
- [OpenTelemetry æ–‡æ¡£](https://opentelemetry.io/docs/)
