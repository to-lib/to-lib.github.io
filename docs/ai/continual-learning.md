---
sidebar_position: 40
title: ğŸ“š æŒç»­å­¦ä¹ 
---

# æŒç»­å­¦ä¹ ï¼ˆContinual Learningï¼‰

æŒç»­å­¦ä¹ æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿä¸æ–­å­¦ä¹ æ–°çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™æ—§çŸ¥è¯†çš„æŠ€æœ¯ï¼Œè§£å†³"ç¾éš¾æ€§é—å¿˜"é—®é¢˜ã€‚

## ç¾éš¾æ€§é—å¿˜

```
ä¼ ç»Ÿå¾®è°ƒï¼š
ä»»åŠ¡Aè®­ç»ƒ â”€â”€> æ¨¡å‹æ“…é•¿A
    â”‚
    â–¼
ä»»åŠ¡Bè®­ç»ƒ â”€â”€> æ¨¡å‹æ“…é•¿Bï¼Œä½†å¿˜è®°A âŒ

æŒç»­å­¦ä¹ ï¼š
ä»»åŠ¡Aè®­ç»ƒ â”€â”€> æ¨¡å‹æ“…é•¿A
    â”‚
    â–¼
ä»»åŠ¡Bè®­ç»ƒ â”€â”€> æ¨¡å‹åŒæ—¶æ“…é•¿Aå’ŒB âœ“
```

## æŒç»­å­¦ä¹ æ–¹æ³•

| æ–¹æ³• | åŸç† | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| Replay | é‡æ”¾æ—§æ•°æ® | æ•°æ®å¯å­˜å‚¨ |
| EWC | ä¿æŠ¤é‡è¦å‚æ•° | å‚æ•°çº§ä¿æŠ¤ |
| LoRA ç´¯åŠ  | ç‹¬ç«‹é€‚é…å™¨ | LLM å¾®è°ƒ |
| çŸ¥è¯†è’¸é¦ | ä¿ç•™æ—§æ¨¡å‹çŸ¥è¯† | æ¨¡å‹æ›´æ–° |

## ç»éªŒå›æ”¾ï¼ˆReplayï¼‰

```python
import random
from collections import deque

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, max_size: int = 10000):
        self.buffer = deque(maxlen=max_size)
    
    def add(self, samples: list):
        """æ·»åŠ æ–°æ ·æœ¬"""
        self.buffer.extend(samples)
    
    def sample(self, batch_size: int) -> list:
        """éšæœºé‡‡æ ·"""
        return random.sample(list(self.buffer), min(batch_size, len(self.buffer)))

class ContinualTrainer:
    """æŒç»­å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, model, replay_ratio: float = 0.3):
        self.model = model
        self.replay_buffer = ReplayBuffer()
        self.replay_ratio = replay_ratio
    
    def train_task(self, new_data: list, epochs: int = 3):
        """è®­ç»ƒæ–°ä»»åŠ¡"""
        for epoch in range(epochs):
            # æ··åˆæ–°æ•°æ®å’Œå›æ”¾æ•°æ®
            replay_size = int(len(new_data) * self.replay_ratio)
            replay_data = self.replay_buffer.sample(replay_size)
            
            combined_data = new_data + replay_data
            random.shuffle(combined_data)
            
            # è®­ç»ƒ
            for batch in self._batch(combined_data):
                self._train_step(batch)
        
        # å°†æ–°æ•°æ®åŠ å…¥å›æ”¾ç¼“å†²åŒº
        self.replay_buffer.add(new_data)
    
    def _batch(self, data, batch_size=32):
        for i in range(0, len(data), batch_size):
            yield data[i:i + batch_size]
    
    def _train_step(self, batch):
        # è®­ç»ƒé€»è¾‘
        pass
```

## EWCï¼ˆå¼¹æ€§æƒé‡å·©å›ºï¼‰

```python
import torch
import torch.nn as nn
from copy import deepcopy

class EWC:
    """Elastic Weight Consolidation"""
    
    def __init__(self, model: nn.Module, lambda_ewc: float = 1000):
        self.model = model
        self.lambda_ewc = lambda_ewc
        self.fisher = {}
        self.optimal_params = {}
    
    def compute_fisher(self, dataloader):
        """è®¡ç®— Fisher ä¿¡æ¯çŸ©é˜µ"""
        self.fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}
        
        self.model.eval()
        for batch in dataloader:
            self.model.zero_grad()
            output = self.model(batch["input_ids"])
            loss = output.loss
            loss.backward()
            
            for n, p in self.model.named_parameters():
                if p.grad is not None:
                    self.fisher[n] += p.grad.data ** 2
        
        # å½’ä¸€åŒ–
        for n in self.fisher:
            self.fisher[n] /= len(dataloader)
        
        # ä¿å­˜æœ€ä¼˜å‚æ•°
        self.optimal_params = {n: p.clone() for n, p in self.model.named_parameters()}
    
    def ewc_loss(self) -> torch.Tensor:
        """è®¡ç®— EWC æ­£åˆ™åŒ–æŸå¤±"""
        loss = 0
        for n, p in self.model.named_parameters():
            if n in self.fisher:
                loss += (self.fisher[n] * (p - self.optimal_params[n]) ** 2).sum()
        return self.lambda_ewc * loss

def train_with_ewc(model, ewc, dataloader, optimizer, epochs=3):
    """å¸¦ EWC çš„è®­ç»ƒ"""
    for epoch in range(epochs):
        for batch in dataloader:
            optimizer.zero_grad()
            
            # ä»»åŠ¡æŸå¤±
            output = model(batch["input_ids"], labels=batch["labels"])
            task_loss = output.loss
            
            # EWC æŸå¤±
            ewc_loss = ewc.ewc_loss()
            
            # æ€»æŸå¤±
            total_loss = task_loss + ewc_loss
            total_loss.backward()
            optimizer.step()
```

## LoRA ç´¯åŠ 

```python
from peft import LoraConfig, get_peft_model, PeftModel

class LoRAContinualLearning:
    """åŸºäº LoRA çš„æŒç»­å­¦ä¹ """
    
    def __init__(self, base_model_path: str):
        self.base_model_path = base_model_path
        self.adapters = {}  # task_name -> adapter_path
    
    def train_task(self, task_name: str, train_data, output_dir: str):
        """ä¸ºæ–°ä»»åŠ¡è®­ç»ƒ LoRA é€‚é…å™¨"""
        from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(self.base_model_path)
        
        # æ·»åŠ  LoRA
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05
        )
        model = get_peft_model(model, lora_config)
        
        # è®­ç»ƒ
        trainer = Trainer(
            model=model,
            train_dataset=train_data,
            args=TrainingArguments(output_dir=output_dir, num_train_epochs=3)
        )
        trainer.train()
        
        # ä¿å­˜é€‚é…å™¨
        model.save_pretrained(output_dir)
        self.adapters[task_name] = output_dir
    
    def load_for_task(self, task_name: str):
        """åŠ è½½ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹"""
        model = AutoModelForCausalLM.from_pretrained(self.base_model_path)
        model = PeftModel.from_pretrained(model, self.adapters[task_name])
        return model
    
    def merge_adapters(self, task_names: list, weights: list = None):
        """åˆå¹¶å¤šä¸ªé€‚é…å™¨"""
        if weights is None:
            weights = [1.0 / len(task_names)] * len(task_names)
        
        model = AutoModelForCausalLM.from_pretrained(self.base_model_path)
        
        # åŠ è½½å¹¶åŠ æƒåˆå¹¶é€‚é…å™¨
        merged_state = None
        for task_name, weight in zip(task_names, weights):
            adapter = PeftModel.from_pretrained(model, self.adapters[task_name])
            adapter_state = adapter.state_dict()
            
            if merged_state is None:
                merged_state = {k: v * weight for k, v in adapter_state.items()}
            else:
                for k, v in adapter_state.items():
                    merged_state[k] += v * weight
        
        model.load_state_dict(merged_state, strict=False)
        return model
```

## çŸ¥è¯†è’¸é¦ä¿ç•™

```python
class DistillationContinualLearning:
    """åŸºäºè’¸é¦çš„æŒç»­å­¦ä¹ """
    
    def __init__(self, model, temperature: float = 2.0, alpha: float = 0.5):
        self.current_model = model
        self.old_model = None
        self.temperature = temperature
        self.alpha = alpha
    
    def before_task(self):
        """ä»»åŠ¡å¼€å§‹å‰ä¿å­˜æ—§æ¨¡å‹"""
        self.old_model = deepcopy(self.current_model)
        self.old_model.eval()
        for p in self.old_model.parameters():
            p.requires_grad = False
    
    def compute_loss(self, inputs, labels):
        """è®¡ç®—å¸¦è’¸é¦çš„æŸå¤±"""
        # å½“å‰æ¨¡å‹è¾“å‡º
        current_outputs = self.current_model(inputs)
        current_logits = current_outputs.logits
        
        # ä»»åŠ¡æŸå¤±
        task_loss = nn.CrossEntropyLoss()(
            current_logits.view(-1, current_logits.size(-1)),
            labels.view(-1)
        )
        
        if self.old_model is None:
            return task_loss
        
        # è’¸é¦æŸå¤±
        with torch.no_grad():
            old_logits = self.old_model(inputs).logits
        
        distill_loss = nn.KLDivLoss(reduction="batchmean")(
            nn.functional.log_softmax(current_logits / self.temperature, dim=-1),
            nn.functional.softmax(old_logits / self.temperature, dim=-1)
        ) * (self.temperature ** 2)
        
        return self.alpha * distill_loss + (1 - self.alpha) * task_loss
```


## LLM æŒç»­é¢„è®­ç»ƒ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments

def continual_pretrain(
    model_path: str,
    new_corpus_path: str,
    output_dir: str,
    replay_corpus_path: str = None
):
    """LLM æŒç»­é¢„è®­ç»ƒ"""
    model = AutoModelForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # åŠ è½½æ–°è¯­æ–™
    from datasets import load_dataset
    new_data = load_dataset("text", data_files=new_corpus_path)
    
    # å¯é€‰ï¼šæ··åˆæ—§è¯­æ–™
    if replay_corpus_path:
        old_data = load_dataset("text", data_files=replay_corpus_path)
        # æ··åˆæ¯”ä¾‹ 8:2
        combined = concatenate_datasets([
            new_data["train"].select(range(int(len(new_data["train"]) * 0.8))),
            old_data["train"].select(range(int(len(old_data["train"]) * 0.2)))
        ])
    else:
        combined = new_data["train"]
    
    # è®­ç»ƒé…ç½®
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=4,
        learning_rate=1e-5,  # è¾ƒå°çš„å­¦ä¹ ç‡
        warmup_ratio=0.1,
        save_strategy="epoch"
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=combined,
        tokenizer=tokenizer
    )
    
    trainer.train()
```

## è¯„ä¼°é—å¿˜ç¨‹åº¦

```python
def evaluate_forgetting(model, task_datasets: dict) -> dict:
    """è¯„ä¼°å„ä»»åŠ¡çš„é—å¿˜ç¨‹åº¦"""
    results = {}
    
    for task_name, dataset in task_datasets.items():
        # è¯„ä¼°å½“å‰æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°
        score = evaluate_task(model, dataset)
        results[task_name] = score
    
    return results

def compute_forgetting_rate(
    scores_before: dict,
    scores_after: dict
) -> dict:
    """è®¡ç®—é—å¿˜ç‡"""
    forgetting = {}
    
    for task in scores_before:
        if task in scores_after:
            forgetting[task] = (scores_before[task] - scores_after[task]) / scores_before[task]
    
    return forgetting
```

## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| Replay | ç®€å•æœ‰æ•ˆ | éœ€è¦å­˜å‚¨æ•°æ® |
| EWC | ä¸éœ€è¦æ—§æ•°æ® | è®¡ç®— Fisher å¼€é”€å¤§ |
| LoRA ç´¯åŠ  | çµæ´»ã€å¯ç»„åˆ | å¤šä¸ªé€‚é…å™¨ç®¡ç†å¤æ‚ |
| è’¸é¦ | æ•ˆæœå¥½ | éœ€è¦ä¿å­˜æ—§æ¨¡å‹ |

## æœ€ä½³å®è·µ

1. **æ··åˆä½¿ç”¨**ï¼šReplay + æ­£åˆ™åŒ–æ•ˆæœæ›´å¥½
2. **æ§åˆ¶å­¦ä¹ ç‡**ï¼šæŒç»­å­¦ä¹ ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
3. **å®šæœŸè¯„ä¼°**ï¼šç›‘æ§æ—§ä»»åŠ¡çš„æ€§èƒ½
4. **æ•°æ®å¹³è¡¡**ï¼šæ–°æ—§æ•°æ®æ¯”ä¾‹è¦åˆç†
5. **é€‰æ‹©æ€§æ›´æ–°**ï¼šåªæ›´æ–°éƒ¨åˆ†å‚æ•°ï¼ˆå¦‚ LoRAï¼‰

## å»¶ä¼¸é˜…è¯»

- [EWC Paper](https://arxiv.org/abs/1612.00796)
- [Continual Learning Survey](https://arxiv.org/abs/2302.00487)
- [LLM Continual Learning](https://arxiv.org/abs/2308.04014)