---
sidebar_position: 2
title: ğŸ§  LLM åŸºç¡€çŸ¥è¯†
description: æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ ¸å¿ƒæ¦‚å¿µã€Transformer æ¶æ„ã€Token è®¡ç®—ã€ç”Ÿæˆå‚æ•°ä»¥åŠä¸»æµæ¨¡å‹ï¼ˆå¦‚ GPT-4o, Claude 3.5, Llama 3ï¼‰çš„å¯¹æ¯”ã€‚
keywords:
  [
    LLM åŸºç¡€,
    Transformer,
    Token,
    ä¸Šä¸‹æ–‡çª—å£,
    Temperature,
    Top-P,
    GPT-4o,
    Claude 3.5,
    Llama 3,
  ]
---

# LLM åŸºç¡€çŸ¥è¯†

å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Model, LLM) æ˜¯ä¸€ç±»åŸºäºæ·±åº¦å­¦ä¹ çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚æœ¬æ–‡ä»‹ç» LLM çš„æ ¸å¿ƒæ¦‚å¿µå’Œå·¥ä½œåŸç†ã€‚

## Transformer æ¶æ„

ç°ä»£ LLM å‡ ä¹éƒ½åŸºäº **Transformer** æ¶æ„ï¼Œè¿™æ˜¯ 2017 å¹´ Google åœ¨è®ºæ–‡ã€ŠAttention Is All You Needã€‹ä¸­æå‡ºçš„ã€‚

### æ ¸å¿ƒç»„ä»¶

```
è¾“å…¥æ–‡æœ¬ â†’ Tokenization â†’ Embedding â†’ Transformer Layers â†’ è¾“å‡ºæ¦‚ç‡
```

| ç»„ä»¶                     | ä½œç”¨                                       |
| ------------------------ | ------------------------------------------ |
| **Self-Attention**       | æ•æ‰åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»       |
| **Multi-Head Attention** | å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰ä¸åŒç±»å‹çš„å…³ç³» |
| **Feed-Forward Network** | å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹è¿›è¡Œéçº¿æ€§å˜æ¢               |
| **Layer Normalization**  | ç¨³å®šè®­ç»ƒè¿‡ç¨‹                               |

### æ¨¡å‹ç±»å‹

| ç±»å‹                | ä»£è¡¨æ¨¡å‹           | ç‰¹ç‚¹                       |
| ------------------- | ------------------ | -------------------------- |
| **Encoder-only**    | BERT               | åŒå‘ç†è§£ï¼Œé€‚åˆåˆ†ç±»ã€NER    |
| **Decoder-only**    | GPT, LLaMA, Claude | è‡ªå›å½’ç”Ÿæˆï¼Œé€‚åˆæ–‡æœ¬ç”Ÿæˆ   |
| **Encoder-Decoder** | T5, BART           | åºåˆ—åˆ°åºåˆ—ï¼Œé€‚åˆç¿»è¯‘ã€æ‘˜è¦ |

## Token ä¸ä¸Šä¸‹æ–‡çª—å£

### ä»€ä¹ˆæ˜¯ Tokenï¼Ÿ

Token æ˜¯æ¨¡å‹å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚ä¸€ä¸ª token å¯èƒ½æ˜¯ï¼š

- ä¸€ä¸ªå®Œæ•´çš„å•è¯ï¼š`hello`
- å•è¯çš„ä¸€éƒ¨åˆ†ï¼š`un` + `believ` + `able`
- ä¸€ä¸ªä¸­æ–‡å­—ï¼š`ä½ ` `å¥½`
- æ ‡ç‚¹ç¬¦å·ï¼š`,` `.`

**ç»éªŒæ³•åˆ™**ï¼š

- è‹±æ–‡ï¼š1 token â‰ˆ 4 ä¸ªå­—ç¬¦ æˆ– 0.75 ä¸ªå•è¯
- ä¸­æ–‡ï¼š1 token â‰ˆ 1-2 ä¸ªæ±‰å­—

### ä¸Šä¸‹æ–‡çª—å£ (Context Window)

ä¸Šä¸‹æ–‡çª—å£æ˜¯æ¨¡å‹ä¸€æ¬¡èƒ½å¤„ç†çš„æœ€å¤§ token æ•°é‡ã€‚

| æ¨¡å‹           | ä¸Šä¸‹æ–‡é•¿åº¦ |
| -------------- | ---------- |
| GPT-4o         | 128K       |
| Claude 3.5     | 200K       |
| Gemini 1.5 Pro | 1M - 2M    |
| LLaMA 3.1      | 128K       |
| Qwen 2.5       | 32K - 128K |

:::tip è®¡ç®— Token
ä½¿ç”¨ tiktoken (OpenAI) æˆ– Hugging Face tokenizers åº“è®¡ç®— token æ•°é‡ï¼š

```python
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4o")
tokens = enc.encode("Hello, world!")
print(len(tokens))  # 4
```

:::

## ç”Ÿæˆå‚æ•°

### Temperature (æ¸©åº¦)

æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚

| å€¼   | æ•ˆæœ                                 |
| ---- | ------------------------------------ |
| 0.0  | ç¡®å®šæ€§è¾“å‡ºï¼Œæ€»æ˜¯é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„ token |
| 0.7  | å¹³è¡¡åˆ›é€ æ€§å’Œä¸€è‡´æ€§ (æ¨èé»˜è®¤å€¼)      |
| 1.0+ | é«˜åˆ›é€ æ€§ï¼Œå¯èƒ½äº§ç”Ÿæ„å¤–æˆ–ä¸è¿è´¯çš„è¾“å‡º |

### Top-P (æ ¸é‡‡æ ·)

åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P çš„æœ€å° token é›†åˆä¸­é‡‡æ ·ã€‚

```
top_p=0.9 â†’ åªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡å‰ 90% çš„ tokens
```

### Max Tokens

é™åˆ¶ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚æ³¨æ„ï¼šè¾“å…¥ + è¾“å‡ºä¸èƒ½è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£ã€‚

### å…¶ä»–å‚æ•°

| å‚æ•°                | ä½œç”¨                           |
| ------------------- | ------------------------------ |
| `frequency_penalty` | æƒ©ç½šå·²å‡ºç°è¿‡çš„ tokenï¼Œå‡å°‘é‡å¤ |
| `presence_penalty`  | æƒ©ç½šå·²å‡ºç°çš„ä¸»é¢˜ï¼Œå¢åŠ å¤šæ ·æ€§   |
| `stop`              | åœæ­¢è¯åºåˆ—ï¼Œé‡åˆ°æ—¶åœæ­¢ç”Ÿæˆ     |

## ä¸»æµæ¨¡å‹å¯¹æ¯”

### å•†ä¸šæ¨¡å‹

| æ¨¡å‹                  | æä¾›å•†    | ç‰¹ç‚¹                                    |
| --------------------- | --------- | --------------------------------------- |
| **GPT-4o**            | OpenAI    | å¤šæ¨¡æ€ï¼Œé€Ÿåº¦å¿«ï¼Œæ€§ä»·æ¯”æé«˜ï¼Œç»¼åˆèƒ½åŠ›å¼º  |
| **Claude 3.5 Sonnet** | Anthropic | ç¼–ç èƒ½åŠ›æå¼ºï¼Œé€»è¾‘æ¨ç†å‡ºè‰²ï¼ŒUI è®¾è®¡å‹å¥½ |
| **Gemini 1.5 Pro**    | Google    | 2M è¶…é•¿ä¸Šä¸‹æ–‡ï¼ŒåŸç”Ÿå¤šæ¨¡æ€ï¼Œç”Ÿæ€é›†æˆå¥½   |
| **DeepSeek V3**       | DeepSeek  | å›½äº§ä¹‹å…‰ï¼Œå¼€æºé—­æºçš†å¼ºï¼Œç¼–ç ä¸æ¨ç†ä¸€æµ  |
| **Doubao (è±†åŒ…)**     | å­—èŠ‚è·³åŠ¨  | è¯­éŸ³äº¤äº’å¼ºï¼ŒC ç«¯åº”ç”¨å¹¿æ³›ï¼ŒAPI ä»·æ ¼ä½    |

### å¼€æºæ¨¡å‹

| æ¨¡å‹         | å‚æ•°é‡     | ç‰¹ç‚¹                |
| ------------ | ---------- | ------------------- |
| **LLaMA 3**  | 8B / 70B   | Meta å‡ºå“ï¼Œæ€§èƒ½ä¼˜ç§€ |
| **Mistral**  | 7B         | å°å·§é«˜æ•ˆï¼Œé€‚åˆéƒ¨ç½²  |
| **Qwen 2**   | 0.5B - 72B | é˜¿é‡Œå‡ºå“ï¼Œä¸­è‹±æ–‡ä½³  |
| **DeepSeek** | 7B / 67B   | å›½äº§ï¼Œä»£ç èƒ½åŠ›å¼º    |

## API è°ƒç”¨ç¤ºä¾‹

### OpenAI API

```python
from openai import OpenAI

client = OpenAI(api_key="your-api-key")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is machine learning?"}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)
```

### Anthropic API

```python
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Explain transformers in simple terms."}
    ]
)

print(message.content[0].text)
```

## å»¶ä¼¸é˜…è¯»

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [Anthropic API æ–‡æ¡£](https://docs.anthropic.com)
- [Hugging Face æ¨¡å‹åº“](https://huggingface.co/models)
