---
sidebar_position: 10
title: ğŸ“ Evaluationï¼ˆè¯„ä¼°ä¸æµ‹è¯•ï¼‰
---

# Evaluationï¼ˆè¯„ä¼°ä¸æµ‹è¯•ï¼‰

LLM åº”ç”¨çš„éš¾ç‚¹ä¹‹ä¸€æ˜¯"çœ‹èµ·æ¥èƒ½ç”¨"ï¼Œä½†çº¿ä¸Šè¡¨ç°ä¸ç¨³å®šã€‚ä¸€ä¸ªå¯æŒç»­è¿­ä»£çš„ AI ç³»ç»Ÿï¼Œéœ€è¦æŠŠè¯„ä¼°å½“æˆå·¥ç¨‹èƒ½åŠ›ï¼šå¯å¤ç°ã€å¯å¯¹æ¯”ã€å¯å›å½’ã€‚

## è¯„ä¼°ç›®æ ‡

| ç»´åº¦         | è¯´æ˜                               |
| ------------ | ---------------------------------- |
| **æ­£ç¡®æ€§**   | ç­”æ¡ˆæ˜¯å¦ç¬¦åˆäº‹å®/ä¸šåŠ¡è§„åˆ™          |
| **ç›¸å…³æ€§**   | å›ç­”æ˜¯å¦åˆ‡ä¸­ç”¨æˆ·é—®é¢˜               |
| **å®Œæ•´æ€§**   | å…³é”®ç‚¹æ˜¯å¦é—æ¼                     |
| **å®‰å…¨æ€§**   | æ˜¯å¦æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€æ˜¯å¦éµå®ˆç­–ç•¥     |
| **æ ¼å¼/ç»“æ„** | JSON/è¡¨æ ¼/å­—æ®µæ˜¯å¦ç¬¦åˆçº¦æŸ         |
| **æˆæœ¬ä¸å»¶è¿Ÿ** | æ˜¯å¦æ»¡è¶³ SLA                       |

## è¯„ä¼°ä½“ç³»æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¯„ä¼°ä½“ç³»                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç¦»çº¿è¯„ä¼° (Offline)          â”‚  åœ¨çº¿è¯„ä¼° (Online)        â”‚
â”‚  â”œâ”€ é»„é‡‘æ•°æ®é›†               â”‚  â”œâ”€ A/B æµ‹è¯•              â”‚
â”‚  â”œâ”€ è‡ªåŠ¨åŒ–è¯„åˆ†               â”‚  â”œâ”€ ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§          â”‚
â”‚  â”œâ”€ LLM-as-Judge            â”‚  â”œâ”€ ç”¨æˆ·åé¦ˆæ”¶é›†          â”‚
â”‚  â””â”€ äººå·¥æŠ½æ£€                 â”‚  â””â”€ å¼‚å¸¸æ£€æµ‹              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ç¦»çº¿è¯„ä¼°ï¼ˆOffline Evalï¼‰

### 1. æ„å»ºé»„é‡‘æ•°æ®é›†ï¼ˆGolden Setï¼‰

```python
from dataclasses import dataclass
from typing import Optional
import json

@dataclass
class EvalSample:
    """è¯„ä¼°æ ·æœ¬"""
    id: str
    input: str
    expected_output: str
    context: Optional[str] = None  # RAG åœºæ™¯çš„æ£€ç´¢ç»“æœ
    category: str = "general"
    difficulty: str = "medium"
    
    def to_dict(self):
        return {
            "id": self.id,
            "input": self.input,
            "expected_output": self.expected_output,
            "context": self.context,
            "category": self.category,
            "difficulty": self.difficulty
        }

# åˆ›å»ºè¯„ä¼°æ•°æ®é›†
eval_dataset = [
    EvalSample(
        id="001",
        input="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
        expected_output="RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„ AI æŠ€æœ¯...",
        category="concept",
        difficulty="easy"
    ),
    EvalSample(
        id="002",
        input="å¦‚ä½•ä¼˜åŒ– LLM çš„å“åº”å»¶è¿Ÿï¼Ÿ",
        expected_output="ä¼˜åŒ– LLM å“åº”å»¶è¿Ÿçš„æ–¹æ³•åŒ…æ‹¬ï¼š1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹...",
        category="optimization",
        difficulty="hard"
    )
]

# ä¿å­˜ä¸º JSONL
def save_eval_dataset(samples: list[EvalSample], path: str):
    with open(path, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample.to_dict(), ensure_ascii=False) + '\n')
```

### 2. è‡ªåŠ¨åŒ–è¯„åˆ†

#### è§„åˆ™/æ–­è¨€è¯„åˆ†

```python
import json
import re

class RuleBasedEvaluator:
    """åŸºäºè§„åˆ™çš„è¯„ä¼°å™¨"""
    
    @staticmethod
    def check_json_valid(output: str) -> bool:
        """æ£€æŸ¥ JSON æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        try:
            json.loads(output)
            return True
        except json.JSONDecodeError:
            return False
    
    @staticmethod
    def check_contains_keywords(output: str, keywords: list[str]) -> float:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯"""
        found = sum(1 for kw in keywords if kw.lower() in output.lower())
        return found / len(keywords) if keywords else 0
    
    @staticmethod
    def check_length(output: str, min_len: int = 10, max_len: int = 1000) -> bool:
        """æ£€æŸ¥é•¿åº¦æ˜¯å¦åœ¨èŒƒå›´å†…"""
        return min_len <= len(output) <= max_len
    
    @staticmethod
    def check_no_sensitive_info(output: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯"""
        patterns = [
            r'\b\d{11}\b',  # æ‰‹æœºå·
            r'\b\d{18}\b',  # èº«ä»½è¯å·
            r'sk-[a-zA-Z0-9]+',  # API Key
        ]
        for pattern in patterns:
            if re.search(pattern, output):
                return False
        return True

# ä½¿ç”¨ç¤ºä¾‹
evaluator = RuleBasedEvaluator()
output = '{"result": "success", "data": [1, 2, 3]}'

results = {
    "json_valid": evaluator.check_json_valid(output),
    "length_ok": evaluator.check_length(output),
    "no_sensitive": evaluator.check_no_sensitive_info(output)
}
```

#### æŒ‡æ ‡è®¡ç®—

```python
from collections import Counter
import numpy as np

def calculate_accuracy(predictions: list[str], labels: list[str]) -> float:
    """è®¡ç®—å‡†ç¡®ç‡"""
    correct = sum(1 for p, l in zip(predictions, labels) if p.strip() == l.strip())
    return correct / len(predictions)

def calculate_f1(predictions: list[str], labels: list[str], positive_label: str) -> dict:
    """è®¡ç®— F1 åˆ†æ•°"""
    tp = sum(1 for p, l in zip(predictions, labels) if p == positive_label and l == positive_label)
    fp = sum(1 for p, l in zip(predictions, labels) if p == positive_label and l != positive_label)
    fn = sum(1 for p, l in zip(predictions, labels) if p != positive_label and l == positive_label)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {"precision": precision, "recall": recall, "f1": f1}

def calculate_retrieval_metrics(retrieved_ids: list[list[str]], relevant_ids: list[list[str]], k: int = 5) -> dict:
    """è®¡ç®—æ£€ç´¢æŒ‡æ ‡"""
    recall_at_k = []
    mrr = []
    
    for retrieved, relevant in zip(retrieved_ids, relevant_ids):
        # Recall@K
        retrieved_k = set(retrieved[:k])
        relevant_set = set(relevant)
        recall = len(retrieved_k & relevant_set) / len(relevant_set) if relevant_set else 0
        recall_at_k.append(recall)
        
        # MRR
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant_set:
                mrr.append(1 / (i + 1))
                break
        else:
            mrr.append(0)
    
    return {
        f"recall@{k}": np.mean(recall_at_k),
        "mrr": np.mean(mrr)
    }
```

### 3. LLM-as-Judge

```python
from openai import OpenAI

client = OpenAI()

def llm_judge(question: str, answer: str, reference: str, criteria: str = "accuracy") -> dict:
    """ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…"""
    
    judge_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°å›ç­”çš„è´¨é‡ã€‚

è¯„ä¼°æ ‡å‡†ï¼š{criteria}

é—®é¢˜ï¼š{question}

å‚è€ƒç­”æ¡ˆï¼š{reference}

å¾…è¯„ä¼°ç­”æ¡ˆï¼š{answer}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
1. åˆ†æ•° (1-5)ï¼š
2. ç†ç”±ï¼š
3. æ”¹è¿›å»ºè®®ï¼š

åªè¾“å‡º JSON æ ¼å¼ï¼š
{{"score": <1-5>, "reason": "<ç†ç”±>", "suggestion": "<å»ºè®®>"}}
"""
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": judge_prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)

# æ‰¹é‡è¯„ä¼°
def batch_evaluate(samples: list[dict], judge_model: str = "gpt-4o") -> list[dict]:
    """æ‰¹é‡è¯„ä¼°"""
    results = []
    for sample in samples:
        result = llm_judge(
            question=sample["input"],
            answer=sample["output"],
            reference=sample["expected"]
        )
        results.append({
            "id": sample["id"],
            **result
        })
    return results
```

### 4. RAG ä¸“é¡¹è¯„ä¼°

```python
def evaluate_rag_faithfulness(answer: str, context: str) -> dict:
    """è¯„ä¼° RAG å›ç­”çš„å¿ å®åº¦ï¼ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼‰"""
    
    prompt = f"""è¯„ä¼°ä»¥ä¸‹å›ç­”æ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

å›ç­”ï¼š
{answer}

è¯„ä¼°æ ‡å‡†ï¼š
- å›ç­”ä¸­çš„æ‰€æœ‰äº‹å®æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®
- æ˜¯å¦æœ‰ç¼–é€ æˆ–è‡†æµ‹çš„å†…å®¹

è¾“å‡º JSONï¼š{{"faithfulness_score": <0-1>, "unsupported_claims": ["<ä¸æ”¯æŒçš„å£°æ˜>"]}}
"""
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)

def evaluate_rag_relevancy(question: str, answer: str) -> dict:
    """è¯„ä¼°å›ç­”ä¸é—®é¢˜çš„ç›¸å…³æ€§"""
    
    prompt = f"""è¯„ä¼°ä»¥ä¸‹å›ç­”ä¸é—®é¢˜çš„ç›¸å…³æ€§ã€‚

é—®é¢˜ï¼š{question}

å›ç­”ï¼š{answer}

è¯„ä¼°æ ‡å‡†ï¼š
- å›ç­”æ˜¯å¦ç›´æ¥å›åº”äº†é—®é¢˜
- æ˜¯å¦æœ‰æ— å…³çš„å†…å®¹

è¾“å‡º JSONï¼š{{"relevancy_score": <0-1>, "irrelevant_parts": ["<æ— å…³éƒ¨åˆ†>"]}}
"""
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)
```

## åœ¨çº¿è¯„ä¼°ï¼ˆOnline Evalï¼‰

### A/B æµ‹è¯•æ¡†æ¶

```python
import random
import hashlib
from datetime import datetime

class ABTestManager:
    """A/B æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.experiments = {}
    
    def create_experiment(self, name: str, variants: dict[str, float]):
        """åˆ›å»ºå®éªŒï¼Œvariants ä¸ºå˜ä½“åç§°å’Œæµé‡æ¯”ä¾‹"""
        self.experiments[name] = {
            "variants": variants,
            "created_at": datetime.now(),
            "metrics": {v: [] for v in variants}
        }
    
    def get_variant(self, experiment_name: str, user_id: str) -> str:
        """æ ¹æ®ç”¨æˆ· ID åˆ†é…å˜ä½“ï¼ˆç¡®ä¿åŒä¸€ç”¨æˆ·å§‹ç»ˆåˆ†åˆ°åŒä¸€ç»„ï¼‰"""
        exp = self.experiments[experiment_name]
        
        # ä½¿ç”¨ hash ç¡®ä¿åˆ†é…ä¸€è‡´æ€§
        hash_value = int(hashlib.md5(f"{experiment_name}:{user_id}".encode()).hexdigest(), 16)
        random_value = (hash_value % 10000) / 10000
        
        cumulative = 0
        for variant, ratio in exp["variants"].items():
            cumulative += ratio
            if random_value < cumulative:
                return variant
        
        return list(exp["variants"].keys())[-1]
    
    def record_metric(self, experiment_name: str, variant: str, metric_name: str, value: float):
        """è®°å½•æŒ‡æ ‡"""
        self.experiments[experiment_name]["metrics"][variant].append({
            "metric": metric_name,
            "value": value,
            "timestamp": datetime.now()
        })

# ä½¿ç”¨ç¤ºä¾‹
ab_manager = ABTestManager()
ab_manager.create_experiment("prompt_v2", {
    "control": 0.5,    # 50% ä½¿ç”¨æ—§ prompt
    "treatment": 0.5   # 50% ä½¿ç”¨æ–° prompt
})

# è·å–ç”¨æˆ·åˆ†ç»„
variant = ab_manager.get_variant("prompt_v2", user_id="user123")
```

### ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

```python
from dataclasses import dataclass, field
from typing import Optional
import time

@dataclass
class RequestMetrics:
    """è¯·æ±‚æŒ‡æ ‡"""
    request_id: str
    user_id: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    latency_ms: float
    success: bool
    error_type: Optional[str] = None
    user_rating: Optional[int] = None  # 1-5
    timestamp: float = field(default_factory=time.time)

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.metrics: list[RequestMetrics] = []
    
    def record(self, metrics: RequestMetrics):
        self.metrics.append(metrics)
    
    def get_summary(self, time_window_hours: int = 24) -> dict:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        cutoff = time.time() - time_window_hours * 3600
        recent = [m for m in self.metrics if m.timestamp > cutoff]
        
        if not recent:
            return {}
        
        return {
            "total_requests": len(recent),
            "success_rate": sum(1 for m in recent if m.success) / len(recent),
            "avg_latency_ms": sum(m.latency_ms for m in recent) / len(recent),
            "p95_latency_ms": sorted([m.latency_ms for m in recent])[int(len(recent) * 0.95)],
            "avg_tokens": sum(m.prompt_tokens + m.completion_tokens for m in recent) / len(recent),
            "avg_rating": sum(m.user_rating for m in recent if m.user_rating) / 
                         sum(1 for m in recent if m.user_rating) if any(m.user_rating for m in recent) else None
        }
```

## è¯„ä¼°å·¥å…·æ¨è

### LangSmith

```python
from langsmith import Client
from langsmith.evaluation import evaluate

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = Client()

# åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset("my-eval-dataset")

# æ·»åŠ æ ·æœ¬
client.create_examples(
    inputs=[{"question": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"}],
    outputs=[{"answer": "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ..."}],
    dataset_id=dataset.id
)

# å®šä¹‰è¯„ä¼°å‡½æ•°
def my_evaluator(run, example):
    prediction = run.outputs["answer"]
    reference = example.outputs["answer"]
    # è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
    score = calculate_similarity(prediction, reference)
    return {"score": score}

# è¿è¡Œè¯„ä¼°
results = evaluate(
    lambda inputs: my_llm_app(inputs["question"]),
    data=dataset.name,
    evaluators=[my_evaluator]
)
```

### DeepEval

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric
from deepeval.test_case import LLMTestCase

# åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
test_case = LLMTestCase(
    input="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    actual_output="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...",
    retrieval_context=["æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ çš„æŠ€æœ¯..."]
)

# å®šä¹‰æŒ‡æ ‡
relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
faithfulness_metric = FaithfulnessMetric(threshold=0.7)

# è¿è¡Œè¯„ä¼°
evaluate([test_case], [relevancy_metric, faithfulness_metric])
```

### RAGAS

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision
from datasets import Dataset

# å‡†å¤‡æ•°æ®
data = {
    "question": ["ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"],
    "answer": ["RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯..."],
    "contexts": [["RAG å…¨ç§° Retrieval-Augmented Generation..."]],
    "ground_truth": ["RAG æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„ AI æŠ€æœ¯"]
}

dataset = Dataset.from_dict(data)

# è¯„ä¼°
results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision]
)

print(results)
```

## æœ€å°å¯è¡Œè¯„ä¼°ä½“ç³»ï¼ˆMVPï¼‰

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è¯„ä¼°

```python
# 1. 10-20 æ¡é»„é‡‘æ ·æœ¬
golden_set = load_golden_set("golden_samples.jsonl")

# 2. 3 ä¸ªç¡¬æŒ‡æ ‡
def basic_eval(model_output: str, expected: str) -> dict:
    return {
        "format_correct": check_format(model_output),
        "latency_ms": measure_latency(),
        "cost_usd": calculate_cost()
    }

# 3. äººå·¥ spot-check
def spot_check(samples: list, n: int = 20):
    """éšæœºæŠ½å– n æ¡è¿›è¡Œäººå·¥æ£€æŸ¥"""
    import random
    return random.sample(samples, min(n, len(samples)))
```

### ç¬¬äºŒé˜¶æ®µï¼šè‡ªåŠ¨åŒ–è¯„ä¼°

```python
# æ·»åŠ  LLM-as-Judge
# æ·»åŠ  CI/CD é›†æˆ
# æ·»åŠ å›å½’æµ‹è¯•
```

### ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨çº¿è¯„ä¼°

```python
# æ·»åŠ  A/B æµ‹è¯•
# æ·»åŠ ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
# æ·»åŠ å¼‚å¸¸æ£€æµ‹
```

## å»¶ä¼¸é˜…è¯»

- [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/)
- [DeepEval æ–‡æ¡£](https://github.com/confident-ai/deepeval)
- [RAGAS æ–‡æ¡£](https://docs.ragas.io/)
- [OpenAI Evals](https://github.com/openai/evals)
