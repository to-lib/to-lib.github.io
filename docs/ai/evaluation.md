---
sidebar_position: 10
title: 📏 Evaluation（评估与测试）
---

# Evaluation（评估与测试）

LLM 应用的难点之一是“看起来能用”，但线上表现不稳定。一个可持续迭代的 AI 系统，需要把评估当成工程能力：可复现、可对比、可回归。

## 评估目标

- **正确性**：答案是否符合事实/业务规则。
- **相关性**：回答是否切中用户问题。
- **完整性**：关键点是否遗漏。
- **安全性**：是否泄露敏感信息、是否遵守策略。
- **格式/结构**：JSON/表格/字段是否符合约束。
- **成本与延迟**：是否满足 SLA。

## 离线评估（Offline Eval）

### 1. 构建黄金数据集（Golden Set）

- 从真实用户问题中抽样。
- 覆盖常见问题与长尾。
- 每条样本包含：输入、期望输出/评分标准、上下文（如 RAG 检索结果）。

### 2. 自动化评分方式

- **规则/断言**：字段存在、JSON 可解析、SQL 可执行（在沙箱）。
- **指标**：
  - 分类/抽取：Accuracy/F1
  - 检索：Recall@K、MRR
  - RAG 生成：faithfulness / answer relevancy（通常需要 LLM-as-judge 或人工）
- **LLM-as-judge**：让另一个模型按 rubric 打分（要控制偏置，尽量用多模型交叉验证）。

## 在线评估（Online Eval）

- **A/B 测试**：对比不同 prompt/模型/检索策略。
- **业务指标**：解决率、转人工率、用户满意度、复问率。
- **回归监控**：一旦指标下滑触发报警。

## RAG 场景特别关注

- **检索质量**：是否召回了正确文档；Top-K 是否足够。
- **引用一致性**：回答是否能被检索片段支持（减少幻觉）。
- **权限过滤**：是否只从用户有权限的文档召回。

## 最小可行评估体系（MVP）

- **10-20 条黄金样本**：先跑通回归。
- **3 个硬指标**：格式正确率、平均延迟、平均成本。
- **人工 spot-check**：每次发布抽查 20 条线上对话。

## 延伸阅读

- https://docs.smith.langchain.com/
- https://github.com/confident-ai/deepeval
