---
sidebar_position: 27
title: ğŸ’° æˆæœ¬ä¼˜åŒ–
---

# æˆæœ¬ä¼˜åŒ–

AI åº”ç”¨çš„æˆæœ¬ä¸»è¦æ¥è‡ª API è°ƒç”¨è´¹ç”¨ã€‚æœ¬æ–‡ä»‹ç»å„ç§é™ä½æˆæœ¬çš„ç­–ç•¥ã€‚

## æˆæœ¬æ„æˆ

```
æ€»æˆæœ¬ = è¾“å…¥ Token Ã— è¾“å…¥ä»·æ ¼ + è¾“å‡º Token Ã— è¾“å‡ºä»·æ ¼
```

### ä¸»æµæ¨¡å‹ä»·æ ¼ï¼ˆ2025ï¼‰

| æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ | ç‰¹ç‚¹ |
|------|---------|---------|------|
| GPT-4o | $2.50/1M | $10.00/1M | æ€§èƒ½æœ€å¼º |
| GPT-4o-mini | $0.15/1M | $0.60/1M | æ€§ä»·æ¯”é«˜ |
| Claude 3.5 Sonnet | $3.00/1M | $15.00/1M | é•¿ä¸Šä¸‹æ–‡ |
| Claude 3.5 Haiku | $0.80/1M | $4.00/1M | å¿«é€Ÿå“åº” |
| Gemini 1.5 Pro | $1.25/1M | $5.00/1M | è¶…é•¿ä¸Šä¸‹æ–‡ |
| Gemini 1.5 Flash | $0.075/1M | $0.30/1M | æè‡´æ€§ä»·æ¯” |

## ç­–ç•¥ 1: æ¨¡å‹é€‰æ‹©

### æŒ‰ä»»åŠ¡é€‰æ‹©æ¨¡å‹

```python
from openai import OpenAI

client = OpenAI()

def select_model(task_type: str) -> str:
    """æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ¨¡å‹"""
    model_map = {
        "simple_qa": "gpt-4o-mini",      # ç®€å•é—®ç­”
        "classification": "gpt-4o-mini",  # åˆ†ç±»ä»»åŠ¡
        "summarization": "gpt-4o-mini",   # æ‘˜è¦
        "code_generation": "gpt-4o",      # ä»£ç ç”Ÿæˆ
        "complex_reasoning": "o1-mini",   # å¤æ‚æ¨ç†
        "creative_writing": "gpt-4o",     # åˆ›æ„å†™ä½œ
    }
    return model_map.get(task_type, "gpt-4o-mini")


def smart_query(query: str, task_type: str) -> str:
    model = select_model(task_type)
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": query}]
    )
    return response.choices[0].message.content
```

### çº§è”è°ƒç”¨

```python
def cascade_query(query: str) -> str:
    """å…ˆç”¨å°æ¨¡å‹ï¼Œä¸ç¡®å®šæ—¶ç”¨å¤§æ¨¡å‹"""
    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šå°æ¨¡å‹
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "å›ç­”é—®é¢˜ã€‚å¦‚æœä¸ç¡®å®šï¼Œå›å¤ 'UNCERTAIN'ã€‚"
            },
            {"role": "user", "content": query}
        ]
    )
    
    answer = response.choices[0].message.content
    
    # å¦‚æœä¸ç¡®å®šï¼Œå‡çº§åˆ°å¤§æ¨¡å‹
    if "UNCERTAIN" in answer:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": query}]
        )
        answer = response.choices[0].message.content
    
    return answer
```

## ç­–ç•¥ 2: Token ä¼˜åŒ–

### ç²¾ç®€ Prompt

```python
# âŒ å†—é•¿çš„ Prompt
bad_prompt = """
ä½ æ˜¯ä¸€ä¸ªéå¸¸ä¸“ä¸šçš„ã€ç»éªŒä¸°å¯Œçš„ã€çŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚
ä½ éœ€è¦ä»”ç»†åœ°ã€è®¤çœŸåœ°ã€å…¨é¢åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¯·ç¡®ä¿ä½ çš„å›ç­”æ˜¯å‡†ç¡®çš„ã€æœ‰å¸®åŠ©çš„ã€è¯¦ç»†çš„ã€‚
ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{question}
"""

# âœ… ç²¾ç®€çš„ Prompt
good_prompt = """
ç®€æ´å›ç­”ï¼š{question}
"""
```

### å‹ç¼©ä¸Šä¸‹æ–‡

```python
def compress_context(context: str, max_tokens: int = 2000) -> str:
    """å‹ç¼©ä¸Šä¸‹æ–‡"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": f"å°†ä»¥ä¸‹å†…å®¹å‹ç¼©åˆ° {max_tokens} tokens ä»¥å†…ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚"
            },
            {"role": "user", "content": context}
        ],
        max_tokens=max_tokens
    )
    return response.choices[0].message.content
```

### é™åˆ¶è¾“å‡ºé•¿åº¦

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": query}],
    max_tokens=500  # é™åˆ¶è¾“å‡ºé•¿åº¦
)
```

## ç­–ç•¥ 3: ç¼“å­˜

### è¯­ä¹‰ç¼“å­˜

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import hashlib

class SemanticCache:
    """è¯­ä¹‰ç¼“å­˜"""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vectorstore = Chroma(embedding_function=self.embeddings)
        self.cache = {}  # query_id -> response
        self.threshold = similarity_threshold
    
    def _get_similar(self, query: str) -> str | None:
        """æŸ¥æ‰¾ç›¸ä¼¼æŸ¥è¯¢"""
        results = self.vectorstore.similarity_search_with_score(query, k=1)
        
        if results and results[0][1] >= self.threshold:
            query_id = results[0][0].metadata.get("query_id")
            return self.cache.get(query_id)
        
        return None
    
    def get(self, query: str) -> str | None:
        return self._get_similar(query)
    
    def set(self, query: str, response: str):
        query_id = hashlib.md5(query.encode()).hexdigest()
        self.cache[query_id] = response
        self.vectorstore.add_texts(
            texts=[query],
            metadatas=[{"query_id": query_id}]
        )
    
    def query(self, query: str) -> str:
        # æ£€æŸ¥ç¼“å­˜
        cached = self.get(query)
        if cached:
            print("Cache hit!")
            return cached
        
        # è°ƒç”¨ API
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": query}]
        )
        
        result = response.choices[0].message.content
        self.set(query, result)
        return result
```

### Prompt Caching

å‚è€ƒ [Prompt Caching](./prompt-caching) æ–‡æ¡£ã€‚

## ç­–ç•¥ 4: æ‰¹å¤„ç†

### æ‰¹é‡è¯·æ±‚

```python
import asyncio
from openai import AsyncOpenAI

async_client = AsyncOpenAI()

async def batch_process(queries: list[str], batch_size: int = 10) -> list[str]:
    """æ‰¹é‡å¤„ç†è¯·æ±‚"""
    results = []
    
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i + batch_size]
        
        tasks = [
            async_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": q}]
            )
            for q in batch
        ]
        
        responses = await asyncio.gather(*tasks)
        results.extend([r.choices[0].message.content for r in responses])
    
    return results
```

### OpenAI Batch API

```python
import json

def create_batch_file(requests: list[dict], filename: str):
    """åˆ›å»ºæ‰¹å¤„ç†æ–‡ä»¶"""
    with open(filename, 'w') as f:
        for i, req in enumerate(requests):
            line = {
                "custom_id": f"request-{i}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": "gpt-4o-mini",
                    "messages": req["messages"]
                }
            }
            f.write(json.dumps(line) + "\n")

# ä¸Šä¼ æ–‡ä»¶
batch_file = client.files.create(
    file=open("batch_requests.jsonl", "rb"),
    purpose="batch"
)

# åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
batch = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h"  # 24 å°æ—¶å†…å®Œæˆï¼Œä»·æ ¼å‡åŠ
)

# æ£€æŸ¥çŠ¶æ€
status = client.batches.retrieve(batch.id)
print(f"Status: {status.status}")
```


## ç­–ç•¥ 5: æœ¬åœ°æ¨¡å‹

å¯¹äºé«˜é¢‘ã€ä½å¤æ‚åº¦ä»»åŠ¡ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹ã€‚

```python
import ollama

def local_or_cloud(query: str, complexity: str = "low") -> str:
    """æ ¹æ®å¤æ‚åº¦é€‰æ‹©æœ¬åœ°æˆ–äº‘ç«¯æ¨¡å‹"""
    if complexity == "low":
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        response = ollama.chat(
            model="qwen2.5:7b",
            messages=[{"role": "user", "content": query}]
        )
        return response["message"]["content"]
    else:
        # ä½¿ç”¨äº‘ç«¯æ¨¡å‹
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": query}]
        )
        return response.choices[0].message.content
```

## ç­–ç•¥ 6: æˆæœ¬ç›‘æ§

### Token è®¡æ•°

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4o") -> int:
    """è®¡ç®— token æ•°é‡"""
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

def estimate_cost(
    input_text: str,
    output_tokens: int,
    model: str = "gpt-4o"
) -> float:
    """ä¼°ç®—æˆæœ¬"""
    prices = {
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
    }
    
    input_tokens = count_tokens(input_text, model)
    price = prices.get(model, prices["gpt-4o"])
    
    cost = (input_tokens * price["input"] + output_tokens * price["output"]) / 1_000_000
    return cost
```

### ä½¿ç”¨é‡è¿½è¸ª

```python
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class UsageTracker:
    """ä½¿ç”¨é‡è¿½è¸ª"""
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost: float = 0.0
    requests: int = 0
    daily_stats: dict = field(default_factory=dict)
    
    def record(self, input_tokens: int, output_tokens: int, model: str):
        self.requests += 1
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        
        # è®¡ç®—æˆæœ¬
        prices = {
            "gpt-4o": {"input": 2.50, "output": 10.00},
            "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        }
        price = prices.get(model, prices["gpt-4o"])
        cost = (input_tokens * price["input"] + output_tokens * price["output"]) / 1_000_000
        self.total_cost += cost
        
        # æŒ‰æ—¥ç»Ÿè®¡
        today = datetime.now().strftime("%Y-%m-%d")
        if today not in self.daily_stats:
            self.daily_stats[today] = {"tokens": 0, "cost": 0}
        self.daily_stats[today]["tokens"] += input_tokens + output_tokens
        self.daily_stats[today]["cost"] += cost
    
    def report(self) -> str:
        return f"""
ä½¿ç”¨æŠ¥å‘Šï¼š
- æ€»è¯·æ±‚æ•°ï¼š{self.requests}
- æ€»è¾“å…¥ tokensï¼š{self.total_input_tokens:,}
- æ€»è¾“å‡º tokensï¼š{self.total_output_tokens:,}
- æ€»æˆæœ¬ï¼š${self.total_cost:.4f}
"""

tracker = UsageTracker()
```

## æˆæœ¬ä¼˜åŒ–æ¸…å•

| ç­–ç•¥ | èŠ‚çœæ¯”ä¾‹ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| ä½¿ç”¨å°æ¨¡å‹ | 90%+ | ç®€å•ä»»åŠ¡ |
| Prompt Caching | 50-90% | é‡å¤å‰ç¼€ |
| æ‰¹å¤„ç† API | 50% | éå®æ—¶ä»»åŠ¡ |
| è¯­ä¹‰ç¼“å­˜ | å˜åŒ–å¤§ | é‡å¤æŸ¥è¯¢ |
| æœ¬åœ°æ¨¡å‹ | 100% | é«˜é¢‘ä½å¤æ‚åº¦ |
| å‹ç¼©ä¸Šä¸‹æ–‡ | 30-50% | é•¿æ–‡æ¡£ |
| é™åˆ¶è¾“å‡º | 20-40% | ç®€æ´å›ç­” |

## æœ€ä½³å®è·µ

1. **ç›‘æ§å…ˆè¡Œ**ï¼šå…ˆäº†è§£æˆæœ¬åˆ†å¸ƒå†ä¼˜åŒ–
2. **åˆ†çº§å¤„ç†**ï¼šä¸åŒä»»åŠ¡ç”¨ä¸åŒæ¨¡å‹
3. **ç¼“å­˜ä¼˜å…ˆ**ï¼šç›¸ä¼¼æŸ¥è¯¢å¤ç”¨ç»“æœ
4. **æ‰¹é‡å¤„ç†**ï¼šéå®æ—¶ä»»åŠ¡ç”¨ Batch API
5. **è®¾ç½®é¢„ç®—**ï¼šé…ç½®ç”¨é‡å‘Šè­¦

## å»¶ä¼¸é˜…è¯»

- [OpenAI Pricing](https://openai.com/pricing)
- [OpenAI Batch API](https://platform.openai.com/docs/guides/batch)
- [tiktoken](https://github.com/openai/tiktoken)