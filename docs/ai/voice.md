---
sidebar_position: 30
title: ğŸ™ï¸ è¯­éŸ³äº¤äº’
---

# è¯­éŸ³äº¤äº’

è¯­éŸ³äº¤äº’è®© AI åº”ç”¨èƒ½å¤Ÿå¬å’Œè¯´ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰ã€è¯­éŸ³åˆæˆï¼ˆTTSï¼‰å’Œå®æ—¶è¯­éŸ³å¯¹è¯ã€‚

## æŠ€æœ¯æ ˆ

```
è¯­éŸ³è¾“å…¥ â”€â”€> STT â”€â”€> æ–‡æœ¬ â”€â”€> LLM â”€â”€> æ–‡æœ¬ â”€â”€> TTS â”€â”€> è¯­éŸ³è¾“å‡º
                              â”‚
                    Realtime APIï¼ˆç«¯åˆ°ç«¯ï¼‰
```

## OpenAI è¯­éŸ³ API

### è¯­éŸ³åˆæˆï¼ˆTTSï¼‰

```python
from openai import OpenAI
from pathlib import Path

client = OpenAI()

def text_to_speech(text: str, output_file: str = "output.mp3"):
    """æ–‡æœ¬è½¬è¯­éŸ³"""
    response = client.audio.speech.create(
        model="tts-1",  # tts-1 æˆ– tts-1-hd
        voice="alloy",  # alloy, echo, fable, onyx, nova, shimmer
        input=text
    )
    
    response.stream_to_file(Path(output_file))
    return output_file

# ä½¿ç”¨
text_to_speech("ä½ å¥½ï¼Œæˆ‘æ˜¯ AI åŠ©æ‰‹ã€‚", "greeting.mp3")
```

### æµå¼ TTS

```python
def stream_tts(text: str):
    """æµå¼è¯­éŸ³åˆæˆ"""
    response = client.audio.speech.create(
        model="tts-1",
        voice="nova",
        input=text,
        response_format="pcm"  # æµå¼éœ€è¦ pcm æ ¼å¼
    )
    
    # æµå¼æ’­æ”¾
    for chunk in response.iter_bytes(chunk_size=1024):
        yield chunk

# é…åˆéŸ³é¢‘æ’­æ”¾åº“ä½¿ç”¨
import pyaudio

def play_stream(text: str):
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paInt16, channels=1, rate=24000, output=True)
    
    for chunk in stream_tts(text):
        stream.write(chunk)
    
    stream.stop_stream()
    stream.close()
    p.terminate()
```

### è¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰

```python
def speech_to_text(audio_file: str) -> str:
    """è¯­éŸ³è½¬æ–‡æœ¬"""
    with open(audio_file, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="text"
        )
    return transcript

# å¸¦æ—¶é—´æˆ³
def speech_to_text_with_timestamps(audio_file: str) -> dict:
    """è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰"""
    with open(audio_file, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="verbose_json",
            timestamp_granularities=["word", "segment"]
        )
    return transcript

# ç¿»è¯‘ï¼ˆä»»æ„è¯­è¨€è½¬è‹±è¯­ï¼‰
def translate_audio(audio_file: str) -> str:
    """è¯­éŸ³ç¿»è¯‘"""
    with open(audio_file, "rb") as f:
        translation = client.audio.translations.create(
            model="whisper-1",
            file=f
        )
    return translation.text
```

## OpenAI Realtime API

Realtime API æä¾›ç«¯åˆ°ç«¯çš„å®æ—¶è¯­éŸ³å¯¹è¯èƒ½åŠ›ã€‚

### WebSocket è¿æ¥

```python
import asyncio
import websockets
import json
import base64

async def realtime_conversation():
    """å®æ—¶è¯­éŸ³å¯¹è¯"""
    url = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview"
    headers = {
        "Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
        "OpenAI-Beta": "realtime=v1"
    }
    
    async with websockets.connect(url, extra_headers=headers) as ws:
        # é…ç½®ä¼šè¯
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å›ç­”ã€‚",
                "voice": "alloy",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "turn_detection": {
                    "type": "server_vad",  # æœåŠ¡ç«¯è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                    "threshold": 0.5
                }
            }
        }))
        
        # å¤„ç†æ¶ˆæ¯
        async for message in ws:
            event = json.loads(message)
            await handle_event(event)

async def handle_event(event: dict):
    """å¤„ç† Realtime API äº‹ä»¶"""
    event_type = event.get("type")
    
    if event_type == "response.audio.delta":
        # æ”¶åˆ°éŸ³é¢‘æ•°æ®
        audio_data = base64.b64decode(event["delta"])
        # æ’­æ”¾éŸ³é¢‘...
        
    elif event_type == "response.text.delta":
        # æ”¶åˆ°æ–‡æœ¬
        print(event["delta"], end="", flush=True)
        
    elif event_type == "response.done":
        print("\n--- å›å¤å®Œæˆ ---")
```

### å‘é€éŸ³é¢‘

```python
async def send_audio(ws, audio_data: bytes):
    """å‘é€éŸ³é¢‘æ•°æ®"""
    await ws.send(json.dumps({
        "type": "input_audio_buffer.append",
        "audio": base64.b64encode(audio_data).decode()
    }))

async def commit_audio(ws):
    """æäº¤éŸ³é¢‘ç¼“å†²åŒº"""
    await ws.send(json.dumps({
        "type": "input_audio_buffer.commit"
    }))
    
    # è¯·æ±‚å“åº”
    await ws.send(json.dumps({
        "type": "response.create"
    }))
```

### å®Œæ•´ç¤ºä¾‹

```python
import sounddevice as sd
import numpy as np

class RealtimeVoiceChat:
    """å®æ—¶è¯­éŸ³èŠå¤©"""
    
    def __init__(self):
        self.sample_rate = 24000
        self.channels = 1
        self.ws = None
    
    async def connect(self):
        url = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview"
        headers = {
            "Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
            "OpenAI-Beta": "realtime=v1"
        }
        self.ws = await websockets.connect(url, extra_headers=headers)
        
        # é…ç½®
        await self.ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "voice": "nova",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16"
            }
        }))
    
    async def start(self):
        await self.connect()
        
        # å¯åŠ¨å½•éŸ³å’Œæ’­æ”¾
        asyncio.create_task(self.record_audio())
        asyncio.create_task(self.receive_messages())
    
    async def record_audio(self):
        """å½•åˆ¶éŸ³é¢‘å¹¶å‘é€"""
        def callback(indata, frames, time, status):
            audio_bytes = indata.tobytes()
            asyncio.create_task(self.send_audio(audio_bytes))
        
        with sd.InputStream(
            samplerate=self.sample_rate,
            channels=self.channels,
            dtype=np.int16,
            callback=callback
        ):
            await asyncio.sleep(float('inf'))
    
    async def send_audio(self, audio_data: bytes):
        if self.ws:
            await self.ws.send(json.dumps({
                "type": "input_audio_buffer.append",
                "audio": base64.b64encode(audio_data).decode()
            }))
    
    async def receive_messages(self):
        """æ¥æ”¶å¹¶å¤„ç†æ¶ˆæ¯"""
        audio_buffer = []
        
        async for message in self.ws:
            event = json.loads(message)
            
            if event["type"] == "response.audio.delta":
                audio_data = base64.b64decode(event["delta"])
                audio_buffer.append(audio_data)
                
            elif event["type"] == "response.audio.done":
                # æ’­æ”¾å®Œæ•´éŸ³é¢‘
                full_audio = b"".join(audio_buffer)
                self.play_audio(full_audio)
                audio_buffer = []
    
    def play_audio(self, audio_data: bytes):
        """æ’­æ”¾éŸ³é¢‘"""
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        sd.play(audio_array, self.sample_rate)
        sd.wait()
```


## å…¶ä»–è¯­éŸ³æœåŠ¡

### Azure Speech

```python
import azure.cognitiveservices.speech as speechsdk

# é…ç½®
speech_config = speechsdk.SpeechConfig(
    subscription="your_key",
    region="eastus"
)
speech_config.speech_synthesis_voice_name = "zh-CN-XiaoxiaoNeural"

# TTS
synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)
result = synthesizer.speak_text_async("ä½ å¥½").get()

# STT
audio_config = speechsdk.AudioConfig(filename="audio.wav")
recognizer = speechsdk.SpeechRecognizer(
    speech_config=speech_config,
    audio_config=audio_config
)
result = recognizer.recognize_once()
print(result.text)
```

### ElevenLabs

```python
from elevenlabs import generate, play, set_api_key

set_api_key("your_api_key")

# ç”Ÿæˆè¯­éŸ³
audio = generate(
    text="Hello, this is a test.",
    voice="Rachel",
    model="eleven_multilingual_v2"
)

play(audio)
```

## è¯­éŸ³åŠ©æ‰‹æ¶æ„

```python
class VoiceAssistant:
    """è¯­éŸ³åŠ©æ‰‹"""
    
    def __init__(self):
        self.client = OpenAI()
        self.conversation_history = []
    
    def listen(self, audio_file: str) -> str:
        """å¬å–ç”¨æˆ·è¾“å…¥"""
        with open(audio_file, "rb") as f:
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=f
            )
        return transcript.text
    
    def think(self, user_input: str) -> str:
        """å¤„ç†å¹¶ç”Ÿæˆå›å¤"""
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¯­éŸ³åŠ©æ‰‹ï¼Œå›å¤è¦ç®€æ´è‡ªç„¶ã€‚"},
                *self.conversation_history
            ]
        )
        
        assistant_message = response.choices[0].message.content
        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_message
        })
        
        return assistant_message
    
    def speak(self, text: str, output_file: str = "response.mp3"):
        """è¯­éŸ³è¾“å‡º"""
        response = self.client.audio.speech.create(
            model="tts-1",
            voice="nova",
            input=text
        )
        response.stream_to_file(output_file)
        return output_file
    
    def process(self, audio_file: str) -> str:
        """å®Œæ•´å¤„ç†æµç¨‹"""
        # 1. è¯­éŸ³è½¬æ–‡æœ¬
        user_input = self.listen(audio_file)
        print(f"ç”¨æˆ·ï¼š{user_input}")
        
        # 2. ç”Ÿæˆå›å¤
        response = self.think(user_input)
        print(f"åŠ©æ‰‹ï¼š{response}")
        
        # 3. æ–‡æœ¬è½¬è¯­éŸ³
        output_file = self.speak(response)
        
        return output_file
```

## å®æ—¶è¯­éŸ³ç¿»è¯‘

```python
class RealtimeTranslator:
    """å®æ—¶è¯­éŸ³ç¿»è¯‘"""
    
    def __init__(self, source_lang: str = "zh", target_lang: str = "en"):
        self.client = OpenAI()
        self.source_lang = source_lang
        self.target_lang = target_lang
    
    def translate(self, audio_file: str) -> tuple[str, str]:
        """ç¿»è¯‘éŸ³é¢‘"""
        # 1. è¯­éŸ³è¯†åˆ«
        with open(audio_file, "rb") as f:
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language=self.source_lang
            )
        
        original_text = transcript.text
        
        # 2. æ–‡æœ¬ç¿»è¯‘
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{self.target_lang}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœã€‚"
                },
                {"role": "user", "content": original_text}
            ]
        )
        
        translated_text = response.choices[0].message.content
        
        # 3. è¯­éŸ³åˆæˆ
        audio_response = self.client.audio.speech.create(
            model="tts-1",
            voice="nova",
            input=translated_text
        )
        
        output_file = "translated.mp3"
        audio_response.stream_to_file(output_file)
        
        return original_text, translated_text
```

## ä»·æ ¼å‚è€ƒ

| æœåŠ¡ | ä»·æ ¼ |
|------|------|
| Whisper (STT) | $0.006/åˆ†é’Ÿ |
| TTS-1 | $15/1M å­—ç¬¦ |
| TTS-1-HD | $30/1M å­—ç¬¦ |
| Realtime API | $0.06/åˆ†é’Ÿï¼ˆéŸ³é¢‘ï¼‰+ Token è´¹ç”¨ |

## æœ€ä½³å®è·µ

1. **é™å™ªå¤„ç†**ï¼šè¾“å…¥éŸ³é¢‘å…ˆåšé™å™ª
2. **åˆ†æ®µå¤„ç†**ï¼šé•¿éŸ³é¢‘åˆ†æ®µè¯†åˆ«
3. **æµå¼è¾“å‡º**ï¼šTTS ä½¿ç”¨æµå¼æå‡ä½“éªŒ
4. **é”™è¯¯å¤„ç†**ï¼šå¤„ç†ç½‘ç»œä¸­æ–­å’Œè¯†åˆ«å¤±è´¥
5. **éšç§ä¿æŠ¤**ï¼šæ•æ„ŸéŸ³é¢‘æœ¬åœ°å¤„ç†

## å»¶ä¼¸é˜…è¯»

- [OpenAI Audio API](https://platform.openai.com/docs/guides/speech-to-text)
- [OpenAI Realtime API](https://platform.openai.com/docs/guides/realtime)
- [Azure Speech](https://azure.microsoft.com/en-us/products/ai-services/speech-services)