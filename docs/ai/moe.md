---
sidebar_position: 36
title: ğŸ§© Mixture of Experts
---

# Mixture of Experts (MoE)

Mixture of Experts æ˜¯ä¸€ç§ç¨€ç–æ¿€æ´»çš„æ¨¡å‹æ¶æ„ï¼Œé€šè¿‡è·¯ç”±æœºåˆ¶é€‰æ‹©æ€§æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ç½‘ç»œï¼Œåœ¨ä¿æŒå¤§æ¨¡å‹èƒ½åŠ›çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚

## MoE åŸç†

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Router    â”‚
                    â”‚  (é—¨æ§ç½‘ç»œ)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
           â–¼       â–¼       â–¼       â–¼       â–¼
        â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
        â”‚ E1  â”‚ â”‚ E2  â”‚ â”‚ E3  â”‚ â”‚ E4  â”‚ â”‚ E5  â”‚
        â”‚ä¸“å®¶1â”‚ â”‚ä¸“å®¶2â”‚ â”‚ä¸“å®¶3â”‚ â”‚ä¸“å®¶4â”‚ â”‚ä¸“å®¶5â”‚
        â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜
           â”‚       â”‚       â”‚       â”‚       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  åŠ æƒæ±‚å’Œ   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¯æ¬¡åªæ¿€æ´» Top-K ä¸ªä¸“å®¶ï¼ˆé€šå¸¸ K=2ï¼‰
```

## MoE vs Dense æ¨¡å‹

| ç‰¹æ€§ | Dense æ¨¡å‹ | MoE æ¨¡å‹ |
|------|-----------|---------|
| å‚æ•°æ¿€æ´» | 100% | 10-20% |
| æ€»å‚æ•°é‡ | è¾ƒå° | è¾ƒå¤§ |
| æ¨ç†æˆæœ¬ | é«˜ | ä½ |
| è®­ç»ƒéš¾åº¦ | ç®€å• | å¤æ‚ |
| ä»£è¡¨æ¨¡å‹ | LLaMA, GPT-4 | Mixtral, DeepSeek |

## ä¸»æµ MoE æ¨¡å‹

| æ¨¡å‹ | æ€»å‚æ•° | æ¿€æ´»å‚æ•° | ä¸“å®¶æ•° |
|------|--------|---------|--------|
| Mixtral 8x7B | 47B | 13B | 8 |
| Mixtral 8x22B | 141B | 39B | 8 |
| DeepSeek-V2 | 236B | 21B | 160 |
| Qwen2-MoE | 57B | 14B | 64 |
| DBRX | 132B | 36B | 16 |

## ä½¿ç”¨ MoE æ¨¡å‹

### Ollama

```bash
# ä¸‹è½½ Mixtral
ollama pull mixtral

# è¿è¡Œ
ollama run mixtral "è§£é‡Šä»€ä¹ˆæ˜¯ MoE æ¶æ„"
```


### vLLM

```python
from vllm import LLM, SamplingParams

# åŠ è½½ MoE æ¨¡å‹
llm = LLM(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    tensor_parallel_size=2,  # å¤šå¡å¹¶è¡Œ
    dtype="float16"
)

sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=512
)

outputs = llm.generate(["è§£é‡Š MoE æ¶æ„çš„ä¼˜åŠ¿"], sampling_params)
print(outputs[0].outputs[0].text)
```

### Transformers

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

messages = [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ MoEï¼Ÿ"}]
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

outputs = model.generate(inputs, max_new_tokens=256)
print(tokenizer.decode(outputs[0]))
```

## MoE æ¶æ„å®ç°

### ç®€åŒ–ç‰ˆ MoE å±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Expert(nn.Module):
    """å•ä¸ªä¸“å®¶ç½‘ç»œ"""
    
    def __init__(self, input_dim: int, hidden_dim: int):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, input_dim)
        self.activation = nn.SiLU()
    
    def forward(self, x):
        return self.fc2(self.activation(self.fc1(x)))

class Router(nn.Module):
    """è·¯ç”±ç½‘ç»œ"""
    
    def __init__(self, input_dim: int, num_experts: int):
        super().__init__()
        self.gate = nn.Linear(input_dim, num_experts)
    
    def forward(self, x):
        # è¿”å›æ¯ä¸ªä¸“å®¶çš„æƒé‡
        return F.softmax(self.gate(x), dim=-1)

class MoELayer(nn.Module):
    """MoE å±‚"""
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        num_experts: int = 8,
        top_k: int = 2
    ):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        # åˆ›å»ºä¸“å®¶
        self.experts = nn.ModuleList([
            Expert(input_dim, hidden_dim)
            for _ in range(num_experts)
        ])
        
        # è·¯ç”±å™¨
        self.router = Router(input_dim, num_experts)
    
    def forward(self, x):
        batch_size, seq_len, dim = x.shape
        x_flat = x.view(-1, dim)
        
        # è®¡ç®—è·¯ç”±æƒé‡
        router_logits = self.router(x_flat)
        
        # é€‰æ‹© Top-K ä¸“å®¶
        top_k_weights, top_k_indices = torch.topk(router_logits, self.top_k, dim=-1)
        top_k_weights = F.softmax(top_k_weights, dim=-1)
        
        # è®¡ç®—è¾“å‡º
        output = torch.zeros_like(x_flat)
        
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]
            expert_weight = top_k_weights[:, i].unsqueeze(-1)
            
            for j in range(self.num_experts):
                mask = expert_idx == j
                if mask.any():
                    expert_input = x_flat[mask]
                    expert_output = self.experts[j](expert_input)
                    output[mask] += expert_weight[mask] * expert_output
        
        return output.view(batch_size, seq_len, dim)
```

### è´Ÿè½½å‡è¡¡æŸå¤±

```python
def load_balancing_loss(router_logits: torch.Tensor, top_k: int = 2) -> torch.Tensor:
    """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé˜²æ­¢ä¸“å®¶ä½¿ç”¨ä¸å‡"""
    num_experts = router_logits.shape[-1]
    
    # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
    top_k_indices = torch.topk(router_logits, top_k, dim=-1).indices
    expert_counts = torch.zeros(num_experts, device=router_logits.device)
    
    for i in range(num_experts):
        expert_counts[i] = (top_k_indices == i).float().sum()
    
    # å½’ä¸€åŒ–
    expert_probs = expert_counts / expert_counts.sum()
    
    # è®¡ç®—ä¸å‡åŒ€åˆ†å¸ƒçš„å·®å¼‚
    uniform_prob = 1.0 / num_experts
    loss = ((expert_probs - uniform_prob) ** 2).sum()
    
    return loss
```

## MoE ä¼˜åŒ–æŠ€å·§

### ä¸“å®¶å¹¶è¡Œ

```python
# ä½¿ç”¨ DeepSpeed è¿›è¡Œä¸“å®¶å¹¶è¡Œ
import deepspeed

config = {
    "train_batch_size": 32,
    "moe": {
        "enabled": True,
        "ep_size": 4,  # ä¸“å®¶å¹¶è¡Œåº¦
        "moe_param_group": True
    }
}

model, optimizer, _, _ = deepspeed.initialize(
    model=model,
    config=config
)
```

### é‡åŒ– MoE æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    quantization_config=bnb_config,
    device_map="auto"
)
```


## MoE éƒ¨ç½²è€ƒè™‘

### æ˜¾å­˜éœ€æ±‚

```
Mixtral 8x7B (FP16):
- æ€»å‚æ•°ï¼š47B Ã— 2 bytes = 94 GB
- ä½†åªéœ€åŠ è½½æ¿€æ´»çš„ä¸“å®¶
- å®é™…æ˜¾å­˜ï¼š~26 GBï¼ˆå•å¡å¯è¿è¡Œé‡åŒ–ç‰ˆæœ¬ï¼‰

DeepSeek-V2 (FP16):
- æ€»å‚æ•°ï¼š236B
- æ¿€æ´»å‚æ•°ï¼š21B
- æ¨ç†æ˜¾å­˜ï¼š~50 GB
```

### æ¨ç†ä¼˜åŒ–

```python
# ä½¿ç”¨ vLLM çš„ MoE ä¼˜åŒ–
from vllm import LLM

llm = LLM(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    tensor_parallel_size=2,
    # MoE ç‰¹å®šä¼˜åŒ–
    enable_prefix_caching=True,  # å‰ç¼€ç¼“å­˜
    max_num_seqs=256,            # æ‰¹å¤„ç†å¤§å°
)
```

## MoE vs Dense é€‰æ‹©

| åœºæ™¯ | æ¨è |
|------|------|
| èµ„æºæœ‰é™ | MoEï¼ˆæ¿€æ´»å‚æ•°å°‘ï¼‰ |
| è¿½æ±‚æè‡´æ€§èƒ½ | Denseï¼ˆæ›´ç¨³å®šï¼‰ |
| å¤šä»»åŠ¡åœºæ™¯ | MoEï¼ˆä¸“å®¶ä¸“ä¸šåŒ–ï¼‰ |
| ç®€å•éƒ¨ç½² | Denseï¼ˆæ¶æ„ç®€å•ï¼‰ |

## æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„ Top-K**ï¼šé€šå¸¸ K=2 æ˜¯å¥½çš„å¹³è¡¡ç‚¹
2. **æ³¨æ„è´Ÿè½½å‡è¡¡**ï¼šé˜²æ­¢éƒ¨åˆ†ä¸“å®¶è¿‡è½½
3. **é‡åŒ–éƒ¨ç½²**ï¼šMoE æ¨¡å‹ç‰¹åˆ«é€‚åˆé‡åŒ–
4. **ä¸“å®¶å¹¶è¡Œ**ï¼šå¤šå¡éƒ¨ç½²æ—¶ä½¿ç”¨ä¸“å®¶å¹¶è¡Œ
5. **ç›‘æ§ä¸“å®¶ä½¿ç”¨**ï¼šåˆ†æå“ªäº›ä¸“å®¶è¢«é¢‘ç¹ä½¿ç”¨

## å»¶ä¼¸é˜…è¯»

- [Mixtral Paper](https://arxiv.org/abs/2401.04088)
- [DeepSeek-V2](https://arxiv.org/abs/2405.04434)
- [Switch Transformer](https://arxiv.org/abs/2101.03961)
- [GShard](https://arxiv.org/abs/2006.16668)