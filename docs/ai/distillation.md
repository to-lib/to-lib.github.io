---
sidebar_position: 32
title: ğŸ§¬ æ¨¡å‹è’¸é¦
---

# æ¨¡å‹è’¸é¦

æ¨¡å‹è’¸é¦ï¼ˆKnowledge Distillationï¼‰æ˜¯å°†å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰çš„çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰çš„æŠ€æœ¯ï¼Œè®©å°æ¨¡å‹è·å¾—æ¥è¿‘å¤§æ¨¡å‹çš„èƒ½åŠ›ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦è’¸é¦ï¼Ÿ

| å¯¹æ¯” | å¤§æ¨¡å‹ | è’¸é¦åå°æ¨¡å‹ |
|------|--------|-------------|
| å‚æ•°é‡ | 70B+ | 7B æˆ–æ›´å° |
| æ¨ç†æˆæœ¬ | é«˜ | ä½ |
| éƒ¨ç½²éš¾åº¦ | éœ€è¦å¤šå¡ | å•å¡/CPU |
| å“åº”é€Ÿåº¦ | æ…¢ | å¿« |
| èƒ½åŠ› | é€šç”¨å¼º | ç‰¹å®šä»»åŠ¡å¼º |

## è’¸é¦æ–¹æ³•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è’¸é¦æ–¹æ³•                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1. è¾“å‡ºè’¸é¦ï¼šå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒ                    â”‚
â”‚  2. ç‰¹å¾è’¸é¦ï¼šå­¦ä¹ ä¸­é—´å±‚è¡¨ç¤º                            â”‚
â”‚  3. æ•°æ®è’¸é¦ï¼šç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®                    â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æ•°æ®è’¸é¦ï¼ˆæœ€å¸¸ç”¨ï¼‰

ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œç„¶åå¾®è°ƒå°æ¨¡å‹ã€‚

### ç”Ÿæˆè®­ç»ƒæ•°æ®

```python
from openai import OpenAI
import json

client = OpenAI()

def generate_training_data(task_description: str, num_samples: int = 100) -> list:
    """ä½¿ç”¨ GPT-4 ç”Ÿæˆè®­ç»ƒæ•°æ®"""
    samples = []
    
    for i in range(num_samples):
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼š
ä»»åŠ¡ï¼š{task_description}

ç”Ÿæˆæ ¼å¼ï¼š
{{"input": "è¾“å…¥æ–‡æœ¬", "output": "æœŸæœ›è¾“å‡º"}}

è¦æ±‚ï¼š
1. è¾“å…¥è¦å¤šæ ·åŒ–
2. è¾“å‡ºè¦å‡†ç¡®ã€é«˜è´¨é‡
3. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""
                },
                {"role": "user", "content": f"ç”Ÿæˆç¬¬ {i+1} ä¸ªæ ·æœ¬"}
            ],
            response_format={"type": "json_object"}
        )
        
        sample = json.loads(response.choices[0].message.content)
        samples.append(sample)
        
        if (i + 1) % 10 == 0:
            print(f"å·²ç”Ÿæˆ {i + 1}/{num_samples} ä¸ªæ ·æœ¬")
    
    return samples

# ç”Ÿæˆæ•°æ®
samples = generate_training_data(
    task_description="ä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼ˆpositive/negative/neutralï¼‰",
    num_samples=500
)

# ä¿å­˜
with open("training_data.jsonl", "w") as f:
    for sample in samples:
        f.write(json.dumps(sample, ensure_ascii=False) + "\n")
```

### å¾®è°ƒå­¦ç”Ÿæ¨¡å‹

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model

# åŠ è½½å­¦ç”Ÿæ¨¡å‹
model_name = "Qwen/Qwen2.5-1.5B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# åŠ è½½è’¸é¦æ•°æ®
dataset = load_dataset("json", data_files="training_data.jsonl")

def format_sample(sample):
    text = f"è¾“å…¥ï¼š{sample['input']}\nè¾“å‡ºï¼š{sample['output']}"
    return tokenizer(text, truncation=True, max_length=512)

dataset = dataset.map(format_sample)

# LoRA é…ç½®
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05
)

model = get_peft_model(model, lora_config)

# è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./distilled_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-4,
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"]
)

trainer.train()
```

## è¾“å‡ºè’¸é¦

è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    """è’¸é¦æŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature: float = 2.0, alpha: float = 0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆKL æ•£åº¦ï¼‰
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1),
            reduction="batchmean"
        ) * (self.temperature ** 2)
        
        # ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        hard_loss = self.ce_loss(student_logits, labels)
        
        # ç»„åˆæŸå¤±
        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss

class DistillationTrainer:
    """è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        teacher_model,
        student_model,
        tokenizer,
        temperature: float = 2.0
    ):
        self.teacher = teacher_model.eval()
        self.student = student_model
        self.tokenizer = tokenizer
        self.loss_fn = DistillationLoss(temperature=temperature)
    
    def train_step(self, batch):
        # æ•™å¸ˆæ¨¡å‹æ¨ç†ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_outputs = self.teacher(**batch)
            teacher_logits = teacher_outputs.logits
        
        # å­¦ç”Ÿæ¨¡å‹æ¨ç†
        student_outputs = self.student(**batch)
        student_logits = student_outputs.logits
        
        # è®¡ç®—è’¸é¦æŸå¤±
        loss = self.loss_fn(
            student_logits.view(-1, student_logits.size(-1)),
            teacher_logits.view(-1, teacher_logits.size(-1)),
            batch["labels"].view(-1)
        )
        
        return loss
```


## OpenAI è’¸é¦ API

OpenAI æä¾›äº†å®˜æ–¹çš„è’¸é¦åŠŸèƒ½ã€‚

```python
from openai import OpenAI

client = OpenAI()

# 1. ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå¸¦ metadata çš„å“åº”
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "è§£é‡Šé‡å­è®¡ç®—"}],
    store=True,  # å­˜å‚¨ç”¨äºè’¸é¦
    metadata={"task": "explanation", "domain": "physics"}
)

# 2. åˆ›å»ºè’¸é¦å¾®è°ƒä»»åŠ¡
# ä½¿ç”¨å­˜å‚¨çš„é«˜è´¨é‡å“åº”å¾®è°ƒå°æ¨¡å‹
fine_tune = client.fine_tuning.jobs.create(
    training_file="file-xxx",  # åŒ…å«è’¸é¦æ•°æ®
    model="gpt-4o-mini",       # å­¦ç”Ÿæ¨¡å‹
    method={
        "type": "supervised",
        "supervised": {
            "hyperparameters": {"n_epochs": 3}
        }
    }
)
```

## å®æˆ˜ï¼šè’¸é¦ä»£ç åŠ©æ‰‹

```python
class CodeAssistantDistiller:
    """ä»£ç åŠ©æ‰‹è’¸é¦"""
    
    def __init__(self):
        self.client = OpenAI()
    
    def generate_code_samples(self, num_samples: int = 200) -> list:
        """ç”Ÿæˆä»£ç è®­ç»ƒæ ·æœ¬"""
        tasks = [
            "å†™ä¸€ä¸ª Python å‡½æ•°å®ç°å¿«é€Ÿæ’åº",
            "å®ç°ä¸€ä¸ª LRU ç¼“å­˜",
            "å†™ä¸€ä¸ªå¼‚æ­¥ HTTP å®¢æˆ·ç«¯",
            # ... æ›´å¤šä»»åŠ¡
        ]
        
        samples = []
        for task in tasks:
            # ä½¿ç”¨ GPT-4 ç”Ÿæˆé«˜è´¨é‡ä»£ç 
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python å¼€å‘è€…ã€‚ç”Ÿæˆç®€æ´ã€é«˜æ•ˆã€æœ‰æ³¨é‡Šçš„ä»£ç ã€‚"
                    },
                    {"role": "user", "content": task}
                ]
            )
            
            samples.append({
                "instruction": task,
                "output": response.choices[0].message.content
            })
        
        return samples
    
    def format_for_training(self, samples: list) -> list:
        """æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ•°æ®"""
        formatted = []
        for sample in samples:
            formatted.append({
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä»£ç åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": sample["instruction"]},
                    {"role": "assistant", "content": sample["output"]}
                ]
            })
        return formatted

# ä½¿ç”¨
distiller = CodeAssistantDistiller()
samples = distiller.generate_code_samples(200)
training_data = distiller.format_for_training(samples)
```

## è’¸é¦æ•ˆæœè¯„ä¼°

```python
def evaluate_distillation(
    teacher_model,
    student_model,
    test_data: list,
    tokenizer
) -> dict:
    """è¯„ä¼°è’¸é¦æ•ˆæœ"""
    
    teacher_scores = []
    student_scores = []
    
    for sample in test_data:
        prompt = sample["input"]
        reference = sample["output"]
        
        # æ•™å¸ˆæ¨¡å‹è¾“å‡º
        teacher_output = generate(teacher_model, tokenizer, prompt)
        
        # å­¦ç”Ÿæ¨¡å‹è¾“å‡º
        student_output = generate(student_model, tokenizer, prompt)
        
        # è¯„ä¼°ï¼ˆå¯ä»¥ç”¨ GPT-4 è¯„åˆ†ï¼‰
        teacher_score = evaluate_quality(teacher_output, reference)
        student_score = evaluate_quality(student_output, reference)
        
        teacher_scores.append(teacher_score)
        student_scores.append(student_score)
    
    return {
        "teacher_avg": sum(teacher_scores) / len(teacher_scores),
        "student_avg": sum(student_scores) / len(student_scores),
        "retention_rate": sum(student_scores) / sum(teacher_scores)
    }
```

## æœ€ä½³å®è·µ

1. **æ•°æ®è´¨é‡ä¼˜å…ˆ**ï¼šè’¸é¦æ•°æ®çš„è´¨é‡å†³å®šå­¦ç”Ÿæ¨¡å‹ä¸Šé™
2. **ä»»åŠ¡èšç„¦**ï¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡è’¸é¦æ•ˆæœæ›´å¥½
3. **å¤šæ ·æ€§**ï¼šè®­ç»ƒæ•°æ®è¦è¦†ç›–å„ç§åœºæ™¯
4. **è¿­ä»£ä¼˜åŒ–**ï¼šå¤šè½®è’¸é¦é€æ­¥æå‡
5. **è¯„ä¼°éªŒè¯**ï¼šç¡®ä¿å­¦ç”Ÿæ¨¡å‹è¾¾åˆ°é¢„æœŸæ•ˆæœ

## å»¶ä¼¸é˜…è¯»

- [Distilling Step-by-Step](https://arxiv.org/abs/2305.02301)
- [OpenAI Model Distillation](https://platform.openai.com/docs/guides/distillation)
- [LLM Distillation Survey](https://arxiv.org/abs/2402.13116)