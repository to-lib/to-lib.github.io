---
sidebar_position: 19
title: ğŸ  æœ¬åœ°éƒ¨ç½² LLM
---

# æœ¬åœ°éƒ¨ç½² LLM

æœ¬åœ°éƒ¨ç½² LLM å¯ä»¥ä¿æŠ¤æ•°æ®éšç§ã€é™ä½æˆæœ¬ã€å‡å°‘å»¶è¿Ÿã€‚æœ¬æ–‡ä»‹ç»ä¸»æµçš„æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆã€‚

## éƒ¨ç½²æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ         | ç‰¹ç‚¹                   | é€‚ç”¨åœºæ™¯           |
| ------------ | ---------------------- | ------------------ |
| **Ollama**   | ç®€å•æ˜“ç”¨ï¼Œä¸€é”®éƒ¨ç½²     | å¼€å‘æµ‹è¯•ã€ä¸ªäººä½¿ç”¨ |
| **vLLM**     | é«˜æ€§èƒ½ï¼Œç”Ÿäº§çº§         | ç”Ÿäº§ç¯å¢ƒã€é«˜å¹¶å‘   |
| **llama.cpp** | è½»é‡ï¼ŒCPU å‹å¥½        | è¾¹ç¼˜è®¾å¤‡ã€ä½èµ„æº   |
| **TGI**      | HuggingFace å®˜æ–¹       | ä¼ä¸šéƒ¨ç½²           |

## Ollama

Ollama æ˜¯æœ€ç®€å•çš„æœ¬åœ° LLM éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒ macOSã€Linuxã€Windowsã€‚

### å®‰è£…

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–ä½¿ç”¨ Homebrew (macOS)
brew install ollama
```

### åŸºç¡€ä½¿ç”¨

```bash
# å¯åŠ¨æœåŠ¡
ollama serve

# è¿è¡Œæ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
ollama run llama3.2
ollama run qwen2.5:7b
ollama run deepseek-coder:6.7b

# åˆ—å‡ºå·²ä¸‹è½½æ¨¡å‹
ollama list

# åˆ é™¤æ¨¡å‹
ollama rm llama3.2
```

### API è°ƒç”¨

```python
import requests

# Ollama å…¼å®¹ OpenAI API æ ¼å¼
response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "llama3.2",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "stream": False
    }
)

print(response.json()["message"]["content"])
```

### ä½¿ç”¨ OpenAI SDK

```python
from openai import OpenAI

# æŒ‡å‘ Ollama æœåŠ¡
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # ä»»æ„å€¼
)

response = client.chat.completions.create(
    model="llama3.2",
    messages=[{"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}]
)

print(response.choices[0].message.content)
```

### æµå¼è¾“å‡º

```python
stream = client.chat.completions.create(
    model="llama3.2",
    messages=[{"role": "user", "content": "å†™ä¸€é¦–è¯—"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### è‡ªå®šä¹‰æ¨¡å‹

```bash
# åˆ›å»º Modelfile
cat > Modelfile << 'EOF'
FROM llama3.2

# è®¾ç½®ç³»ç»Ÿæç¤º
SYSTEM """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åŠ©æ‰‹ï¼Œæ“…é•¿ Python å’Œ JavaScriptã€‚
å›ç­”è¦ç®€æ´ã€å‡†ç¡®ï¼Œå¹¶æä¾›ä»£ç ç¤ºä¾‹ã€‚
"""

# è°ƒæ•´å‚æ•°
PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹
ollama create code-assistant -f Modelfile

# è¿è¡Œ
ollama run code-assistant
```

## vLLM

vLLM æ˜¯é«˜æ€§èƒ½çš„ LLM æ¨ç†å¼•æ“ï¼Œæ”¯æŒ PagedAttentionã€è¿ç»­æ‰¹å¤„ç†ç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚

### å®‰è£…

```bash
pip install vllm
```

### å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1
```

### API è°ƒç”¨

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="vllm"
)

response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

print(response.choices[0].message.content)
```

### ç¦»çº¿æ‰¹é‡æ¨ç†

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(model="Qwen/Qwen2.5-7B-Instruct")

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512
)

# æ‰¹é‡æ¨ç†
prompts = [
    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "Python æœ‰ä»€ä¹ˆä¼˜ç‚¹ï¼Ÿ",
    "è§£é‡Š RESTful API"
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Output: {output.outputs[0].text}\n")
```

### é«˜çº§é…ç½®

```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=2,      # å¤š GPU å¹¶è¡Œ
    gpu_memory_utilization=0.9,  # GPU å†…å­˜ä½¿ç”¨ç‡
    max_model_len=8192,          # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    quantization="awq",          # é‡åŒ–æ–¹å¼
    dtype="float16"              # æ•°æ®ç±»å‹
)
```

## llama.cpp

llama.cpp æ˜¯çº¯ C/C++ å®ç°çš„ LLM æ¨ç†åº“ï¼Œæ”¯æŒ CPU æ¨ç†ï¼Œèµ„æºå ç”¨ä½ã€‚

### å®‰è£… Python ç»‘å®š

```bash
pip install llama-cpp-python

# æ”¯æŒ GPU åŠ é€Ÿ
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python
```

### ä¸‹è½½æ¨¡å‹

```bash
# ä» HuggingFace ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹
# ä¾‹å¦‚ï¼šhttps://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF
```

### åŸºç¡€ä½¿ç”¨

```python
from llama_cpp import Llama

# åŠ è½½æ¨¡å‹
llm = Llama(
    model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",
    n_ctx=4096,      # ä¸Šä¸‹æ–‡é•¿åº¦
    n_threads=8,     # CPU çº¿ç¨‹æ•°
    n_gpu_layers=35  # GPU åŠ é€Ÿå±‚æ•°ï¼ˆ0 = çº¯ CPUï¼‰
)

# ç”Ÿæˆ
output = llm(
    "Q: ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\nA:",
    max_tokens=256,
    temperature=0.7,
    stop=["Q:", "\n\n"]
)

print(output["choices"][0]["text"])
```

### Chat æ ¼å¼

```python
output = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ "}
    ],
    temperature=0.7,
    max_tokens=512
)

print(output["choices"][0]["message"]["content"])
```

### å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡

```bash
python -m llama_cpp.server \
    --model ./models/llama-2-7b-chat.Q4_K_M.gguf \
    --host 0.0.0.0 \
    --port 8000 \
    --n_ctx 4096
```

## æ¨¡å‹é‡åŒ–

é‡åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å’Œå†…å­˜å ç”¨ã€‚

### å¸¸è§é‡åŒ–æ ¼å¼

| æ ¼å¼   | è¯´æ˜                     | å¤§å°ï¼ˆ7Bï¼‰ |
| ------ | ------------------------ | ---------- |
| FP16   | åŠç²¾åº¦æµ®ç‚¹               | ~14GB      |
| INT8   | 8ä½æ•´æ•°                  | ~7GB       |
| INT4   | 4ä½æ•´æ•°                  | ~4GB       |
| GGUF Q4 | llama.cpp 4ä½é‡åŒ–       | ~4GB       |
| AWQ    | æ¿€æ´»æ„ŸçŸ¥é‡åŒ–             | ~4GB       |
| GPTQ   | åè®­ç»ƒé‡åŒ–               | ~4GB       |

### ä½¿ç”¨ AutoGPTQ é‡åŒ–

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# åŠ è½½æ¨¡å‹
model_id = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# é‡åŒ–é…ç½®
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

# é‡åŒ–
model = AutoGPTQForCausalLM.from_pretrained(
    model_id,
    quantize_config=quantize_config
)

# å‡†å¤‡æ ¡å‡†æ•°æ®
examples = [tokenizer(text) for text in calibration_texts]
model.quantize(examples)

# ä¿å­˜
model.save_quantized("./qwen2.5-7b-gptq")
```

## LangChain é›†æˆ

### Ollama

```python
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

# LLM
llm = Ollama(model="llama3.2")
response = llm.invoke("ä½ å¥½")

# Chat Model
chat = ChatOllama(model="llama3.2")
response = chat.invoke([{"role": "user", "content": "ä½ å¥½"}])
```

### llama.cpp

```python
from langchain_community.llms import LlamaCpp

llm = LlamaCpp(
    model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",
    n_ctx=4096,
    n_gpu_layers=35,
    temperature=0.7
)

response = llm.invoke("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
```

## Docker éƒ¨ç½²

### Ollama Docker

```yaml
# docker-compose.yml
version: '3.8'
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama_data:
```

### vLLM Docker

```yaml
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

## ç¡¬ä»¶è¦æ±‚

### GPU æ˜¾å­˜éœ€æ±‚ï¼ˆæ¨ç†ï¼‰

| æ¨¡å‹å¤§å° | FP16   | INT8   | INT4   |
| -------- | ------ | ------ | ------ |
| 7B       | 14GB   | 7GB    | 4GB    |
| 13B      | 26GB   | 13GB   | 7GB    |
| 70B      | 140GB  | 70GB   | 35GB   |

### æ¨èé…ç½®

| åœºæ™¯       | GPU                | å†…å­˜   | æ¨èæ¨¡å‹      |
| ---------- | ------------------ | ------ | ------------- |
| ä¸ªäººå¼€å‘   | RTX 3060 12GB      | 16GB   | 7B INT4       |
| å°å›¢é˜Ÿ     | RTX 4090 24GB      | 32GB   | 7B-13B        |
| ç”Ÿäº§ç¯å¢ƒ   | A100 40GB/80GB     | 64GB+  | 13B-70B       |

## æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹å¤„ç†

```python
# vLLM è‡ªåŠ¨æ‰¹å¤„ç†
# å¤šä¸ªè¯·æ±‚ä¼šè¢«åˆå¹¶å¤„ç†ï¼Œæé«˜ååé‡
```

### 2. KV Cache ä¼˜åŒ–

```python
# vLLM PagedAttention
llm = LLM(
    model="...",
    gpu_memory_utilization=0.9,  # æ›´å¤šå†…å­˜ç”¨äº KV cache
)
```

### 3. æŠ•æœºè§£ç 

```python
# ä½¿ç”¨å°æ¨¡å‹åŠ é€Ÿå¤§æ¨¡å‹
llm = LLM(
    model="large-model",
    speculative_model="small-model",
    num_speculative_tokens=5
)
```

## å»¶ä¼¸é˜…è¯»

- [Ollama å®˜ç½‘](https://ollama.com/)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [HuggingFace TGI](https://huggingface.co/docs/text-generation-inference)
