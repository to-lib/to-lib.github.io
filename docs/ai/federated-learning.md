---
sidebar_position: 34
title: ğŸ”’ è”é‚¦å­¦ä¹ 
---

# è”é‚¦å­¦ä¹ 

è”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰æ˜¯ä¸€ç§åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…è®¸å¤šæ–¹åœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹åä½œè®­ç»ƒæ¨¡å‹ï¼Œä¿æŠ¤æ•°æ®éšç§ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦è”é‚¦å­¦ä¹ ï¼Ÿ

```
ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼š
æ•°æ®æ–¹ A â”€â”€â”
æ•°æ®æ–¹ B â”€â”€â”¼â”€â”€> ä¸­å¿ƒæœåŠ¡å™¨ï¼ˆæ±‡é›†æ‰€æœ‰æ•°æ®ï¼‰â”€â”€> è®­ç»ƒæ¨¡å‹
æ•°æ®æ–¹ C â”€â”€â”˜
           âš ï¸ éšç§é£é™©

è”é‚¦å­¦ä¹ ï¼š
æ•°æ®æ–¹ A â”€â”€> æœ¬åœ°è®­ç»ƒ â”€â”€> æ¨¡å‹æ›´æ–° â”€â”€â”
æ•°æ®æ–¹ B â”€â”€> æœ¬åœ°è®­ç»ƒ â”€â”€> æ¨¡å‹æ›´æ–° â”€â”€â”¼â”€â”€> èšåˆæœåŠ¡å™¨ â”€â”€> å…¨å±€æ¨¡å‹
æ•°æ®æ–¹ C â”€â”€> æœ¬åœ°è®­ç»ƒ â”€â”€> æ¨¡å‹æ›´æ–° â”€â”€â”˜
                                    âœ… æ•°æ®ä¸å‡ºåŸŸ
```

## è”é‚¦å­¦ä¹ ç±»å‹

| ç±»å‹ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| æ¨ªå‘è”é‚¦ | æ ·æœ¬ä¸åŒï¼Œç‰¹å¾ç›¸åŒ | å¤šå®¶åŒ»é™¢çš„ç—…å†æ•°æ® |
| çºµå‘è”é‚¦ | æ ·æœ¬ç›¸åŒï¼Œç‰¹å¾ä¸åŒ | é“¶è¡Œ+ç”µå•†çš„ç”¨æˆ·æ•°æ® |
| è”é‚¦è¿ç§» | æ ·æœ¬å’Œç‰¹å¾éƒ½ä¸åŒ | è·¨é¢†åŸŸåä½œ |

## åŸºç¡€å®ç°

### è”é‚¦å¹³å‡ç®—æ³• (FedAvg)

```python
import torch
import torch.nn as nn
from typing import List, Dict
import copy

class FederatedServer:
    """è”é‚¦å­¦ä¹ æœåŠ¡å™¨"""
    
    def __init__(self, model: nn.Module):
        self.global_model = model
        self.client_updates = []
    
    def distribute_model(self) -> Dict:
        """åˆ†å‘å…¨å±€æ¨¡å‹"""
        return copy.deepcopy(self.global_model.state_dict())
    
    def receive_update(self, client_update: Dict, num_samples: int):
        """æ¥æ”¶å®¢æˆ·ç«¯æ›´æ–°"""
        self.client_updates.append({
            "weights": client_update,
            "num_samples": num_samples
        })
    
    def aggregate(self):
        """èšåˆå®¢æˆ·ç«¯æ›´æ–°ï¼ˆFedAvgï¼‰"""
        if not self.client_updates:
            return
        
        total_samples = sum(u["num_samples"] for u in self.client_updates)
        
        # åŠ æƒå¹³å‡
        new_weights = {}
        for key in self.client_updates[0]["weights"].keys():
            weighted_sum = sum(
                u["weights"][key] * u["num_samples"]
                for u in self.client_updates
            )
            new_weights[key] = weighted_sum / total_samples
        
        self.global_model.load_state_dict(new_weights)
        self.client_updates = []

class FederatedClient:
    """è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯"""
    
    def __init__(self, client_id: str, local_data, model: nn.Module):
        self.client_id = client_id
        self.local_data = local_data
        self.model = model
        self.optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        self.criterion = nn.CrossEntropyLoss()
    
    def receive_model(self, global_weights: Dict):
        """æ¥æ”¶å…¨å±€æ¨¡å‹"""
        self.model.load_state_dict(global_weights)
    
    def local_train(self, epochs: int = 5) -> Dict:
        """æœ¬åœ°è®­ç»ƒ"""
        self.model.train()
        
        for epoch in range(epochs):
            for batch_x, batch_y in self.local_data:
                self.optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = self.criterion(outputs, batch_y)
                loss.backward()
                self.optimizer.step()
        
        return self.model.state_dict()
    
    def get_num_samples(self) -> int:
        return len(self.local_data.dataset)

def federated_training(
    server: FederatedServer,
    clients: List[FederatedClient],
    rounds: int = 10,
    local_epochs: int = 5
):
    """è”é‚¦è®­ç»ƒä¸»å¾ªç¯"""
    for round_num in range(rounds):
        print(f"Round {round_num + 1}/{rounds}")
        
        # 1. åˆ†å‘å…¨å±€æ¨¡å‹
        global_weights = server.distribute_model()
        
        # 2. å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
        for client in clients:
            client.receive_model(global_weights)
            local_weights = client.local_train(epochs=local_epochs)
            server.receive_update(local_weights, client.get_num_samples())
        
        # 3. èšåˆæ›´æ–°
        server.aggregate()
        
        print(f"Round {round_num + 1} completed")
```

## å·®åˆ†éšç§

æ·»åŠ å™ªå£°ä¿æŠ¤æ¨¡å‹æ›´æ–°ã€‚

```python
import numpy as np

class DifferentialPrivacy:
    """å·®åˆ†éšç§"""
    
    def __init__(self, epsilon: float = 1.0, delta: float = 1e-5):
        self.epsilon = epsilon
        self.delta = delta
    
    def add_noise(self, gradients: Dict, sensitivity: float = 1.0) -> Dict:
        """æ·»åŠ é«˜æ–¯å™ªå£°"""
        sigma = sensitivity * np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon
        
        noisy_gradients = {}
        for key, value in gradients.items():
            noise = torch.normal(0, sigma, size=value.shape)
            noisy_gradients[key] = value + noise
        
        return noisy_gradients
    
    def clip_gradients(self, gradients: Dict, max_norm: float = 1.0) -> Dict:
        """æ¢¯åº¦è£å‰ª"""
        total_norm = 0
        for grad in gradients.values():
            total_norm += grad.norm() ** 2
        total_norm = total_norm ** 0.5
        
        clip_coef = max_norm / (total_norm + 1e-6)
        if clip_coef < 1:
            for key in gradients:
                gradients[key] = gradients[key] * clip_coef
        
        return gradients

class PrivateFederatedClient(FederatedClient):
    """å¸¦å·®åˆ†éšç§çš„å®¢æˆ·ç«¯"""
    
    def __init__(self, *args, epsilon: float = 1.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.dp = DifferentialPrivacy(epsilon=epsilon)
    
    def local_train(self, epochs: int = 5) -> Dict:
        weights = super().local_train(epochs)
        
        # è®¡ç®—æ›´æ–°å·®å€¼
        global_weights = self.model.state_dict()
        updates = {k: weights[k] - global_weights[k] for k in weights}
        
        # è£å‰ªå’ŒåŠ å™ª
        updates = self.dp.clip_gradients(updates)
        updates = self.dp.add_noise(updates)
        
        # è¿”å›åŠ å™ªåçš„æƒé‡
        return {k: global_weights[k] + updates[k] for k in weights}
```

## å®‰å…¨èšåˆ

é˜²æ­¢æœåŠ¡å™¨çœ‹åˆ°å•ä¸ªå®¢æˆ·ç«¯çš„æ›´æ–°ã€‚

```python
import secrets
from typing import Tuple

class SecureAggregation:
    """å®‰å…¨èšåˆ"""
    
    def __init__(self, num_clients: int):
        self.num_clients = num_clients
        self.masks = {}
    
    def generate_masks(self, client_ids: List[str]) -> Dict[str, Dict]:
        """ç”Ÿæˆæˆå¯¹æ©ç """
        masks = {cid: {} for cid in client_ids}
        
        for i, cid1 in enumerate(client_ids):
            for cid2 in client_ids[i+1:]:
                # ç”Ÿæˆéšæœºæ©ç 
                seed = secrets.randbits(256)
                mask = self._generate_mask_from_seed(seed)
                
                masks[cid1][cid2] = mask
                masks[cid2][cid1] = -mask  # ç›¸åçš„æ©ç 
        
        return masks
    
    def _generate_mask_from_seed(self, seed: int) -> torch.Tensor:
        torch.manual_seed(seed)
        return torch.randn(1000)  # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´
    
    def mask_update(self, update: Dict, masks: Dict) -> Dict:
        """åº”ç”¨æ©ç """
        total_mask = sum(masks.values())
        masked = {}
        for key, value in update.items():
            masked[key] = value + total_mask[:value.numel()].reshape(value.shape)
        return masked
```


## Flower æ¡†æ¶

Flower æ˜¯æµè¡Œçš„è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚

```bash
pip install flwr
```

### æœåŠ¡ç«¯

```python
import flwr as fl

# å®šä¹‰èšåˆç­–ç•¥
strategy = fl.server.strategy.FedAvg(
    fraction_fit=0.5,           # æ¯è½®å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æ¯”ä¾‹
    fraction_evaluate=0.5,      # æ¯è½®å‚ä¸è¯„ä¼°çš„å®¢æˆ·ç«¯æ¯”ä¾‹
    min_fit_clients=2,          # æœ€å°‘è®­ç»ƒå®¢æˆ·ç«¯æ•°
    min_evaluate_clients=2,     # æœ€å°‘è¯„ä¼°å®¢æˆ·ç«¯æ•°
    min_available_clients=2,    # æœ€å°‘å¯ç”¨å®¢æˆ·ç«¯æ•°
)

# å¯åŠ¨æœåŠ¡å™¨
fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=10),
    strategy=strategy
)
```

### å®¢æˆ·ç«¯

```python
import flwr as fl
import torch

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, model, trainloader, testloader):
        self.model = model
        self.trainloader = trainloader
        self.testloader = testloader
    
    def get_parameters(self, config):
        return [val.cpu().numpy() for val in self.model.parameters()]
    
    def set_parameters(self, parameters):
        for param, new_val in zip(self.model.parameters(), parameters):
            param.data = torch.tensor(new_val)
    
    def fit(self, parameters, config):
        self.set_parameters(parameters)
        train(self.model, self.trainloader, epochs=5)
        return self.get_parameters(config), len(self.trainloader.dataset), {}
    
    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        loss, accuracy = test(self.model, self.testloader)
        return float(loss), len(self.testloader.dataset), {"accuracy": accuracy}

# å¯åŠ¨å®¢æˆ·ç«¯
fl.client.start_numpy_client(
    server_address="localhost:8080",
    client=FlowerClient(model, trainloader, testloader)
)
```

## LLM è”é‚¦å¾®è°ƒ

```python
class FederatedLLMClient:
    """LLM è”é‚¦å¾®è°ƒå®¢æˆ·ç«¯"""
    
    def __init__(self, model_name: str, local_data_path: str):
        from peft import LoraConfig, get_peft_model
        from transformers import AutoModelForCausalLM
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # åªè®­ç»ƒ LoRA å‚æ•°
        lora_config = LoraConfig(r=8, lora_alpha=16)
        self.model = get_peft_model(self.model, lora_config)
        
        self.local_data = self._load_data(local_data_path)
    
    def get_lora_parameters(self) -> Dict:
        """åªè¿”å› LoRA å‚æ•°"""
        return {
            k: v for k, v in self.model.state_dict().items()
            if "lora" in k
        }
    
    def set_lora_parameters(self, parameters: Dict):
        """è®¾ç½® LoRA å‚æ•°"""
        state_dict = self.model.state_dict()
        state_dict.update(parameters)
        self.model.load_state_dict(state_dict)
    
    def local_train(self, epochs: int = 1):
        """æœ¬åœ°å¾®è°ƒ"""
        # è®­ç»ƒé€»è¾‘...
        pass
```

## åº”ç”¨åœºæ™¯

| åœºæ™¯ | è¯´æ˜ |
|------|------|
| åŒ»ç–— | å¤šå®¶åŒ»é™¢åä½œè®­ç»ƒè¯Šæ–­æ¨¡å‹ |
| é‡‘è | é“¶è¡Œé—´åæ¬ºè¯ˆæ¨¡å‹è®­ç»ƒ |
| ç§»åŠ¨è®¾å¤‡ | æ‰‹æœºé”®ç›˜é¢„æµ‹ã€è¯­éŸ³è¯†åˆ« |
| ä¼ä¸šåä½œ | è·¨å…¬å¸æ•°æ®åä½œ |

## æœ€ä½³å®è·µ

1. **é€šä¿¡æ•ˆç‡**ï¼šå‹ç¼©æ¨¡å‹æ›´æ–°å‡å°‘é€šä¿¡
2. **å¼‚æ„å¤„ç†**ï¼šå¤„ç†å®¢æˆ·ç«¯æ•°æ®ä¸å‡è¡¡
3. **éšç§ä¿æŠ¤**ï¼šç»“åˆå·®åˆ†éšç§å’Œå®‰å…¨èšåˆ
4. **å®¹é”™æœºåˆ¶**ï¼šå¤„ç†å®¢æˆ·ç«¯æ‰çº¿
5. **æ¨¡å‹éªŒè¯**ï¼šé˜²æ­¢æ¶æ„å®¢æˆ·ç«¯æ”»å‡»

## å»¶ä¼¸é˜…è¯»

- [Flower](https://flower.dev/)
- [PySyft](https://github.com/OpenMined/PySyft)
- [TensorFlow Federated](https://www.tensorflow.org/federated)