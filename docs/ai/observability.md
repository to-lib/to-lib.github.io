---
sidebar_position: 28
title: ğŸ“Š AI å¯è§‚æµ‹æ€§
---

# AI å¯è§‚æµ‹æ€§

AI å¯è§‚æµ‹æ€§æ˜¯æŒ‡å¯¹ AI åº”ç”¨è¿›è¡Œç›‘æ§ã€è¿½è¸ªå’Œè°ƒè¯•çš„èƒ½åŠ›ï¼Œå¸®åŠ©ç†è§£æ¨¡å‹è¡Œä¸ºã€å®šä½é—®é¢˜ã€ä¼˜åŒ–æ€§èƒ½ã€‚

## å¯è§‚æµ‹æ€§ä¸‰æ”¯æŸ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI å¯è§‚æµ‹æ€§                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Tracingï¼ˆè¿½è¸ªï¼‰                                        â”‚
â”‚  â””â”€> å®Œæ•´è°ƒç”¨é“¾è·¯ã€æ¯æ­¥è€—æ—¶ã€Token ä½¿ç”¨                 â”‚
â”‚                                                         â”‚
â”‚  Loggingï¼ˆæ—¥å¿—ï¼‰                                        â”‚
â”‚  â””â”€> è¾“å…¥è¾“å‡ºè®°å½•ã€é”™è¯¯ä¿¡æ¯ã€è°ƒè¯•æ•°æ®                   â”‚
â”‚                                                         â”‚
â”‚  Metricsï¼ˆæŒ‡æ ‡ï¼‰                                        â”‚
â”‚  â””â”€> å»¶è¿Ÿã€æˆåŠŸç‡ã€æˆæœ¬ã€è´¨é‡è¯„åˆ†                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## åŸºç¡€å®ç°

### ç®€å•æ—¥å¿—

```python
import logging
import json
from datetime import datetime
from functools import wraps

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ai_app")

def log_llm_call(func):
    """LLM è°ƒç”¨æ—¥å¿—è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = datetime.now()
        
        # è®°å½•è¾“å…¥
        logger.info(json.dumps({
            "event": "llm_call_start",
            "function": func.__name__,
            "timestamp": start_time.isoformat(),
            "kwargs": {k: str(v)[:200] for k, v in kwargs.items()}
        }, ensure_ascii=False))
        
        try:
            result = func(*args, **kwargs)
            duration = (datetime.now() - start_time).total_seconds()
            
            # è®°å½•æˆåŠŸ
            logger.info(json.dumps({
                "event": "llm_call_success",
                "function": func.__name__,
                "duration_seconds": duration,
                "output_preview": str(result)[:200]
            }, ensure_ascii=False))
            
            return result
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            
            # è®°å½•é”™è¯¯
            logger.error(json.dumps({
                "event": "llm_call_error",
                "function": func.__name__,
                "duration_seconds": duration,
                "error": str(e)
            }, ensure_ascii=False))
            
            raise
    
    return wrapper

@log_llm_call
def chat(message: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content
```

### è¿½è¸ªç³»ç»Ÿ

```python
import uuid
from dataclasses import dataclass, field
from typing import Any
from contextlib import contextmanager

@dataclass
class Span:
    """è¿½è¸ª Span"""
    name: str
    trace_id: str
    span_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    parent_id: str | None = None
    start_time: datetime = field(default_factory=datetime.now)
    end_time: datetime | None = None
    attributes: dict = field(default_factory=dict)
    events: list = field(default_factory=list)
    
    def set_attribute(self, key: str, value: Any):
        self.attributes[key] = value
    
    def add_event(self, name: str, attributes: dict = None):
        self.events.append({
            "name": name,
            "timestamp": datetime.now().isoformat(),
            "attributes": attributes or {}
        })
    
    def end(self):
        self.end_time = datetime.now()
    
    @property
    def duration_ms(self) -> float:
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds() * 1000
        return 0

class Tracer:
    """ç®€å•è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.spans: list[Span] = []
        self._current_trace_id: str | None = None
        self._current_span: Span | None = None
    
    @contextmanager
    def start_trace(self, name: str):
        """å¼€å§‹æ–°çš„è¿½è¸ª"""
        self._current_trace_id = str(uuid.uuid4())
        span = Span(name=name, trace_id=self._current_trace_id)
        self._current_span = span
        
        try:
            yield span
        finally:
            span.end()
            self.spans.append(span)
            self._current_span = None
    
    @contextmanager
    def start_span(self, name: str):
        """å¼€å§‹å­ Span"""
        parent = self._current_span
        span = Span(
            name=name,
            trace_id=self._current_trace_id,
            parent_id=parent.span_id if parent else None
        )
        self._current_span = span
        
        try:
            yield span
        finally:
            span.end()
            self.spans.append(span)
            self._current_span = parent
    
    def export(self) -> list[dict]:
        """å¯¼å‡ºè¿½è¸ªæ•°æ®"""
        return [
            {
                "name": s.name,
                "trace_id": s.trace_id,
                "span_id": s.span_id,
                "parent_id": s.parent_id,
                "duration_ms": s.duration_ms,
                "attributes": s.attributes,
                "events": s.events
            }
            for s in self.spans
        ]

tracer = Tracer()

# ä½¿ç”¨ç¤ºä¾‹
def rag_query(question: str) -> str:
    with tracer.start_trace("rag_query") as trace:
        trace.set_attribute("question", question)
        
        # æ£€ç´¢
        with tracer.start_span("retrieval") as span:
            docs = retrieve_documents(question)
            span.set_attribute("num_docs", len(docs))
        
        # ç”Ÿæˆ
        with tracer.start_span("generation") as span:
            response = generate_answer(question, docs)
            span.set_attribute("response_length", len(response))
        
        return response
```


## LangSmith

LangSmith æ˜¯ LangChain å®˜æ–¹çš„å¯è§‚æµ‹æ€§å¹³å°ã€‚

### é…ç½®

```bash
pip install langsmith
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=your_api_key
export LANGCHAIN_PROJECT=my_project
```

### è‡ªåŠ¨è¿½è¸ª

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# é…ç½®åè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ LangChain è°ƒç”¨
llm = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚"),
    ("user", "{input}")
])

chain = prompt | llm
response = chain.invoke({"input": "ä½ å¥½"})
# è‡ªåŠ¨è®°å½•åˆ° LangSmith
```

### æ‰‹åŠ¨è¿½è¸ª

```python
from langsmith import traceable

@traceable(name="my_function")
def process_query(query: str) -> str:
    # å‡½æ•°å†…çš„æ‰€æœ‰ LLM è°ƒç”¨éƒ½ä¼šè¢«è¿½è¸ª
    response = llm.invoke(query)
    return response.content

@traceable(run_type="retriever")
def search_documents(query: str) -> list:
    # æ ‡è®°ä¸ºæ£€ç´¢å™¨ç±»å‹
    return vector_store.similarity_search(query)
```

### è¯„ä¼°

```python
from langsmith import Client
from langsmith.evaluation import evaluate

client = Client()

# åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset("qa_dataset")
client.create_examples(
    inputs=[{"question": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"}],
    outputs=[{"answer": "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ..."}],
    dataset_id=dataset.id
)

# å®šä¹‰è¯„ä¼°å‡½æ•°
def correctness(run, example):
    # æ¯”è¾ƒé¢„æµ‹å’Œå‚è€ƒç­”æ¡ˆ
    prediction = run.outputs["output"]
    reference = example.outputs["answer"]
    # è¿”å›è¯„åˆ†
    return {"score": 0.8}

# è¿è¡Œè¯„ä¼°
results = evaluate(
    lambda x: chain.invoke(x),
    data=dataset.name,
    evaluators=[correctness]
)
```

## OpenTelemetry é›†æˆ

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# é…ç½® OpenTelemetry
provider = TracerProvider()
processor = BatchSpanProcessor(OTLPSpanExporter(endpoint="http://localhost:4317"))
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

def llm_call_with_otel(prompt: str) -> str:
    with tracer.start_as_current_span("llm_call") as span:
        span.set_attribute("prompt", prompt[:100])
        span.set_attribute("model", "gpt-4o")
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        
        result = response.choices[0].message.content
        span.set_attribute("response_length", len(result))
        span.set_attribute("tokens_used", response.usage.total_tokens)
        
        return result
```

## æŒ‡æ ‡æ”¶é›†

```python
from dataclasses import dataclass, field
from collections import defaultdict
import time

@dataclass
class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    latencies: list = field(default_factory=list)
    token_counts: list = field(default_factory=list)
    error_counts: dict = field(default_factory=lambda: defaultdict(int))
    success_count: int = 0
    total_cost: float = 0.0
    
    def record_latency(self, latency_ms: float):
        self.latencies.append(latency_ms)
    
    def record_tokens(self, input_tokens: int, output_tokens: int):
        self.token_counts.append({
            "input": input_tokens,
            "output": output_tokens
        })
    
    def record_error(self, error_type: str):
        self.error_counts[error_type] += 1
    
    def record_success(self):
        self.success_count += 1
    
    def record_cost(self, cost: float):
        self.total_cost += cost
    
    def get_stats(self) -> dict:
        total_requests = self.success_count + sum(self.error_counts.values())
        
        return {
            "total_requests": total_requests,
            "success_rate": self.success_count / total_requests if total_requests > 0 else 0,
            "avg_latency_ms": sum(self.latencies) / len(self.latencies) if self.latencies else 0,
            "p95_latency_ms": sorted(self.latencies)[int(len(self.latencies) * 0.95)] if self.latencies else 0,
            "total_tokens": sum(t["input"] + t["output"] for t in self.token_counts),
            "total_cost": self.total_cost,
            "error_breakdown": dict(self.error_counts)
        }

metrics = MetricsCollector()

def monitored_chat(message: str) -> str:
    start = time.time()
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": message}]
        )
        
        latency = (time.time() - start) * 1000
        metrics.record_latency(latency)
        metrics.record_tokens(
            response.usage.prompt_tokens,
            response.usage.completion_tokens
        )
        metrics.record_success()
        
        return response.choices[0].message.content
        
    except Exception as e:
        metrics.record_error(type(e).__name__)
        raise
```


## è´¨é‡ç›‘æ§

```python
class QualityMonitor:
    """è´¨é‡ç›‘æ§"""
    
    def __init__(self):
        self.client = OpenAI()
        self.scores = []
    
    def evaluate_response(
        self,
        question: str,
        response: str,
        context: str = ""
    ) -> dict:
        """è¯„ä¼°å›å¤è´¨é‡"""
        eval_prompt = f"""
è¯„ä¼°ä»¥ä¸‹ AI å›å¤çš„è´¨é‡ï¼ˆ1-5 åˆ†ï¼‰ï¼š

é—®é¢˜ï¼š{question}
{"ä¸Šä¸‹æ–‡ï¼š" + context if context else ""}
å›å¤ï¼š{response}

è¯„ä¼°ç»´åº¦ï¼š
1. ç›¸å…³æ€§ï¼šå›å¤æ˜¯å¦åˆ‡é¢˜
2. å‡†ç¡®æ€§ï¼šä¿¡æ¯æ˜¯å¦æ­£ç¡®
3. å®Œæ•´æ€§ï¼šæ˜¯å¦å®Œæ•´å›ç­”é—®é¢˜
4. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°

è¿”å› JSONï¼š{{"relevance": 1-5, "accuracy": 1-5, "completeness": 1-5, "clarity": 1-5}}
"""
        
        eval_response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": eval_prompt}],
            response_format={"type": "json_object"}
        )
        
        scores = json.loads(eval_response.choices[0].message.content)
        scores["overall"] = sum(scores.values()) / len(scores)
        self.scores.append(scores)
        
        return scores
    
    def get_average_scores(self) -> dict:
        if not self.scores:
            return {}
        
        keys = self.scores[0].keys()
        return {
            k: sum(s[k] for s in self.scores) / len(self.scores)
            for k in keys
        }
```

## å‘Šè­¦ç³»ç»Ÿ

```python
from enum import Enum

class AlertLevel(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

class AlertSystem:
    """å‘Šè­¦ç³»ç»Ÿ"""
    
    def __init__(self):
        self.thresholds = {
            "latency_p95_ms": 5000,
            "error_rate": 0.05,
            "cost_daily": 100.0
        }
        self.handlers = []
    
    def add_handler(self, handler):
        self.handlers.append(handler)
    
    def check(self, metrics: dict):
        """æ£€æŸ¥æŒ‡æ ‡å¹¶è§¦å‘å‘Šè­¦"""
        alerts = []
        
        # å»¶è¿Ÿå‘Šè­¦
        if metrics.get("p95_latency_ms", 0) > self.thresholds["latency_p95_ms"]:
            alerts.append({
                "level": AlertLevel.WARNING,
                "message": f"P95 å»¶è¿Ÿè¿‡é«˜ï¼š{metrics['p95_latency_ms']:.0f}ms"
            })
        
        # é”™è¯¯ç‡å‘Šè­¦
        error_rate = 1 - metrics.get("success_rate", 1)
        if error_rate > self.thresholds["error_rate"]:
            alerts.append({
                "level": AlertLevel.CRITICAL,
                "message": f"é”™è¯¯ç‡è¿‡é«˜ï¼š{error_rate:.2%}"
            })
        
        # æˆæœ¬å‘Šè­¦
        if metrics.get("total_cost", 0) > self.thresholds["cost_daily"]:
            alerts.append({
                "level": AlertLevel.WARNING,
                "message": f"æ—¥æˆæœ¬è¶…é™ï¼š${metrics['total_cost']:.2f}"
            })
        
        # è§¦å‘å¤„ç†å™¨
        for alert in alerts:
            for handler in self.handlers:
                handler(alert)
        
        return alerts

# ä½¿ç”¨
alert_system = AlertSystem()
alert_system.add_handler(lambda a: print(f"[{a['level'].value}] {a['message']}"))
```

## å¯è§‚æµ‹æ€§å¹³å°å¯¹æ¯”

| å¹³å° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| LangSmith | LangChain åŸç”Ÿæ”¯æŒ | LangChain é¡¹ç›® |
| Arize | å¼ºå¤§çš„æ•°æ®åˆ†æ | ç”Ÿäº§ç¯å¢ƒç›‘æ§ |
| Weights & Biases | ML å®éªŒè¿½è¸ª | æ¨¡å‹è®­ç»ƒ |
| Helicone | ç®€å•æ˜“ç”¨ | å¿«é€Ÿé›†æˆ |
| OpenTelemetry | æ ‡å‡†åŒ–ã€å¯æ‰©å±• | ä¼ä¸šçº§åº”ç”¨ |

## æœ€ä½³å®è·µ

1. **å…¨é“¾è·¯è¿½è¸ª**ï¼šè®°å½•ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´é“¾è·¯
2. **é‡‡æ ·ç­–ç•¥**ï¼šé«˜æµé‡æ—¶ä½¿ç”¨é‡‡æ ·å‡å°‘å¼€é”€
3. **æ•æ„Ÿä¿¡æ¯è„±æ•**ï¼šæ—¥å¿—ä¸­ä¸è®°å½•æ•æ„Ÿæ•°æ®
4. **å®æ—¶å‘Šè­¦**ï¼šå…³é”®æŒ‡æ ‡å¼‚å¸¸æ—¶åŠæ—¶é€šçŸ¥
5. **å®šæœŸå›é¡¾**ï¼šåˆ†æå†å²æ•°æ®ä¼˜åŒ–ç³»ç»Ÿ

## å»¶ä¼¸é˜…è¯»

- [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/)
- [OpenTelemetry](https://opentelemetry.io/)
- [Arize AI](https://arize.com/)