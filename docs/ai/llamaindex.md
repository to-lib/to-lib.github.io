---
sidebar_position: 21
title: ğŸ¦™ LlamaIndex æ¡†æ¶
---

# LlamaIndex æ¡†æ¶

LlamaIndex æ˜¯ä¸“æ³¨äºæ•°æ®ç´¢å¼•å’Œæ£€ç´¢çš„ LLM åº”ç”¨æ¡†æ¶ï¼Œç‰¹åˆ«æ“…é•¿ RAG åœºæ™¯ã€‚ç›¸æ¯” LangChainï¼Œå®ƒåœ¨æ•°æ®å¤„ç†å’Œæ£€ç´¢ä¼˜åŒ–æ–¹é¢æ›´åŠ ä¸“ä¸šã€‚

## LlamaIndex vs LangChain

| ç‰¹æ€§         | LlamaIndex           | LangChain            |
| ------------ | -------------------- | -------------------- |
| **å®šä½**     | æ•°æ®ç´¢å¼•ä¸æ£€ç´¢       | é€šç”¨ LLM åº”ç”¨æ¡†æ¶    |
| **RAG ä¼˜åŒ–** | æ·±åº¦ä¼˜åŒ–ï¼Œå¼€ç®±å³ç”¨   | éœ€è¦æ›´å¤šé…ç½®         |
| **æ•°æ®è¿æ¥** | ä¸°å¯Œçš„æ•°æ®åŠ è½½å™¨     | ç›¸å¯¹è¾ƒå°‘             |
| **ç´¢å¼•ç±»å‹** | å¤šç§ä¸“ä¸šç´¢å¼•         | ä¸»è¦ä¾èµ–å‘é‡ç´¢å¼•     |
| **å­¦ä¹ æ›²çº¿** | ç›¸å¯¹ç®€å•             | æ¦‚å¿µè¾ƒå¤š             |
| **çµæ´»æ€§**   | RAG åœºæ™¯çµæ´»         | é€šç”¨åœºæ™¯æ›´çµæ´»       |

## å®‰è£…

```bash
pip install llama-index
pip install llama-index-llms-openai
pip install llama-index-embeddings-openai
```

## å¿«é€Ÿå¼€å§‹

### 5 åˆ†é’Ÿæ„å»º RAG

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings

# é…ç½®æ¨¡å‹
Settings.llm = OpenAI(model="gpt-4o", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# æŸ¥è¯¢
query_engine = index.as_query_engine()
response = query_engine.query("æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

### æŒä¹…åŒ–ç´¢å¼•

```python
from llama_index.core import StorageContext, load_index_from_storage

# ä¿å­˜ç´¢å¼•
index.storage_context.persist(persist_dir="./storage")

# åŠ è½½ç´¢å¼•
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Document å’Œ Node

```python
from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter

# åˆ›å»ºæ–‡æ¡£
doc = Document(
    text="è¿™æ˜¯æ–‡æ¡£å†…å®¹...",
    metadata={"source": "wiki", "author": "å¼ ä¸‰"}
)

# åˆ‡åˆ†ä¸ºèŠ‚ç‚¹
parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
nodes = parser.get_nodes_from_documents([doc])

for node in nodes:
    print(f"Node ID: {node.node_id}")
    print(f"Content: {node.text[:100]}...")
    print(f"Metadata: {node.metadata}")
```

### 2. ç´¢å¼•ç±»å‹

```python
from llama_index.core import (
    VectorStoreIndex,
    SummaryIndex,
    TreeIndex,
    KeywordTableIndex
)

# å‘é‡ç´¢å¼• - è¯­ä¹‰æœç´¢
vector_index = VectorStoreIndex.from_documents(documents)

# æ‘˜è¦ç´¢å¼• - éå†æ‰€æœ‰èŠ‚ç‚¹
summary_index = SummaryIndex.from_documents(documents)

# æ ‘å½¢ç´¢å¼• - å±‚æ¬¡åŒ–æ‘˜è¦
tree_index = TreeIndex.from_documents(documents)

# å…³é”®è¯ç´¢å¼• - å…³é”®è¯åŒ¹é…
keyword_index = KeywordTableIndex.from_documents(documents)
```

### 3. æ£€ç´¢å™¨

```python
from llama_index.core.retrievers import VectorIndexRetriever

# åŸºç¡€æ£€ç´¢å™¨
retriever = index.as_retriever(similarity_top_k=5)
nodes = retriever.retrieve("æŸ¥è¯¢å†…å®¹")

# è‡ªå®šä¹‰æ£€ç´¢å™¨
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
    filters={"source": "wiki"}  # å…ƒæ•°æ®è¿‡æ»¤
)
```

### 4. æŸ¥è¯¢å¼•æ“

```python
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import get_response_synthesizer

# è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“
retriever = index.as_retriever(similarity_top_k=5)
response_synthesizer = get_response_synthesizer(
    response_mode="compact"  # tree_summarize, refine, compact, simple
)

query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer
)

response = query_engine.query("é—®é¢˜")
```

## æ•°æ®åŠ è½½å™¨

### å†…ç½®åŠ è½½å™¨

```python
from llama_index.core import SimpleDirectoryReader

# è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹
documents = SimpleDirectoryReader(
    input_dir="./data",
    recursive=True,
    required_exts=[".pdf", ".docx", ".txt", ".md"]
).load_data()
```

### LlamaHub åŠ è½½å™¨

```python
# å®‰è£…ç‰¹å®šåŠ è½½å™¨
# pip install llama-index-readers-web

from llama_index.readers.web import SimpleWebPageReader

# ç½‘é¡µåŠ è½½
loader = SimpleWebPageReader()
documents = loader.load_data(urls=["https://example.com"])

# æ›´å¤šåŠ è½½å™¨ï¼šhttps://llamahub.ai/
# - NotionPageReader
# - SlackReader
# - GoogleDocsReader
# - DatabaseReader
```

### è‡ªå®šä¹‰åŠ è½½å™¨

```python
from llama_index.core.readers.base import BaseReader
from llama_index.core import Document

class CustomReader(BaseReader):
    def load_data(self, file_path: str) -> list[Document]:
        with open(file_path, 'r') as f:
            content = f.read()
        
        return [Document(
            text=content,
            metadata={"source": file_path}
        )]
```

## é«˜çº§ RAG æŠ€æœ¯

### 1. æŸ¥è¯¢è½¬æ¢

```python
from llama_index.core.indices.query.query_transform import HyDEQueryTransform
from llama_index.core.query_engine import TransformQueryEngine

# HyDE: å‡è®¾æ–‡æ¡£åµŒå…¥
hyde = HyDEQueryTransform(include_original=True)
query_engine = index.as_query_engine()
hyde_query_engine = TransformQueryEngine(query_engine, hyde)

response = hyde_query_engine.query("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
```

### 2. é‡æ’åº

```python
from llama_index.core.postprocessor import SentenceTransformerRerank

# ä½¿ç”¨é‡æ’åºæ¨¡å‹
reranker = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-6-v2",
    top_n=3
)

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[reranker]
)
```

### 3. æ··åˆæ£€ç´¢

```python
from llama_index.core.retrievers import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever

# BM25 æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_defaults(
    nodes=nodes,
    similarity_top_k=5
)

# å‘é‡æ£€ç´¢å™¨
vector_retriever = index.as_retriever(similarity_top_k=5)

# èåˆæ£€ç´¢å™¨
fusion_retriever = QueryFusionRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    similarity_top_k=5,
    num_queries=1,
    mode="reciprocal_rerank"  # å€’æ•°æ’åèåˆ
)
```

### 4. å­é—®é¢˜æŸ¥è¯¢

```python
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine import SubQuestionQueryEngine

# åˆ›å»ºå¤šä¸ªæŸ¥è¯¢å¼•æ“å·¥å…·
tools = [
    QueryEngineTool.from_defaults(
        query_engine=index1.as_query_engine(),
        name="financial_data",
        description="è´¢åŠ¡æ•°æ®æŸ¥è¯¢"
    ),
    QueryEngineTool.from_defaults(
        query_engine=index2.as_query_engine(),
        name="product_info",
        description="äº§å“ä¿¡æ¯æŸ¥è¯¢"
    )
]

# å­é—®é¢˜æŸ¥è¯¢å¼•æ“
query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=tools
)

# å¤æ‚é—®é¢˜ä¼šè¢«åˆ†è§£ä¸ºå­é—®é¢˜
response = query_engine.query("æ¯”è¾ƒäº§å“Aå’Œäº§å“Bçš„è´¢åŠ¡è¡¨ç°")
```

### 5. é€’å½’æ£€ç´¢

```python
from llama_index.core.retrievers import RecursiveRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

# æ–‡æ¡£ -> æ‘˜è¦ -> è¯¦ç»†å†…å®¹çš„é€’å½’æ£€ç´¢
recursive_retriever = RecursiveRetriever(
    "vector",
    retriever_dict={"vector": vector_retriever},
    node_dict=all_nodes_dict,
    verbose=True
)

query_engine = RetrieverQueryEngine.from_args(recursive_retriever)
```

## Chat Engine

```python
from llama_index.core.memory import ChatMemoryBuffer

# åˆ›å»ºèŠå¤©å¼•æ“
memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

chat_engine = index.as_chat_engine(
    chat_mode="context",  # context, condense_question, react
    memory=memory,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"
)

# å¤šè½®å¯¹è¯
response = chat_engine.chat("ä½ å¥½")
print(response)

response = chat_engine.chat("ç»§ç»­ä¸Šé¢çš„è¯é¢˜")
print(response)

# é‡ç½®å¯¹è¯
chat_engine.reset()
```

## Agent

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool

# å®šä¹‰å·¥å…·
def multiply(a: int, b: int) -> int:
    """ä¸¤æ•°ç›¸ä¹˜"""
    return a * b

def add(a: int, b: int) -> int:
    """ä¸¤æ•°ç›¸åŠ """
    return a + b

multiply_tool = FunctionTool.from_defaults(fn=multiply)
add_tool = FunctionTool.from_defaults(fn=add)

# åˆ›å»º Agent
agent = ReActAgent.from_tools(
    tools=[multiply_tool, add_tool],
    llm=Settings.llm,
    verbose=True
)

response = agent.chat("è®¡ç®— (3 + 5) * 2")
print(response)
```

## å‘é‡æ•°æ®åº“é›†æˆ

### Chroma

```python
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import VectorStoreIndex, StorageContext

# åˆ›å»º Chroma å®¢æˆ·ç«¯
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("documents")

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)
```

### Milvus

```python
from llama_index.vector_stores.milvus import MilvusVectorStore

vector_store = MilvusVectorStore(
    uri="http://localhost:19530",
    collection_name="documents",
    dim=1536
)

storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
```

## è¯„ä¼°

```python
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator
)

# å¿ å®åº¦è¯„ä¼°
faithfulness_evaluator = FaithfulnessEvaluator()
result = faithfulness_evaluator.evaluate_response(
    query="é—®é¢˜",
    response=response
)
print(f"å¿ å®åº¦: {result.passing}")

# ç›¸å…³æ€§è¯„ä¼°
relevancy_evaluator = RelevancyEvaluator()
result = relevancy_evaluator.evaluate_response(
    query="é—®é¢˜",
    response=response
)
print(f"ç›¸å…³æ€§: {result.passing}")
```

## æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„ç´¢å¼•**ï¼šç®€å•åœºæ™¯ç”¨ VectorStoreIndexï¼Œå¤æ‚åœºæ™¯è€ƒè™‘ç»„åˆç´¢å¼•
2. **ä¼˜åŒ–åˆ‡åˆ†ç­–ç•¥**ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´ chunk_size å’Œ overlap
3. **ä½¿ç”¨é‡æ’åº**ï¼šæé«˜æ£€ç´¢ç²¾åº¦
4. **æ·»åŠ å…ƒæ•°æ®**ï¼šä¾¿äºè¿‡æ»¤å’Œè¿½æº¯
5. **æŒä¹…åŒ–ç´¢å¼•**ï¼šé¿å…é‡å¤æ„å»º

## å»¶ä¼¸é˜…è¯»

- [LlamaIndex å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/)
- [LlamaHub æ•°æ®åŠ è½½å™¨](https://llamahub.ai/)
- [LlamaIndex GitHub](https://github.com/run-llama/llama_index)
