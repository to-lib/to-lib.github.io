---
sidebar_position: 37
title: ğŸ¯ RLHF ä¸ DPO
---

# RLHF ä¸ DPO

RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰å’Œ DPOï¼ˆDirect Preference Optimizationï¼‰æ˜¯è®© LLM ä¸äººç±»åå¥½å¯¹é½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦å¯¹é½ï¼Ÿ

```
é¢„è®­ç»ƒæ¨¡å‹ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼ˆå¯èƒ½ç”Ÿæˆæœ‰å®³/æ— ç”¨å†…å®¹ï¼‰
     â”‚
     â–¼
å¯¹é½åæ¨¡å‹ï¼šç”Ÿæˆæœ‰å¸®åŠ©ã€è¯šå®ã€æ— å®³çš„å†…å®¹
```

## RLHF æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLHF ä¸‰é˜¶æ®µ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  é˜¶æ®µ1: SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰                                 â”‚
â”‚  â””â”€> ç”¨é«˜è´¨é‡æ•°æ®å¾®è°ƒåŸºç¡€æ¨¡å‹                           â”‚
â”‚                                                         â”‚
â”‚  é˜¶æ®µ2: è®­ç»ƒå¥–åŠ±æ¨¡å‹                                    â”‚
â”‚  â””â”€> äººç±»æ ‡æ³¨åå¥½æ•°æ®ï¼Œè®­ç»ƒ RM                          â”‚
â”‚                                                         â”‚
â”‚  é˜¶æ®µ3: PPO å¼ºåŒ–å­¦ä¹                                     â”‚
â”‚  â””â”€> ç”¨ RM æŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–                                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## åå¥½æ•°æ®æ ¼å¼

```json
{
  "prompt": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
  "chosen": "æ˜¥é£æ‹‚é¢æš–æ„æµ“ï¼Œ\næ¡ƒèŠ±æœµæœµæ˜ æ—¥çº¢ã€‚\nç‡•å­å½’æ¥å¯»æ—§å·¢ï¼Œ\næŸ³çµ®é£˜é£˜èˆä¸œé£ã€‚",
  "rejected": "æ˜¥å¤©æ¥äº†ï¼ŒèŠ±å¼€äº†ï¼Œå¾ˆæ¼‚äº®ã€‚"
}
```

## å¥–åŠ±æ¨¡å‹è®­ç»ƒ

```python
import torch
import torch.nn as nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer


class RewardModel(nn.Module):
    """å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self, model_name: str):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=1
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits

def reward_loss(chosen_rewards, rejected_rewards):
    """å¥–åŠ±æ¨¡å‹æŸå¤±å‡½æ•°"""
    # å¸Œæœ› chosen çš„å¥–åŠ±é«˜äº rejected
    return -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()

# è®­ç»ƒå¾ªç¯
def train_reward_model(model, dataloader, optimizer, epochs=3):
    model.train()
    
    for epoch in range(epochs):
        for batch in dataloader:
            # è®¡ç®— chosen å’Œ rejected çš„å¥–åŠ±
            chosen_rewards = model(
                batch["chosen_input_ids"],
                batch["chosen_attention_mask"]
            )
            rejected_rewards = model(
                batch["rejected_input_ids"],
                batch["rejected_attention_mask"]
            )
            
            loss = reward_loss(chosen_rewards, rejected_rewards)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

## DPOï¼ˆDirect Preference Optimizationï¼‰

DPO ç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥ï¼Œæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚

### DPO åŸç†

```
RLHF: åå¥½æ•°æ® â†’ å¥–åŠ±æ¨¡å‹ â†’ PPO è®­ç»ƒ â†’ å¯¹é½æ¨¡å‹
DPO:  åå¥½æ•°æ® â†’ ç›´æ¥ä¼˜åŒ– â†’ å¯¹é½æ¨¡å‹ï¼ˆæ›´ç®€å•ï¼ï¼‰
```

### DPO æŸå¤±å‡½æ•°

```python
import torch.nn.functional as F

def dpo_loss(
    policy_chosen_logps: torch.Tensor,
    policy_rejected_logps: torch.Tensor,
    reference_chosen_logps: torch.Tensor,
    reference_rejected_logps: torch.Tensor,
    beta: float = 0.1
) -> torch.Tensor:
    """DPO æŸå¤±å‡½æ•°"""
    # è®¡ç®— log ratio
    chosen_logratios = policy_chosen_logps - reference_chosen_logps
    rejected_logratios = policy_rejected_logps - reference_rejected_logps
    
    # DPO æŸå¤±
    logits = beta * (chosen_logratios - rejected_logratios)
    loss = -F.logsigmoid(logits).mean()
    
    return loss
```

### ä½¿ç”¨ TRL åº“è®­ç»ƒ DPO

```bash
pip install trl
```

```python
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B")
ref_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B")

# åŠ è½½åå¥½æ•°æ®
dataset = load_dataset("json", data_files="preferences.jsonl")

# DPO é…ç½®
training_args = DPOConfig(
    output_dir="./dpo_model",
    beta=0.1,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    learning_rate=5e-7,
    logging_steps=10
)

# è®­ç»ƒ
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer
)

trainer.train()
```

## ORPOï¼ˆæ— éœ€å‚è€ƒæ¨¡å‹ï¼‰

```python
from trl import ORPOTrainer, ORPOConfig

# ORPO ä¸éœ€è¦å‚è€ƒæ¨¡å‹
training_args = ORPOConfig(
    output_dir="./orpo_model",
    beta=0.1,
    per_device_train_batch_size=4,
    num_train_epochs=3
)

trainer = ORPOTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer
)

trainer.train()
```

## åå¥½æ•°æ®æ”¶é›†

### äººå·¥æ ‡æ³¨

```python
def collect_preferences(prompts: list, model) -> list:
    """æ”¶é›†äººå·¥åå¥½æ ‡æ³¨"""
    preferences = []
    
    for prompt in prompts:
        # ç”Ÿæˆå¤šä¸ªå€™é€‰å›å¤
        responses = []
        for _ in range(3):
            response = model.generate(prompt, temperature=0.8)
            responses.append(response)
        
        # äººå·¥é€‰æ‹©æœ€ä½³å’Œæœ€å·®
        print(f"Prompt: {prompt}")
        for i, r in enumerate(responses):
            print(f"{i}: {r}")
        
        chosen_idx = int(input("Best response: "))
        rejected_idx = int(input("Worst response: "))
        
        preferences.append({
            "prompt": prompt,
            "chosen": responses[chosen_idx],
            "rejected": responses[rejected_idx]
        })
    
    return preferences
```

### AI è¾…åŠ©æ ‡æ³¨

```python
from openai import OpenAI

client = OpenAI()

def ai_preference_labeling(prompt: str, response_a: str, response_b: str) -> dict:
    """ä½¿ç”¨ GPT-4 è¿›è¡Œåå¥½æ ‡æ³¨"""
    judge_prompt = f"""
æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå›å¤ï¼Œé€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªã€‚

é—®é¢˜ï¼š{prompt}

å›å¤ Aï¼š{response_a}

å›å¤ Bï¼š{response_b}

å“ªä¸ªå›å¤æ›´å¥½ï¼Ÿåªå›ç­” "A" æˆ– "B"ã€‚
"""
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": judge_prompt}],
        max_tokens=1
    )
    
    choice = response.choices[0].message.content.strip()
    
    if choice == "A":
        return {"prompt": prompt, "chosen": response_a, "rejected": response_b}
    else:
        return {"prompt": prompt, "chosen": response_b, "rejected": response_a}
```


## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å¤æ‚åº¦ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|---------|
| RLHF | é«˜ | æœ€å¥½ | å¤§è§„æ¨¡å¯¹é½ |
| DPO | ä¸­ | å¾ˆå¥½ | æ¨èé¦–é€‰ |
| ORPO | ä½ | å¥½ | èµ„æºæœ‰é™ |
| KTO | ä½ | å¥½ | åªæœ‰æ­£/è´Ÿæ ·æœ¬ |

## æœ€ä½³å®è·µ

1. **æ•°æ®è´¨é‡ä¼˜å…ˆ**ï¼šåå¥½æ•°æ®è´¨é‡å†³å®šå¯¹é½æ•ˆæœ
2. **ä» DPO å¼€å§‹**ï¼šæ¯” RLHF ç®€å•ï¼Œæ•ˆæœæ¥è¿‘
3. **å¤šæ ·åŒ–æ•°æ®**ï¼šè¦†ç›–å„ç§åœºæ™¯å’Œè¾¹ç•Œæƒ…å†µ
4. **è¿­ä»£ä¼˜åŒ–**ï¼šå¤šè½®å¯¹é½é€æ­¥æå‡
5. **è¯„ä¼°éªŒè¯**ï¼šç”¨äººå·¥è¯„ä¼°éªŒè¯å¯¹é½æ•ˆæœ

## å»¶ä¼¸é˜…è¯»

- [InstructGPT Paper](https://arxiv.org/abs/2203.02155)
- [DPO Paper](https://arxiv.org/abs/2305.18290)
- [TRL Library](https://github.com/huggingface/trl)