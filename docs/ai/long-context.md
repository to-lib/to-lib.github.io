---
sidebar_position: 23
title: ğŸ“œ é•¿ä¸Šä¸‹æ–‡å¤„ç†
---

# é•¿ä¸Šä¸‹æ–‡å¤„ç†

å¤„ç†è¶…é•¿æ–‡æ¡£æ˜¯ LLM åº”ç”¨çš„å¸¸è§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»å„ç§é•¿ä¸Šä¸‹æ–‡å¤„ç†ç­–ç•¥ã€‚

## æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦

| æ¨¡å‹              | ä¸Šä¸‹æ–‡é•¿åº¦ | çº¦ç­‰äº         |
| ----------------- | ---------- | -------------- |
| GPT-4o            | 128K       | ~300 é¡µæ–‡æ¡£    |
| GPT-4o-mini       | 128K       | ~300 é¡µæ–‡æ¡£    |
| Claude 3.5 Sonnet | 200K       | ~500 é¡µæ–‡æ¡£    |
| Gemini 1.5 Pro    | 2M         | ~5000 é¡µæ–‡æ¡£   |
| Qwen2.5           | 128K       | ~300 é¡µæ–‡æ¡£    |

## å¤„ç†ç­–ç•¥æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   é•¿æ–‡æ¡£å¤„ç†ç­–ç•¥                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  æ–‡æ¡£é•¿åº¦ < ä¸Šä¸‹æ–‡çª—å£                                   â”‚
â”‚  â””â”€> ç›´æ¥å¤„ç†                                           â”‚
â”‚                                                         â”‚
â”‚  æ–‡æ¡£é•¿åº¦ > ä¸Šä¸‹æ–‡çª—å£                                   â”‚
â”‚  â”œâ”€> ç­–ç•¥1: åˆ‡åˆ† + RAG æ£€ç´¢                             â”‚
â”‚  â”œâ”€> ç­–ç•¥2: Map-Reduce æ‘˜è¦                             â”‚
â”‚  â”œâ”€> ç­–ç•¥3: Refine è¿­ä»£ç²¾ç‚¼                             â”‚
â”‚  â””â”€> ç­–ç•¥4: å±‚æ¬¡åŒ–æ‘˜è¦                                  â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ç­–ç•¥ 1: ç›´æ¥å¤„ç†ï¼ˆçŸ­æ–‡æ¡£ï¼‰

```python
from openai import OpenAI

client = OpenAI()

def process_short_document(document: str, question: str) -> str:
    """ç›´æ¥å¤„ç†çŸ­æ–‡æ¡£"""
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚"
            },
            {
                "role": "user",
                "content": f"æ–‡æ¡£å†…å®¹ï¼š\n\n{document}\n\né—®é¢˜ï¼š{question}"
            }
        ]
    )
    return response.choices[0].message.content
```

## ç­–ç•¥ 2: Map-Reduce

å°†é•¿æ–‡æ¡£åˆ‡åˆ†ï¼Œåˆ†åˆ«å¤„ç†ååˆå¹¶ç»“æœã€‚é€‚åˆæ‘˜è¦ã€ä¿¡æ¯æå–ç­‰ä»»åŠ¡ã€‚

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from concurrent.futures import ThreadPoolExecutor
import tiktoken

class MapReduceProcessor:
    """Map-Reduce æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int = 4000, chunk_overlap: int = 200):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=self._count_tokens
        )
        self.client = OpenAI()
    
    def _count_tokens(self, text: str) -> int:
        enc = tiktoken.encoding_for_model("gpt-4o")
        return len(enc.encode(text))
    
    def _map_chunk(self, chunk: str, task: str) -> str:
        """Map é˜¶æ®µï¼šå¤„ç†å•ä¸ªå—"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",  # Map é˜¶æ®µç”¨å°æ¨¡å‹
            messages=[
                {"role": "system", "content": f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬æ‰§è¡Œä»»åŠ¡ï¼š{task}"},
                {"role": "user", "content": chunk}
            ],
            temperature=0
        )
        return response.choices[0].message.content
    
    def _reduce(self, results: list[str], task: str) -> str:
        """Reduce é˜¶æ®µï¼šåˆå¹¶ç»“æœ"""
        combined = "\n\n---\n\n".join(results)
        
        response = self.client.chat.completions.create(
            model="gpt-4o",  # Reduce é˜¶æ®µç”¨å¤§æ¨¡å‹
            messages=[
                {
                    "role": "system",
                    "content": f"ä»¥ä¸‹æ˜¯å¯¹æ–‡æ¡£å„éƒ¨åˆ†æ‰§è¡Œ'{task}'çš„ç»“æœã€‚è¯·ç»¼åˆè¿™äº›ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚"
                },
                {"role": "user", "content": combined}
            ],
            temperature=0
        )
        return response.choices[0].message.content
    
    def process(self, document: str, task: str, parallel: bool = True) -> str:
        """å¤„ç†é•¿æ–‡æ¡£"""
        # åˆ‡åˆ†æ–‡æ¡£
        chunks = self.splitter.split_text(document)
        print(f"æ–‡æ¡£è¢«åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
        
        # Map é˜¶æ®µ
        if parallel:
            with ThreadPoolExecutor(max_workers=5) as executor:
                results = list(executor.map(
                    lambda c: self._map_chunk(c, task),
                    chunks
                ))
        else:
            results = [self._map_chunk(c, task) for c in chunks]
        
        # Reduce é˜¶æ®µ
        if len(results) == 1:
            return results[0]
        
        return self._reduce(results, task)

# ä½¿ç”¨ç¤ºä¾‹
processor = MapReduceProcessor()
summary = processor.process(
    long_document,
    task="æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆæ‘˜è¦"
)
```

## ç­–ç•¥ 3: Refineï¼ˆè¿­ä»£ç²¾ç‚¼ï¼‰

é€å—å¤„ç†ï¼Œæ¯æ¬¡åŸºäºå‰ä¸€æ¬¡çš„ç»“æœè¿›è¡Œç²¾ç‚¼ã€‚é€‚åˆéœ€è¦è¿è´¯æ€§çš„ä»»åŠ¡ã€‚

```python
class RefineProcessor:
    """Refine è¿­ä»£ç²¾ç‚¼å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int = 4000):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=200
        )
        self.client = OpenAI()
    
    def process(self, document: str, task: str) -> str:
        """è¿­ä»£ç²¾ç‚¼å¤„ç†"""
        chunks = self.splitter.split_text(document)
        
        # å¤„ç†ç¬¬ä¸€ä¸ªå—
        current_result = self._initial_process(chunks[0], task)
        
        # è¿­ä»£ç²¾ç‚¼åç»­å—
        for i, chunk in enumerate(chunks[1:], 2):
            print(f"å¤„ç†ç¬¬ {i}/{len(chunks)} å—...")
            current_result = self._refine(current_result, chunk, task)
        
        return current_result
    
    def _initial_process(self, chunk: str, task: str) -> str:
        """å¤„ç†ç¬¬ä¸€ä¸ªå—"""
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬æ‰§è¡Œä»»åŠ¡ï¼š{task}"},
                {"role": "user", "content": chunk}
            ]
        )
        return response.choices[0].message.content
    
    def _refine(self, current_result: str, new_chunk: str, task: str) -> str:
        """åŸºäºæ–°å†…å®¹ç²¾ç‚¼ç»“æœ"""
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": f"""ä½ ä¹‹å‰å¯¹æ–‡æ¡£éƒ¨åˆ†å†…å®¹æ‰§è¡Œäº†ä»»åŠ¡ï¼š{task}
                    
å½“å‰ç»“æœï¼š
{current_result}

ç°åœ¨æœ‰æ–°çš„æ–‡æ¡£å†…å®¹ã€‚è¯·æ ¹æ®æ–°å†…å®¹æ›´æ–°å’Œå®Œå–„ä½ çš„ç»“æœã€‚
å¦‚æœæ–°å†…å®¹åŒ…å«é‡è¦ä¿¡æ¯ï¼Œè¯·æ·»åŠ åˆ°ç»“æœä¸­ã€‚
å¦‚æœæ–°å†…å®¹ä¸ç°æœ‰ç»“æœçŸ›ç›¾ï¼Œè¯·è¿›è¡Œä¿®æ­£ã€‚"""
                },
                {"role": "user", "content": f"æ–°å†…å®¹ï¼š\n{new_chunk}"}
            ]
        )
        return response.choices[0].message.content

# ä½¿ç”¨
refiner = RefineProcessor()
result = refiner.process(long_document, "ç”Ÿæˆè¯¦ç»†æ‘˜è¦")
```

## ç­–ç•¥ 4: å±‚æ¬¡åŒ–æ‘˜è¦

æ„å»ºæ‘˜è¦æ ‘ï¼Œé€‚åˆè¶…é•¿æ–‡æ¡£ã€‚

```python
class HierarchicalSummarizer:
    """å±‚æ¬¡åŒ–æ‘˜è¦"""
    
    def __init__(self, leaf_size: int = 2000, branch_factor: int = 4):
        self.leaf_size = leaf_size
        self.branch_factor = branch_factor
        self.client = OpenAI()
    
    def summarize(self, document: str) -> dict:
        """ç”Ÿæˆå±‚æ¬¡åŒ–æ‘˜è¦"""
        # åˆ‡åˆ†ä¸ºå¶å­èŠ‚ç‚¹
        splitter = RecursiveCharacterTextSplitter(chunk_size=self.leaf_size)
        chunks = splitter.split_text(document)
        
        # æ„å»ºæ‘˜è¦æ ‘
        tree = self._build_tree(chunks)
        
        return {
            "final_summary": tree["summary"],
            "tree": tree
        }
    
    def _build_tree(self, chunks: list[str], level: int = 0) -> dict:
        """é€’å½’æ„å»ºæ‘˜è¦æ ‘"""
        if len(chunks) == 1:
            summary = self._summarize_chunk(chunks[0])
            return {"level": level, "summary": summary, "children": None}
        
        # åˆ†ç»„
        groups = [
            chunks[i:i + self.branch_factor]
            for i in range(0, len(chunks), self.branch_factor)
        ]
        
        # é€’å½’å¤„ç†æ¯ç»„
        children = []
        child_summaries = []
        
        for group in groups:
            if len(group) == 1:
                child_summary = self._summarize_chunk(group[0])
            else:
                # å…ˆåˆå¹¶ç»„å†…å†…å®¹å†æ‘˜è¦
                combined = "\n\n".join(group)
                child_summary = self._summarize_chunk(combined)
            
            children.append({
                "level": level + 1,
                "summary": child_summary,
                "original_chunks": group
            })
            child_summaries.append(child_summary)
        
        # å¦‚æœå­æ‘˜è¦æ•°é‡ä»ç„¶å¾ˆå¤šï¼Œç»§ç»­é€’å½’
        if len(child_summaries) > self.branch_factor:
            return self._build_tree(child_summaries, level)
        
        # åˆå¹¶å­æ‘˜è¦
        final_summary = self._merge_summaries(child_summaries)
        
        return {
            "level": level,
            "summary": final_summary,
            "children": children
        }
    
    def _summarize_chunk(self, text: str) -> str:
        """æ‘˜è¦å•ä¸ªå—"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "è¯·ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚"},
                {"role": "user", "content": text}
            ],
            max_tokens=500
        )
        return response.choices[0].message.content
    
    def _merge_summaries(self, summaries: list[str]) -> str:
        """åˆå¹¶å¤šä¸ªæ‘˜è¦"""
        combined = "\n\n".join([f"éƒ¨åˆ† {i+1}ï¼š{s}" for i, s in enumerate(summaries)])
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "è¯·å°†ä»¥ä¸‹å¤šä¸ªæ‘˜è¦åˆå¹¶ä¸ºä¸€ä¸ªè¿è´¯ã€å…¨é¢çš„æ‘˜è¦ã€‚"
                },
                {"role": "user", "content": combined}
            ]
        )
        return response.choices[0].message.content
```

## ç­–ç•¥ 5: RAG æ£€ç´¢

å¯¹äºé—®ç­”åœºæ™¯ï¼Œåªæ£€ç´¢ç›¸å…³éƒ¨åˆ†ã€‚

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

class LongDocumentQA:
    """é•¿æ–‡æ¡£é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, document: str):
        # åˆ‡åˆ†æ–‡æ¡£
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = splitter.split_text(document)
        
        # åˆ›å»ºå‘é‡ç´¢å¼•
        self.vectorstore = Chroma.from_texts(
            texts=chunks,
            embedding=OpenAIEmbeddings(model="text-embedding-3-small")
        )
        
        self.client = OpenAI()
    
    def query(self, question: str, k: int = 5) -> str:
        """æŸ¥è¯¢"""
        # æ£€ç´¢ç›¸å…³å—
        docs = self.vectorstore.similarity_search(question, k=k)
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ç”Ÿæˆå›ç­”
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚"
                },
                {
                    "role": "user",
                    "content": f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"
                }
            ]
        )
        
        return response.choices[0].message.content
```

## ç­–ç•¥ 6: æ»‘åŠ¨çª—å£

é€‚åˆéœ€è¦å¤„ç†æ•´ä¸ªæ–‡æ¡£ä½†ä¸Šä¸‹æ–‡æœ‰é™çš„åœºæ™¯ã€‚

```python
class SlidingWindowProcessor:
    """æ»‘åŠ¨çª—å£å¤„ç†å™¨"""
    
    def __init__(self, window_size: int = 4000, stride: int = 2000):
        self.window_size = window_size
        self.stride = stride
        self.client = OpenAI()
    
    def process(self, document: str, task: str) -> list[dict]:
        """æ»‘åŠ¨çª—å£å¤„ç†"""
        results = []
        
        # æŒ‰å­—ç¬¦æ»‘åŠ¨ï¼ˆå®é™…åº”ç”¨ä¸­åº”æŒ‰ tokenï¼‰
        for i in range(0, len(document), self.stride):
            window = document[i:i + self.window_size]
            
            if len(window) < 100:  # è·³è¿‡å¤ªçŸ­çš„çª—å£
                continue
            
            result = self._process_window(window, task, i)
            results.append({
                "start": i,
                "end": i + len(window),
                "result": result
            })
        
        return results
    
    def _process_window(self, window: str, task: str, position: int) -> str:
        """å¤„ç†å•ä¸ªçª—å£"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"è¿™æ˜¯æ–‡æ¡£çš„ä¸€éƒ¨åˆ†ï¼ˆä½ç½®ï¼š{position}ï¼‰ã€‚è¯·æ‰§è¡Œä»»åŠ¡ï¼š{task}"
                },
                {"role": "user", "content": window}
            ]
        )
        return response.choices[0].message.content
```

## LangChain é›†æˆ

```python
from langchain.chains.summarize import load_summarize_chain
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

llm = ChatOpenAI(model="gpt-4o", temperature=0)

# Map-Reduce æ‘˜è¦
def langchain_map_reduce(text: str) -> str:
    splitter = RecursiveCharacterTextSplitter(chunk_size=4000)
    docs = [Document(page_content=t) for t in splitter.split_text(text)]
    
    chain = load_summarize_chain(llm, chain_type="map_reduce")
    return chain.run(docs)

# Refine æ‘˜è¦
def langchain_refine(text: str) -> str:
    splitter = RecursiveCharacterTextSplitter(chunk_size=4000)
    docs = [Document(page_content=t) for t in splitter.split_text(text)]
    
    chain = load_summarize_chain(llm, chain_type="refine")
    return chain.run(docs)
```

## ç­–ç•¥é€‰æ‹©æŒ‡å—

| åœºæ™¯           | æ¨èç­–ç•¥       | åŸå›                      |
| -------------- | -------------- | ------------------------ |
| æ–‡æ¡£æ‘˜è¦       | Map-Reduce     | å¹¶è¡Œå¤„ç†ï¼Œé€Ÿåº¦å¿«         |
| è¯¦ç»†åˆ†æ       | Refine         | ä¿æŒè¿è´¯æ€§               |
| è¶…é•¿æ–‡æ¡£æ‘˜è¦   | å±‚æ¬¡åŒ–æ‘˜è¦     | å¤„ç†ä»»æ„é•¿åº¦             |
| é—®ç­”           | RAG            | åªæ£€ç´¢ç›¸å…³éƒ¨åˆ†           |
| ä¿¡æ¯æå–       | æ»‘åŠ¨çª—å£       | ä¸é—æ¼ä»»ä½•éƒ¨åˆ†           |
| æ–‡æ¡£ < ä¸Šä¸‹æ–‡  | ç›´æ¥å¤„ç†       | æœ€ç®€å•                   |

## æœ€ä½³å®è·µ

1. **å…ˆè¯„ä¼°æ–‡æ¡£é•¿åº¦**ï¼šé€‰æ‹©åˆé€‚çš„ç­–ç•¥
2. **åˆç†è®¾ç½® chunk_size**ï¼šå¤ªå°ä¸¢å¤±ä¸Šä¸‹æ–‡ï¼Œå¤ªå¤§è¶…å‡ºé™åˆ¶
3. **ä¿ç•™ overlap**ï¼šé¿å…ä¿¡æ¯åœ¨è¾¹ç•Œä¸¢å¤±
4. **å¹¶è¡Œå¤„ç†**ï¼šMap-Reduce å¯ä»¥å¹¶è¡ŒåŠ é€Ÿ
5. **ä½¿ç”¨å°æ¨¡å‹å¤„ç†ä¸­é—´æ­¥éª¤**ï¼šé™ä½æˆæœ¬

## å»¶ä¼¸é˜…è¯»

- [LangChain Summarization](https://python.langchain.com/docs/tutorials/summarization/)
- [LlamaIndex Document Summary](https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/)
