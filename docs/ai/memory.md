---
sidebar_position: 24
title: ğŸ§  å¯¹è¯è®°å¿†ç®¡ç†
---

# å¯¹è¯è®°å¿†ç®¡ç†

å¯¹è¯è®°å¿†æ˜¯è®© AI åŠ©æ‰‹èƒ½å¤Ÿè®°ä½ä¸Šä¸‹æ–‡ã€ä¿æŒè¿è´¯å¯¹è¯çš„å…³é”®æŠ€æœ¯ã€‚æœ¬æ–‡ä»‹ç»å„ç§è®°å¿†ç®¡ç†ç­–ç•¥ã€‚

## è®°å¿†ç±»å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     è®°å¿†ç³»ç»Ÿ                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  çŸ­æœŸè®°å¿† (Short-term)                                  â”‚
â”‚  â””â”€> å½“å‰å¯¹è¯çš„æ¶ˆæ¯å†å²                                 â”‚
â”‚                                                         â”‚
â”‚  é•¿æœŸè®°å¿† (Long-term)                                   â”‚
â”‚  â”œâ”€> å‘é‡è®°å¿†ï¼šè¯­ä¹‰æ£€ç´¢å†å²ä¿¡æ¯                         â”‚
â”‚  â”œâ”€> æ‘˜è¦è®°å¿†ï¼šå‹ç¼©å†å²å¯¹è¯                             â”‚
â”‚  â””â”€> å®ä½“è®°å¿†ï¼šè®°ä½å…³é”®å®ä½“ä¿¡æ¯                         â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## çŸ­æœŸè®°å¿†ï¼šæ¶ˆæ¯å†å²

### åŸºç¡€å®ç°

```python
from openai import OpenAI
from typing import List, Dict

class ConversationMemory:
    """åŸºç¡€å¯¹è¯è®°å¿†"""
    
    def __init__(self, system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"):
        self.client = OpenAI()
        self.system_prompt = system_prompt
        self.messages: List[Dict] = []
    
    def add_user_message(self, content: str):
        self.messages.append({"role": "user", "content": content})
    
    def add_assistant_message(self, content: str):
        self.messages.append({"role": "assistant", "content": content})
    
    def get_messages(self) -> List[Dict]:
        return [{"role": "system", "content": self.system_prompt}] + self.messages
    
    def chat(self, user_input: str) -> str:
        self.add_user_message(user_input)
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=self.get_messages()
        )
        
        assistant_message = response.choices[0].message.content
        self.add_assistant_message(assistant_message)
        
        return assistant_message
    
    def clear(self):
        self.messages = []

# ä½¿ç”¨
memory = ConversationMemory()
print(memory.chat("æˆ‘å«å¼ ä¸‰"))
print(memory.chat("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"))  # èƒ½è®°ä½åå­—
```

### çª—å£è®°å¿†ï¼ˆé™åˆ¶æ¶ˆæ¯æ•°é‡ï¼‰

```python
class WindowMemory:
    """æ»‘åŠ¨çª—å£è®°å¿†"""
    
    def __init__(self, window_size: int = 10):
        self.window_size = window_size
        self.messages: List[Dict] = []
        self.client = OpenAI()
    
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        # ä¿æŒçª—å£å¤§å°
        if len(self.messages) > self.window_size * 2:  # user + assistant
            self.messages = self.messages[-self.window_size * 2:]
    
    def chat(self, user_input: str) -> str:
        self.add_message("user", user_input)
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚"},
                *self.messages
            ]
        )
        
        assistant_message = response.choices[0].message.content
        self.add_message("assistant", assistant_message)
        
        return assistant_message
```

### Token é™åˆ¶è®°å¿†

```python
import tiktoken

class TokenLimitMemory:
    """åŸºäº Token é™åˆ¶çš„è®°å¿†"""
    
    def __init__(self, max_tokens: int = 4000, model: str = "gpt-4o"):
        self.max_tokens = max_tokens
        self.model = model
        self.messages: List[Dict] = []
        self.client = OpenAI()
        self.encoder = tiktoken.encoding_for_model(model)
    
    def _count_tokens(self, messages: List[Dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯çš„ token æ•°"""
        total = 0
        for msg in messages:
            total += len(self.encoder.encode(msg["content"])) + 4  # è§’è‰²æ ‡è®°
        return total
    
    def _trim_messages(self):
        """è£å‰ªæ¶ˆæ¯ä»¥ç¬¦åˆ token é™åˆ¶"""
        while self._count_tokens(self.messages) > self.max_tokens and len(self.messages) > 2:
            # ä¿ç•™æœ€æ–°çš„æ¶ˆæ¯ï¼Œåˆ é™¤æœ€æ—§çš„
            self.messages.pop(0)
    
    def chat(self, user_input: str) -> str:
        self.messages.append({"role": "user", "content": user_input})
        self._trim_messages()
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚"},
                *self.messages
            ]
        )
        
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        self._trim_messages()
        
        return assistant_message
```

## æ‘˜è¦è®°å¿†

å½“å¯¹è¯è¿‡é•¿æ—¶ï¼Œè‡ªåŠ¨ç”Ÿæˆæ‘˜è¦æ¥å‹ç¼©å†å²ã€‚

```python
class SummaryMemory:
    """æ‘˜è¦è®°å¿†"""
    
    def __init__(self, summary_threshold: int = 10):
        self.summary_threshold = summary_threshold
        self.messages: List[Dict] = []
        self.summary: str = ""
        self.client = OpenAI()
    
    def _generate_summary(self) -> str:
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        conversation = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in self.messages
        ])
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "è¯·ç®€æ´åœ°æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„è¦ç‚¹ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼ˆäººåã€æ•°å­—ã€å†³å®šç­‰ï¼‰ã€‚"
                },
                {"role": "user", "content": conversation}
            ],
            max_tokens=500
        )
        
        return response.choices[0].message.content
    
    def _maybe_summarize(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦"""
        if len(self.messages) >= self.summary_threshold:
            # ç”Ÿæˆæ‘˜è¦
            new_summary = self._generate_summary()
            
            # åˆå¹¶æ—§æ‘˜è¦
            if self.summary:
                self.summary = f"ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š{self.summary}\n\næœ€è¿‘çš„å¯¹è¯æ‘˜è¦ï¼š{new_summary}"
            else:
                self.summary = new_summary
            
            # æ¸…ç©ºæ¶ˆæ¯ï¼Œåªä¿ç•™æœ€è¿‘å‡ æ¡
            self.messages = self.messages[-4:]
    
    def get_context(self) -> str:
        """è·å–å®Œæ•´ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        if self.summary:
            context_parts.append(f"å¯¹è¯å†å²æ‘˜è¦ï¼š\n{self.summary}")
        
        if self.messages:
            recent = "\n".join([
                f"{msg['role']}: {msg['content']}"
                for msg in self.messages
            ])
            context_parts.append(f"æœ€è¿‘çš„å¯¹è¯ï¼š\n{recent}")
        
        return "\n\n".join(context_parts)
    
    def chat(self, user_input: str) -> str:
        self.messages.append({"role": "user", "content": user_input})
        self._maybe_summarize()
        
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚"
        if self.summary:
            system_prompt += f"\n\n{self.get_context()}"
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                *self.messages
            ]
        )
        
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        
        return assistant_message
```

## å‘é‡è®°å¿†

ä½¿ç”¨å‘é‡æ•°æ®åº“å­˜å‚¨å’Œæ£€ç´¢å†å²å¯¹è¯ã€‚

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from datetime import datetime
import uuid

class VectorMemory:
    """å‘é‡è®°å¿†"""
    
    def __init__(self, collection_name: str = "conversation_memory"):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory="./memory_db"
        )
        self.client = OpenAI()
        self.session_id = str(uuid.uuid4())
    
    def _store_exchange(self, user_input: str, assistant_response: str):
        """å­˜å‚¨å¯¹è¯äº¤æ¢"""
        exchange = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹: {assistant_response}"
        
        self.vectorstore.add_texts(
            texts=[exchange],
            metadatas=[{
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "user_input": user_input
            }]
        )
    
    def _retrieve_relevant(self, query: str, k: int = 3) -> List[str]:
        """æ£€ç´¢ç›¸å…³å†å²"""
        docs = self.vectorstore.similarity_search(query, k=k)
        return [doc.page_content for doc in docs]
    
    def chat(self, user_input: str) -> str:
        # æ£€ç´¢ç›¸å…³å†å²
        relevant_history = self._retrieve_relevant(user_input)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = ""
        if relevant_history:
            context = "ç›¸å…³çš„å†å²å¯¹è¯ï¼š\n" + "\n---\n".join(relevant_history)
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚{context}"
                },
                {"role": "user", "content": user_input}
            ]
        )
        
        assistant_message = response.choices[0].message.content
        
        # å­˜å‚¨è¿™æ¬¡å¯¹è¯
        self._store_exchange(user_input, assistant_message)
        
        return assistant_message
```

## å®ä½“è®°å¿†

è®°ä½å¯¹è¯ä¸­æåˆ°çš„å…³é”®å®ä½“ã€‚

```python
from typing import Dict, Any
import json

class EntityMemory:
    """å®ä½“è®°å¿†"""
    
    def __init__(self):
        self.entities: Dict[str, Any] = {}
        self.client = OpenAI()
    
    def _extract_entities(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ä»æ–‡æœ¬ä¸­æå–å…³é”®å®ä½“ä¿¡æ¯ï¼Œè¿”å› JSON æ ¼å¼ï¼š
{
  "äººç‰©": {"å§“å": "...", "ç‰¹å¾": "..."},
  "åœ°ç‚¹": ["..."],
  "æ—¶é—´": ["..."],
  "äº‹ä»¶": ["..."],
  "åå¥½": {"...": "..."}
}
åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¯¹åº”å­—æ®µä¸ºç©ºã€‚"""
                },
                {"role": "user", "content": text}
            ],
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response.choices[0].message.content)
        except:
            return {}
    
    def _update_entities(self, new_entities: Dict[str, Any]):
        """æ›´æ–°å®ä½“å­˜å‚¨"""
        for key, value in new_entities.items():
            if key not in self.entities:
                self.entities[key] = value
            elif isinstance(value, dict) and isinstance(self.entities[key], dict):
                self.entities[key].update(value)
            elif isinstance(value, list) and isinstance(self.entities[key], list):
                self.entities[key].extend(value)
                self.entities[key] = list(set(self.entities[key]))  # å»é‡
            else:
                self.entities[key] = value
    
    def get_entity_context(self) -> str:
        """è·å–å®ä½“ä¸Šä¸‹æ–‡"""
        if not self.entities:
            return ""
        return f"å·²çŸ¥ä¿¡æ¯ï¼š\n{json.dumps(self.entities, ensure_ascii=False, indent=2)}"
    
    def chat(self, user_input: str) -> str:
        # æå–å®ä½“
        new_entities = self._extract_entities(user_input)
        self._update_entities(new_entities)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        entity_context = self.get_entity_context()
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚\n\n{entity_context}"
                },
                {"role": "user", "content": user_input}
            ]
        )
        
        assistant_message = response.choices[0].message.content
        
        # ä»å›å¤ä¸­ä¹Ÿæå–å®ä½“
        response_entities = self._extract_entities(assistant_message)
        self._update_entities(response_entities)
        
        return assistant_message

# ä½¿ç”¨
memory = EntityMemory()
print(memory.chat("æˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´ 30 å²ï¼Œä½åœ¨åŒ—äº¬"))
print(memory.chat("æˆ‘å–œæ¬¢åƒå·èœ"))
print(memory.chat("ä½ è¿˜è®°å¾—æˆ‘çš„ä¿¡æ¯å—ï¼Ÿ"))
print(f"å­˜å‚¨çš„å®ä½“ï¼š{memory.entities}")
```

## ç»„åˆè®°å¿†

ç»“åˆå¤šç§è®°å¿†ç±»å‹ã€‚

```python
class CombinedMemory:
    """ç»„åˆè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.short_term = []  # çŸ­æœŸï¼šæœ€è¿‘æ¶ˆæ¯
        self.summary = ""     # ä¸­æœŸï¼šæ‘˜è¦
        self.entities = {}    # é•¿æœŸï¼šå®ä½“
        self.vector_memory = VectorMemory()  # é•¿æœŸï¼šå‘é‡æ£€ç´¢
        self.client = OpenAI()
        self.message_count = 0
    
    def _build_context(self, user_input: str) -> str:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        parts = []
        
        # å®ä½“ä¿¡æ¯
        if self.entities:
            parts.append(f"ç”¨æˆ·ä¿¡æ¯ï¼š{json.dumps(self.entities, ensure_ascii=False)}")
        
        # å†å²æ‘˜è¦
        if self.summary:
            parts.append(f"å¯¹è¯æ‘˜è¦ï¼š{self.summary}")
        
        # ç›¸å…³å†å²ï¼ˆå‘é‡æ£€ç´¢ï¼‰
        relevant = self.vector_memory._retrieve_relevant(user_input, k=2)
        if relevant:
            parts.append(f"ç›¸å…³å†å²ï¼š\n" + "\n".join(relevant))
        
        return "\n\n".join(parts)
    
    def chat(self, user_input: str) -> str:
        self.message_count += 1
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(user_input)
        
        # çŸ­æœŸè®°å¿†
        self.short_term.append({"role": "user", "content": user_input})
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚\n\n{context}"},
                *self.short_term[-10:]  # æœ€è¿‘ 10 æ¡
            ]
        )
        
        assistant_message = response.choices[0].message.content
        self.short_term.append({"role": "assistant", "content": assistant_message})
        
        # æ›´æ–°å„ç§è®°å¿†
        self._update_memories(user_input, assistant_message)
        
        return assistant_message
    
    def _update_memories(self, user_input: str, response: str):
        """æ›´æ–°å„ç§è®°å¿†"""
        # å­˜å‚¨åˆ°å‘é‡è®°å¿†
        self.vector_memory._store_exchange(user_input, response)
        
        # æ¯ 10 è½®æ›´æ–°æ‘˜è¦
        if self.message_count % 10 == 0:
            self._update_summary()
        
        # æå–å®ä½“
        self._extract_and_update_entities(user_input)
    
    def _update_summary(self):
        """æ›´æ–°æ‘˜è¦"""
        if len(self.short_term) < 4:
            return
        
        conversation = "\n".join([
            f"{m['role']}: {m['content']}" for m in self.short_term
        ])
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ç®€æ´æ€»ç»“å¯¹è¯è¦ç‚¹ã€‚"},
                {"role": "user", "content": conversation}
            ],
            max_tokens=300
        )
        
        new_summary = response.choices[0].message.content
        
        if self.summary:
            self.summary = f"{self.summary}\n\n{new_summary}"
        else:
            self.summary = new_summary
        
        # æ¸…ç†çŸ­æœŸè®°å¿†
        self.short_term = self.short_term[-4:]
    
    def _extract_and_update_entities(self, text: str):
        """æå–å¹¶æ›´æ–°å®ä½“"""
        # ç®€åŒ–ç‰ˆå®ä½“æå–
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "æå–æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå§“åã€åå¥½ç­‰ï¼‰ï¼Œè¿”å› JSONã€‚"
                },
                {"role": "user", "content": text}
            ],
            response_format={"type": "json_object"}
        )
        
        try:
            new_entities = json.loads(response.choices[0].message.content)
            self.entities.update(new_entities)
        except:
            pass
```

## LangChain è®°å¿†

```python
from langchain.memory import (
    ConversationBufferMemory,
    ConversationBufferWindowMemory,
    ConversationSummaryMemory,
    ConversationSummaryBufferMemory,
    VectorStoreRetrieverMemory
)
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model="gpt-4o")

# ç¼“å†²è®°å¿†
buffer_memory = ConversationBufferMemory()

# çª—å£è®°å¿†
window_memory = ConversationBufferWindowMemory(k=5)

# æ‘˜è¦è®°å¿†
summary_memory = ConversationSummaryMemory(llm=llm)

# æ‘˜è¦ç¼“å†²è®°å¿†ï¼ˆç»“åˆä¸¤è€…ï¼‰
summary_buffer_memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000
)

# ä½¿ç”¨è®°å¿†çš„å¯¹è¯é“¾
conversation = ConversationChain(
    llm=llm,
    memory=summary_buffer_memory,
    verbose=True
)

response = conversation.predict(input="ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰")
response = conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
```

## æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„è®°å¿†ç±»å‹**ï¼šç®€å•åœºæ™¯ç”¨çª—å£è®°å¿†ï¼Œå¤æ‚åœºæ™¯ç”¨ç»„åˆè®°å¿†
2. **æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šé¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶
3. **å®šæœŸæ¸…ç†**ï¼šé˜²æ­¢è®°å¿†æ— é™å¢é•¿
4. **æŒä¹…åŒ–å­˜å‚¨**ï¼šé‡è¦ä¿¡æ¯å­˜å…¥æ•°æ®åº“
5. **éšç§ä¿æŠ¤**ï¼šæ•æ„Ÿä¿¡æ¯è„±æ•å¤„ç†

## å»¶ä¼¸é˜…è¯»

- [LangChain Memory](https://python.langchain.com/docs/modules/memory/)
- [LlamaIndex Chat Engine](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/)
