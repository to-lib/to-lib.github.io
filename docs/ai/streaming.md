---
sidebar_position: 18
title: ğŸŒŠ æµå¼å¤„ç†
---

# æµå¼å¤„ç† (Streaming)

æµå¼å¤„ç†æ˜¯è®© LLM é€æ­¥è¿”å›ç”Ÿæˆå†…å®¹çš„æŠ€æœ¯ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”ã€‚è¿™èƒ½æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œè®©ç”¨æˆ·æ›´å¿«çœ‹åˆ°è¾“å‡ºã€‚

## ä¸ºä»€ä¹ˆéœ€è¦æµå¼å¤„ç†ï¼Ÿ

| åœºæ™¯           | éæµå¼         | æµå¼           |
| -------------- | -------------- | -------------- |
| é¦–å­—èŠ‚æ—¶é—´     | ç­‰å¾…å®Œæ•´ç”Ÿæˆ   | å‡ ç™¾æ¯«ç§’       |
| ç”¨æˆ·ä½“éªŒ       | é•¿æ—¶é—´ç­‰å¾…     | å®æ—¶åé¦ˆ       |
| é•¿æ–‡æœ¬ç”Ÿæˆ     | å¯èƒ½è¶…æ—¶       | æŒç»­è¾“å‡º       |
| èµ„æºåˆ©ç”¨       | ä¸€æ¬¡æ€§åŠ è½½     | æ¸è¿›å¼å¤„ç†     |

## OpenAI æµå¼ API

### åŸºç¡€ç”¨æ³•

```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### å¼‚æ­¥æµå¼

```python
from openai import AsyncOpenAI
import asyncio

client = AsyncOpenAI()

async def stream_chat(prompt: str):
    stream = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    
    async for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

asyncio.run(stream_chat("è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "))
```

### æ”¶é›†å®Œæ•´å“åº”

```python
def stream_with_full_response(prompt: str) -> tuple[str, dict]:
    """æµå¼è¾“å‡ºåŒæ—¶æ”¶é›†å®Œæ•´å“åº”"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True,
        stream_options={"include_usage": True}  # åŒ…å« token ä½¿ç”¨é‡
    )
    
    full_content = ""
    usage = None
    
    for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            full_content += content
            print(content, end="", flush=True)
        
        # æœ€åä¸€ä¸ª chunk åŒ…å« usage
        if chunk.usage:
            usage = {
                "prompt_tokens": chunk.usage.prompt_tokens,
                "completion_tokens": chunk.usage.completion_tokens,
                "total_tokens": chunk.usage.total_tokens
            }
    
    print()  # æ¢è¡Œ
    return full_content, usage

content, usage = stream_with_full_response("ä½ å¥½")
print(f"Token ä½¿ç”¨: {usage}")
```

## Anthropic æµå¼ API

### åŸºç¡€ç”¨æ³•

```python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ª Python å¿«é€Ÿæ’åº"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

### äº‹ä»¶å¤„ç†

```python
with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "ä½ å¥½"}]
) as stream:
    for event in stream:
        if event.type == "content_block_delta":
            print(event.delta.text, end="", flush=True)
        elif event.type == "message_stop":
            print("\n[å®Œæˆ]")
        elif event.type == "message_delta":
            print(f"\n[åœæ­¢åŸå› : {event.delta.stop_reason}]")
```

### è·å–æœ€ç»ˆæ¶ˆæ¯

```python
with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "ä½ å¥½"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
    
    # è·å–å®Œæ•´æ¶ˆæ¯å¯¹è±¡
    final_message = stream.get_final_message()
    print(f"\nToken ä½¿ç”¨: {final_message.usage}")
```

## æµå¼ Function Calling

### OpenAI æµå¼å·¥å…·è°ƒç”¨

```python
import json

def stream_with_tools(prompt: str, tools: list):
    """æµå¼å¤„ç†å¸¦å·¥å…·è°ƒç”¨"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        tools=tools,
        stream=True
    )
    
    tool_calls = {}
    content = ""
    
    for chunk in stream:
        delta = chunk.choices[0].delta
        
        # å¤„ç†æ–‡æœ¬å†…å®¹
        if delta.content:
            content += delta.content
            print(delta.content, end="", flush=True)
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if delta.tool_calls:
            for tc in delta.tool_calls:
                idx = tc.index
                if idx not in tool_calls:
                    tool_calls[idx] = {
                        "id": tc.id,
                        "name": tc.function.name if tc.function else "",
                        "arguments": ""
                    }
                if tc.function and tc.function.arguments:
                    tool_calls[idx]["arguments"] += tc.function.arguments
    
    return content, list(tool_calls.values())

# ä½¿ç”¨
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "è·å–å¤©æ°”",
        "parameters": {
            "type": "object",
            "properties": {"city": {"type": "string"}},
            "required": ["city"]
        }
    }
}]

content, tool_calls = stream_with_tools("åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", tools)
if tool_calls:
    print(f"\nå·¥å…·è°ƒç”¨: {tool_calls}")
```

## Web åº”ç”¨é›†æˆ

### FastAPI SSE

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI

app = FastAPI()
client = OpenAI()

async def generate_stream(prompt: str):
    """ç”Ÿæˆ SSE æµ"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            # SSE æ ¼å¼
            yield f"data: {json.dumps({'content': content})}\n\n"
    
    yield "data: [DONE]\n\n"

@app.get("/chat/stream")
async def chat_stream(prompt: str):
    return StreamingResponse(
        generate_stream(prompt),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

### å‰ç«¯ JavaScript æ¶ˆè´¹

```javascript
async function streamChat(prompt) {
    const response = await fetch(`/chat/stream?prompt=${encodeURIComponent(prompt)}`);
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const text = decoder.decode(value);
        const lines = text.split('\n');
        
        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = line.slice(6);
                if (data === '[DONE]') {
                    console.log('Stream completed');
                    return;
                }
                const parsed = JSON.parse(data);
                // æ›´æ–° UI
                document.getElementById('output').textContent += parsed.content;
            }
        }
    }
}
```

### React Hook

```typescript
import { useState, useCallback } from 'react';

function useStreamChat() {
    const [content, setContent] = useState('');
    const [isLoading, setIsLoading] = useState(false);
    
    const sendMessage = useCallback(async (prompt: string) => {
        setIsLoading(true);
        setContent('');
        
        try {
            const response = await fetch('/chat/stream', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ prompt })
            });
            
            const reader = response.body?.getReader();
            if (!reader) return;
            
            const decoder = new TextDecoder();
            
            while (true) {
                const { done, value } = await reader.read();
                if (done) break;
                
                const text = decoder.decode(value);
                // è§£æ SSE æ•°æ®
                const matches = text.matchAll(/data: ({.*?})\n/g);
                for (const match of matches) {
                    const data = JSON.parse(match[1]);
                    setContent(prev => prev + data.content);
                }
            }
        } finally {
            setIsLoading(false);
        }
    }, []);
    
    return { content, isLoading, sendMessage };
}
```

## LangChain æµå¼å¤„ç†

### åŸºç¡€æµå¼

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4o", streaming=True)

for chunk in llm.stream([HumanMessage(content="å†™ä¸€é¦–è¯—")]):
    print(chunk.content, end="", flush=True)
```

### æµå¼ Chain

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("ç”¨ {language} è§£é‡Š {topic}")
chain = prompt | llm | StrOutputParser()

for chunk in chain.stream({"language": "ç®€å•çš„è¯­è¨€", "topic": "é‡å­è®¡ç®—"}):
    print(chunk, end="", flush=True)
```

### å¼‚æ­¥æµå¼

```python
async def async_stream():
    async for chunk in llm.astream([HumanMessage(content="ä½ å¥½")]):
        print(chunk.content, end="", flush=True)

import asyncio
asyncio.run(async_stream())
```

### æµå¼äº‹ä»¶

```python
async def stream_events():
    """è·å–è¯¦ç»†çš„æµå¼äº‹ä»¶"""
    chain = prompt | llm | StrOutputParser()
    
    async for event in chain.astream_events(
        {"language": "ä¸­æ–‡", "topic": "AI"},
        version="v2"
    ):
        kind = event["event"]
        
        if kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                print(content, end="", flush=True)
        elif kind == "on_chain_end":
            print("\n[Chain å®Œæˆ]")
```

## æµå¼å¤„ç†æœ€ä½³å®è·µ

### 1. è¶…æ—¶å¤„ç†

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def stream_with_timeout(prompt: str, timeout: float = 30.0):
    """å¸¦è¶…æ—¶çš„æµå¼å¤„ç†"""
    try:
        stream = await asyncio.wait_for(
            client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                stream=True
            ),
            timeout=timeout
        )
        
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
    except asyncio.TimeoutError:
        yield "\n[è¶…æ—¶]"
```

### 2. é”™è¯¯æ¢å¤

```python
async def resilient_stream(prompt: str, max_retries: int = 3):
    """å¸¦é‡è¯•çš„æµå¼å¤„ç†"""
    for attempt in range(max_retries):
        try:
            stream = await client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            return
            
        except Exception as e:
            if attempt == max_retries - 1:
                yield f"\n[é”™è¯¯: {e}]"
            else:
                await asyncio.sleep(2 ** attempt)
```

### 3. å–æ¶ˆå¤„ç†

```python
import asyncio

async def cancellable_stream(prompt: str):
    """å¯å–æ¶ˆçš„æµå¼å¤„ç†"""
    stream = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    
    try:
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    except asyncio.CancelledError:
        print("\n[å·²å–æ¶ˆ]")
        raise

# ä½¿ç”¨
async def main():
    task = asyncio.create_task(consume_stream())
    await asyncio.sleep(2)
    task.cancel()  # 2ç§’åå–æ¶ˆ
```

### 4. ç¼“å†²å¤„ç†

```python
async def buffered_stream(prompt: str, buffer_size: int = 10):
    """ç¼“å†²æµå¼è¾“å‡ºï¼Œå‡å°‘ UI æ›´æ–°é¢‘ç‡"""
    buffer = ""
    
    stream = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    
    async for chunk in stream:
        if chunk.choices[0].delta.content:
            buffer += chunk.choices[0].delta.content
            
            if len(buffer) >= buffer_size:
                yield buffer
                buffer = ""
    
    if buffer:
        yield buffer
```

## æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥å¤ç”¨

```python
from openai import OpenAI
import httpx

# ä½¿ç”¨è‡ªå®šä¹‰ HTTP å®¢æˆ·ç«¯
http_client = httpx.Client(
    limits=httpx.Limits(max_keepalive_connections=10),
    timeout=httpx.Timeout(60.0, connect=5.0)
)

client = OpenAI(http_client=http_client)
```

### 2. å¹¶å‘æµå¼è¯·æ±‚

```python
async def parallel_streams(prompts: list[str]):
    """å¹¶å‘å¤„ç†å¤šä¸ªæµå¼è¯·æ±‚"""
    async def process_one(prompt: str, index: int):
        result = ""
        stream = await client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                result += chunk.choices[0].delta.content
        return index, result
    
    tasks = [process_one(p, i) for i, p in enumerate(prompts)]
    results = await asyncio.gather(*tasks)
    return dict(results)
```

## å»¶ä¼¸é˜…è¯»

- [OpenAI Streaming](https://platform.openai.com/docs/api-reference/streaming)
- [Anthropic Streaming](https://docs.anthropic.com/claude/reference/messages-streaming)
- [LangChain Streaming](https://python.langchain.com/docs/expression_language/streaming)
