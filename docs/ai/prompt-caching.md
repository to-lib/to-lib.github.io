---
sidebar_position: 22
title: ğŸ’¾ Prompt Caching
---

# Prompt Cachingï¼ˆæç¤ºç¼“å­˜ï¼‰

Prompt Caching æ˜¯ OpenAI å’Œ Anthropic æä¾›çš„åŠŸèƒ½ï¼Œå¯ä»¥ç¼“å­˜é‡å¤ä½¿ç”¨çš„æç¤ºå‰ç¼€ï¼Œæ˜¾è‘—é™ä½æˆæœ¬å’Œå»¶è¿Ÿã€‚

## ä¸ºä»€ä¹ˆéœ€è¦ Prompt Cachingï¼Ÿ

| åœºæ™¯               | é—®é¢˜                     | Prompt Caching æ•ˆæœ |
| ------------------ | ------------------------ | ------------------- |
| é•¿ç³»ç»Ÿæç¤º         | æ¯æ¬¡éƒ½è¦å¤„ç†ç›¸åŒå†…å®¹     | ç¼“å­˜ååªå¤„ç†ä¸€æ¬¡    |
| RAG å›ºå®šä¸Šä¸‹æ–‡     | ç›¸åŒæ–‡æ¡£é‡å¤å‘é€         | ç¼“å­˜æ–‡æ¡£å†…å®¹        |
| å¤šè½®å¯¹è¯           | å†å²æ¶ˆæ¯é‡å¤å¤„ç†         | ç¼“å­˜å†å²éƒ¨åˆ†        |
| Few-shot ç¤ºä¾‹      | ç›¸åŒç¤ºä¾‹é‡å¤å‘é€         | ç¼“å­˜ç¤ºä¾‹            |

## æˆæœ¬èŠ‚çœ

### OpenAI

| æ¨¡å‹    | æ­£å¸¸è¾“å…¥ä»·æ ¼ | ç¼“å­˜è¾“å…¥ä»·æ ¼ | èŠ‚çœæ¯”ä¾‹ |
| ------- | ------------ | ------------ | -------- |
| GPT-4o  | $2.50/1M     | $1.25/1M     | 50%      |
| o1      | $15.00/1M    | $7.50/1M     | 50%      |

### Anthropic

| æ¨¡å‹              | æ­£å¸¸è¾“å…¥ä»·æ ¼ | ç¼“å­˜å†™å…¥ä»·æ ¼ | ç¼“å­˜è¯»å–ä»·æ ¼ | èŠ‚çœæ¯”ä¾‹ |
| ----------------- | ------------ | ------------ | ------------ | -------- |
| Claude 3.5 Sonnet | $3.00/1M     | $3.75/1M     | $0.30/1M     | 90%      |
| Claude 3.5 Haiku  | $0.80/1M     | $1.00/1M     | $0.08/1M     | 90%      |

## OpenAI Prompt Caching

OpenAI çš„ Prompt Caching æ˜¯**è‡ªåŠ¨**çš„ï¼Œæ— éœ€é¢å¤–é…ç½®ã€‚

### å·¥ä½œåŸç†

```
è¯·æ±‚ 1: [ç³»ç»Ÿæç¤º 2000 tokens] + [ç”¨æˆ·æ¶ˆæ¯ 100 tokens]
        â†“ è‡ªåŠ¨ç¼“å­˜å‰ç¼€
è¯·æ±‚ 2: [ç³»ç»Ÿæç¤º 2000 tokens] + [ç”¨æˆ·æ¶ˆæ¯ 200 tokens]
        â†“ å‘½ä¸­ç¼“å­˜ï¼Œåªå¤„ç†æ–°å¢éƒ¨åˆ†
```

### ç¼“å­˜æ¡ä»¶

- æç¤ºå‰ç¼€å¿…é¡»**å®Œå…¨ç›¸åŒ**ï¼ˆé€å­—ç¬¦åŒ¹é…ï¼‰
- æœ€å°ç¼“å­˜é•¿åº¦ï¼š1024 tokens
- ç¼“å­˜æœ‰æ•ˆæœŸï¼š5-10 åˆ†é’Ÿï¼ˆä½æµé‡æ—¶æ›´çŸ­ï¼‰
- ç›¸åŒç»„ç»‡å†…çš„è¯·æ±‚å…±äº«ç¼“å­˜

### æŸ¥çœ‹ç¼“å­˜å‘½ä¸­

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹..." * 100},  # é•¿ç³»ç»Ÿæç¤º
        {"role": "user", "content": "ä½ å¥½"}
    ]
)

# æŸ¥çœ‹ç¼“å­˜ä¿¡æ¯
usage = response.usage
print(f"æ€»è¾“å…¥ tokens: {usage.prompt_tokens}")
print(f"ç¼“å­˜å‘½ä¸­ tokens: {usage.prompt_tokens_details.cached_tokens}")
```

### ä¼˜åŒ–ç¼“å­˜å‘½ä¸­ç‡

```python
# âœ… å¥½çš„åšæ³•ï¼šå›ºå®šå‰ç¼€ï¼Œå˜åŒ–éƒ¨åˆ†æ”¾åé¢
messages = [
    {"role": "system", "content": FIXED_SYSTEM_PROMPT},  # å›ºå®š
    {"role": "user", "content": FIXED_EXAMPLES},         # å›ºå®šç¤ºä¾‹
    {"role": "user", "content": user_input}              # å˜åŒ–éƒ¨åˆ†
]

# âŒ ä¸å¥½çš„åšæ³•ï¼šå˜åŒ–éƒ¨åˆ†åœ¨å‰é¢
messages = [
    {"role": "system", "content": f"å½“å‰æ—¶é—´ï¼š{datetime.now()}"},  # æ¯æ¬¡éƒ½å˜
    {"role": "system", "content": FIXED_SYSTEM_PROMPT},
    {"role": "user", "content": user_input}
]
```

### æ‰¹é‡è¯·æ±‚ä¼˜åŒ–

```python
# ç›¸åŒå‰ç¼€çš„è¯·æ±‚ä¼šå…±äº«ç¼“å­˜
async def batch_with_cache(prompts: list[str], system_prompt: str):
    """æ‰¹é‡è¯·æ±‚ï¼Œå…±äº«ç³»ç»Ÿæç¤ºç¼“å­˜"""
    tasks = []
    for prompt in prompts:
        task = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
        )
        tasks.append(task)
    
    return await asyncio.gather(*tasks)
```

## Anthropic Prompt Caching

Anthropic çš„ Prompt Caching éœ€è¦**æ˜¾å¼æ ‡è®°**ç¼“å­˜æ–­ç‚¹ã€‚

### åŸºç¡€ç”¨æ³•

```python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œä»¥ä¸‹æ˜¯ä½ éœ€è¦å‚è€ƒçš„æ–‡æ¡£ï¼š\n" + long_document,
            "cache_control": {"type": "ephemeral"}  # æ ‡è®°ç¼“å­˜ç‚¹
        }
    ],
    messages=[
        {"role": "user", "content": "æ€»ç»“æ–‡æ¡£çš„ä¸»è¦å†…å®¹"}
    ]
)

# æŸ¥çœ‹ç¼“å­˜ä¿¡æ¯
print(f"è¾“å…¥ tokens: {response.usage.input_tokens}")
print(f"ç¼“å­˜åˆ›å»º tokens: {response.usage.cache_creation_input_tokens}")
print(f"ç¼“å­˜è¯»å– tokens: {response.usage.cache_read_input_tokens}")
```

### å¤šä¸ªç¼“å­˜æ–­ç‚¹

```python
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "ä½ æ˜¯ä¸€ä¸ªä»£ç åŠ©æ‰‹ã€‚",
        },
        {
            "type": "text",
            "text": code_documentation,  # ä»£ç æ–‡æ¡£
            "cache_control": {"type": "ephemeral"}  # ç¼“å­˜ç‚¹ 1
        }
    ],
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": few_shot_examples,  # Few-shot ç¤ºä¾‹
                    "cache_control": {"type": "ephemeral"}  # ç¼“å­˜ç‚¹ 2
                },
                {
                    "type": "text",
                    "text": "è¯·å¸®æˆ‘å†™ä¸€ä¸ªæ’åºå‡½æ•°"
                }
            ]
        }
    ]
)
```

### ç¼“å­˜æ¡ä»¶

- æœ€å°ç¼“å­˜é•¿åº¦ï¼š
  - Claude 3.5 Sonnet/Opus: 1024 tokens
  - Claude 3.5 Haiku: 2048 tokens
- æœ€å¤š 4 ä¸ªç¼“å­˜æ–­ç‚¹
- ç¼“å­˜æœ‰æ•ˆæœŸï¼š5 åˆ†é’Ÿ
- å¿…é¡»ä½¿ç”¨ `cache_control` æ˜¾å¼æ ‡è®°

### å·¥å…·å®šä¹‰ç¼“å­˜

```python
tools = [
    {
        "name": "search",
        "description": "æœç´¢æ–‡æ¡£",
        "input_schema": {
            "type": "object",
            "properties": {
                "query": {"type": "string"}
            }
        }
    },
    # ... æ›´å¤šå·¥å…·
]

# ç¼“å­˜å·¥å…·å®šä¹‰
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    tools=tools,
    extra_headers={
        "anthropic-beta": "prompt-caching-2024-07-31"
    },
    system=[
        {
            "type": "text",
            "text": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[{"role": "user", "content": "æœç´¢ç›¸å…³æ–‡æ¡£"}]
)
```

## å®æˆ˜åº”ç”¨

### 1. RAG åœºæ™¯ä¼˜åŒ–

```python
class CachedRAG:
    """å¸¦ç¼“å­˜çš„ RAG ç³»ç»Ÿ"""
    
    def __init__(self, documents: str):
        self.documents = documents
        self.client = anthropic.Anthropic()
    
    def query(self, question: str) -> str:
        """æŸ¥è¯¢æ—¶å¤ç”¨æ–‡æ¡£ç¼“å­˜"""
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            system=[
                {
                    "type": "text",
                    "text": f"ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n\n{self.documents}",
                    "cache_control": {"type": "ephemeral"}
                }
            ],
            messages=[
                {"role": "user", "content": question}
            ]
        )
        return response.content[0].text

# ä½¿ç”¨
rag = CachedRAG(long_document)
# ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼šåˆ›å»ºç¼“å­˜
answer1 = rag.query("æ–‡æ¡£çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ")
# åç»­è¯·æ±‚ï¼šå‘½ä¸­ç¼“å­˜ï¼Œæˆæœ¬é™ä½ 90%
answer2 = rag.query("ä½œè€…æ˜¯è°ï¼Ÿ")
answer3 = rag.query("ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")
```

### 2. å¤šè½®å¯¹è¯ä¼˜åŒ–

```python
class CachedChat:
    """å¸¦ç¼“å­˜çš„å¤šè½®å¯¹è¯"""
    
    def __init__(self, system_prompt: str):
        self.system_prompt = system_prompt
        self.messages = []
        self.client = anthropic.Anthropic()
    
    def chat(self, user_message: str) -> str:
        self.messages.append({"role": "user", "content": user_message})
        
        # æ„å»ºå¸¦ç¼“å­˜çš„æ¶ˆæ¯
        cached_messages = []
        for i, msg in enumerate(self.messages[:-1]):  # å†å²æ¶ˆæ¯
            if i == len(self.messages) - 2:  # æœ€åä¸€æ¡å†å²æ¶ˆæ¯åŠ ç¼“å­˜
                cached_messages.append({
                    "role": msg["role"],
                    "content": [
                        {
                            "type": "text",
                            "text": msg["content"],
                            "cache_control": {"type": "ephemeral"}
                        }
                    ]
                })
            else:
                cached_messages.append(msg)
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        cached_messages.append(self.messages[-1])
        
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            system=[
                {
                    "type": "text",
                    "text": self.system_prompt,
                    "cache_control": {"type": "ephemeral"}
                }
            ],
            messages=cached_messages
        )
        
        assistant_message = response.content[0].text
        self.messages.append({"role": "assistant", "content": assistant_message})
        
        return assistant_message
```

### 3. Few-shot å­¦ä¹ ä¼˜åŒ–

```python
def few_shot_with_cache(examples: str, query: str) -> str:
    """ç¼“å­˜ Few-shot ç¤ºä¾‹"""
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š\n\n{examples}",
                        "cache_control": {"type": "ephemeral"}
                    },
                    {
                        "type": "text",
                        "text": f"\n\nç°åœ¨è¯·å¤„ç†ï¼š{query}"
                    }
                ]
            }
        ]
    )
    return response.content[0].text

# ç¤ºä¾‹
examples = """
è¾“å…¥ï¼šä»Šå¤©å¤©æ°”çœŸå¥½
è¾“å‡ºï¼špositive

è¾“å…¥ï¼šè¿™ä¸ªäº§å“å¤ªå·®äº†
è¾“å‡ºï¼šnegative

è¾“å…¥ï¼šè¿˜è¡Œå§
è¾“å‡ºï¼šneutral
"""

# å¤šæ¬¡è°ƒç”¨ï¼Œç¤ºä¾‹éƒ¨åˆ†è¢«ç¼“å­˜
result1 = few_shot_with_cache(examples, "æˆ‘å¾ˆå–œæ¬¢è¿™ä¸ª")
result2 = few_shot_with_cache(examples, "å¤ªç³Ÿç³•äº†")
result3 = few_shot_with_cache(examples, "ä¸€èˆ¬èˆ¬")
```

## ç›‘æ§ä¸ä¼˜åŒ–

### ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§

```python
class CacheMonitor:
    """ç¼“å­˜ç›‘æ§"""
    
    def __init__(self):
        self.total_input_tokens = 0
        self.cached_tokens = 0
        self.requests = 0
    
    def record(self, usage):
        self.requests += 1
        self.total_input_tokens += usage.input_tokens
        
        # Anthropic
        if hasattr(usage, 'cache_read_input_tokens'):
            self.cached_tokens += usage.cache_read_input_tokens
        # OpenAI
        elif hasattr(usage, 'prompt_tokens_details'):
            self.cached_tokens += usage.prompt_tokens_details.cached_tokens
    
    def get_stats(self) -> dict:
        cache_rate = self.cached_tokens / self.total_input_tokens if self.total_input_tokens > 0 else 0
        return {
            "requests": self.requests,
            "total_input_tokens": self.total_input_tokens,
            "cached_tokens": self.cached_tokens,
            "cache_hit_rate": f"{cache_rate:.2%}",
            "estimated_savings": f"${self.cached_tokens * 0.0000025:.4f}"  # å‡è®¾èŠ‚çœ $2.5/1M
        }

monitor = CacheMonitor()
```

## æœ€ä½³å®è·µ

1. **å›ºå®šå‰ç¼€**ï¼šæŠŠä¸å˜çš„å†…å®¹æ”¾åœ¨æ¶ˆæ¯å¼€å¤´
2. **åˆç†åˆ†ç»„**ï¼šç›¸ä¼¼è¯·æ±‚æ”¾åœ¨ä¸€èµ·å‘é€ï¼Œæé«˜ç¼“å­˜å‘½ä¸­
3. **ç›‘æ§å‘½ä¸­ç‡**ï¼šå®šæœŸæ£€æŸ¥ç¼“å­˜æ•ˆæœ
4. **é¿å…åŠ¨æ€å†…å®¹**ï¼šæ—¶é—´æˆ³ã€éšæœºæ•°ç­‰æ”¾åœ¨æ¶ˆæ¯æœ«å°¾
5. **æ‰¹é‡å¤„ç†**ï¼šç›¸åŒä¸Šä¸‹æ–‡çš„è¯·æ±‚æ‰¹é‡å‘é€

## å»¶ä¼¸é˜…è¯»

- [OpenAI Prompt Caching](https://platform.openai.com/docs/guides/prompt-caching)
- [Anthropic Prompt Caching](https://docs.anthropic.com/claude/docs/prompt-caching)
