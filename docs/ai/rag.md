---
sidebar_position: 4
title: ğŸ“š RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)
---

# RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)

**RAG (Retrieval-Augmented Generation)** æ˜¯ä¸€ç§ç»“åˆäº†**æ£€ç´¢ (Retrieval)** å’Œ **ç”Ÿæˆ (Generation)** çš„ AI æŠ€æœ¯æ¶æ„ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆå›ç­”ä¹‹å‰å…ˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ç»™å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ï¼Œä»è€Œæ˜¾è‘—æå‡å›ç­”çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦ RAGï¼Ÿ

LLM (å¦‚ GPT-4) å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š

- **çŸ¥è¯†æˆªæ­¢**ï¼šæ¨¡å‹è®­ç»ƒæ•°æ®æ˜¯é™æ€çš„ï¼Œæ— æ³•è·çŸ¥æœ€æ–°çš„æ—¶äº‹ã€‚
- **å¹»è§‰ (Hallucination)**ï¼šåœ¨ä¸çŸ¥é“ç­”æ¡ˆæ—¶å¯èƒ½ä¼šä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“ã€‚
- **ç§æœ‰æ•°æ®ç¼ºå¤±**ï¼šæ¨¡å‹ä»æœªè§è¿‡ä¼ä¸šçš„å†…éƒ¨æ–‡æ¡£å’Œç§æœ‰æ•°æ®ã€‚

RAG é€šè¿‡å¤–æŒ‚çŸ¥è¯†åº“é€šè¿‡è§£å†³äº†è¿™äº›é—®é¢˜ã€‚

## RAG çš„å·¥ä½œæµç¨‹

RAG çš„å…¸å‹æµç¨‹åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š

### 1. ç´¢å¼• (Indexing) - å‡†å¤‡é˜¶æ®µ

å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å…¥æ•°æ®åº“ã€‚

- **åŠ è½½ (Load)**ï¼šè¯»å– PDF, Word, Markdown, HTML ç­‰æ–‡ä»¶ã€‚
- **åˆ‡åˆ† (Split)**ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºè¾ƒå°çš„æ–‡æœ¬å— (Chunks)ã€‚
- **åµŒå…¥ (Embed)**ï¼šä½¿ç”¨ Embedding æ¨¡å‹å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡ (Vectors)ã€‚
- **å­˜å‚¨ (Store)**ï¼šå°†å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ (Vector DB) ä¸­ã€‚

### 2. æ£€ç´¢ (Retrieval) - è¿è¡Œé˜¶æ®µ

- **æŸ¥è¯¢ç¼–ç **ï¼šå°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºå‘é‡ã€‚
- **ç›¸ä¼¼åº¦æœç´¢**ï¼šåœ¨å‘é‡æ•°æ®åº“ä¸­æŸ¥æ‰¾ä¸é—®é¢˜å‘é‡æœ€ç›¸ä¼¼çš„æ–‡æœ¬å— (Top-K)ã€‚

### 3. ç”Ÿæˆ (Generation) - è¿è¡Œé˜¶æ®µ

- **æ„å»º Prompt**ï¼šå°†æ£€ç´¢åˆ°çš„æ–‡æœ¬å—ä½œä¸ºâ€œä¸Šä¸‹æ–‡ (Context)â€å¡«å…¥ Prompt æ¨¡æ¿ã€‚
- **LLM å›ç­”**ï¼šLLM åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

## æ ¸å¿ƒæŠ€æœ¯æ ˆ

### å‘é‡æ•°æ®åº“ (Vector Database)

- **Pinecone**: æ‰˜ç®¡å‹å‘é‡æ•°æ®åº“ï¼Œæ˜“äºä½¿ç”¨ã€‚
- **Milvus**: å¼€æºé«˜æ€§èƒ½å‘é‡æ•°æ®åº“ã€‚
- **Chroma**: è½»é‡çº§å¼€æºå‘é‡æ•°æ®åº“ï¼Œé€‚åˆæœ¬åœ°å¼€å‘ã€‚
- **Elasticsearch / pgvector**: ä¼ ç»Ÿæ•°æ®åº“çš„å‘é‡æ‰©å±•ã€‚

### å¼€å‘æ¡†æ¶

- **LangChain**: æœ€æµè¡Œçš„ LLM åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæä¾›äº†ä¸°å¯Œçš„ RAG ç»„ä»¶ã€‚
- **LlamaIndex**: ä¸“æ³¨äºæ•°æ®ç´¢å¼•å’Œæ£€ç´¢çš„æ¡†æ¶ï¼Œå¯¹ RAG ä¼˜åŒ–æä½³ã€‚

## é«˜çº§ RAG æŠ€å·§

- **æ··åˆæ£€ç´¢ (Hybrid Search)**ï¼šç»“åˆå…³é”®è¯æ£€ç´¢ (BM25) å’Œå‘é‡æ£€ç´¢ï¼Œæé«˜å¬å›ç‡ã€‚
- **é‡æ’åº (Re-ranking)**ï¼šæ£€ç´¢å‡ºè¾ƒå¤šç»“æœåï¼Œä½¿ç”¨ä¸“é—¨çš„ Re-rank æ¨¡å‹è¿›è¡Œç²¾ç»†æ’åºã€‚
- **å…ƒæ•°æ®è¿‡æ»¤ (Metadata Filtering)**ï¼šåœ¨æ£€ç´¢å‰é€šè¿‡æ—¶é—´ã€ä½œè€…ç­‰æ ‡ç­¾è¿‡æ»¤æ•°æ®ã€‚
- **æŸ¥è¯¢é‡å†™ (Query Rewriting)**ï¼šå°†ç”¨æˆ·æ¨¡ç³Šçš„é—®é¢˜æ”¹å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ã€‚

## ä»£ç å®ç°ç¤ºä¾‹

### ä½¿ç”¨ LangChain æ„å»º RAG

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. åŠ è½½æ–‡æ¡£
loader = DirectoryLoader("./docs", glob="**/*.md", loader_cls=TextLoader)
documents = loader.load()

# 2. åˆ‡åˆ†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " "]
)
chunks = text_splitter.split_documents(documents)

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory="./chroma_db")

# 4. åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# 5. åˆ›å»º RAG é“¾
llm = ChatOpenAI(model="gpt-4o", temperature=0)

prompt = ChatPromptTemplate.from_template("""
è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 6. ä½¿ç”¨
answer = rag_chain.invoke("ä»€ä¹ˆæ˜¯ Spring Boot çš„è‡ªåŠ¨é…ç½®ï¼Ÿ")
print(answer)
```

### ä½¿ç”¨ LlamaIndex æ„å»º RAG

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

# é…ç½®æ¨¡å‹
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
Settings.llm = OpenAI(model="gpt-4o", temperature=0)

# åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºç´¢å¼•
documents = SimpleDirectoryReader("./docs").load_data()
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(similarity_top_k=3)

# æŸ¥è¯¢
response = query_engine.query("ä»€ä¹ˆæ˜¯ RAGï¼Ÿ")
print(response)
```

### æ·»åŠ æ··åˆæ£€ç´¢ (Hybrid Search)

```python
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# å‘é‡æ£€ç´¢å™¨
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# BM25 å…³é”®è¯æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 3

# é›†æˆæ£€ç´¢å™¨ (æƒé‡å¯è°ƒ)
hybrid_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.6, 0.4]  # 60% å‘é‡ + 40% å…³é”®è¯
)

# ä½¿ç”¨
docs = hybrid_retriever.invoke("Spring Boot é…ç½®æ–‡ä»¶")
```

## Embedding æ¨¡å‹é€‰æ‹©æŒ‡å—

| ä½¿ç”¨åœºæ™¯   | æ¨èæ¨¡å‹                 | è¯´æ˜               |
| ---------- | ------------------------ | ------------------ |
| è‹±æ–‡é€šç”¨   | `text-embedding-3-small` | ç»æµå®æƒ            |
| è‹±æ–‡é«˜ç²¾åº¦ | `text-embedding-3-large` | æ›´é«˜å‡†ç¡®ç‡         |
| ä¸­æ–‡é€šç”¨   | `bge-large-zh-v1.5`      | å¼€æºæœ€ä½³           |
| å¤šè¯­è¨€     | `bge-m3`                 | æ”¯æŒ 100+ è¯­è¨€     |
| æœ¬åœ°éƒ¨ç½²   | `m3e-base`               | è½»é‡ï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡ |

### ä½¿ç”¨å¼€æº Embedding æ¨¡å‹

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True}
)

# ä½¿ç”¨æ–¹å¼ä¸ OpenAI Embeddings ç›¸åŒ
vectorstore = Chroma.from_documents(chunks, embeddings)
```

## æœ€ä½³å®è·µ

1. **æ–‡æ¡£é¢„å¤„ç†**ï¼šæ¸…ç†æ— å…³å†…å®¹ï¼Œä¿ç•™ç»“æ„åŒ–ä¿¡æ¯
2. **åˆç†åˆ‡åˆ†**ï¼šchunk_size 500-1500ï¼Œoverlap 10-20%
3. **å…ƒæ•°æ®ä¸°å¯Œ**ï¼šæ·»åŠ æ¥æºã€æ—¶é—´ã€ç±»åˆ«ç­‰å…ƒæ•°æ®
4. **æ··åˆæ£€ç´¢**ï¼šç»“åˆå‘é‡å’Œå…³é”®è¯ï¼Œæé«˜å¬å›
5. **ç»“æœé‡æ’åº**ï¼šä½¿ç”¨ Re-ranker æå‡ç²¾åº¦
6. **å®šæœŸæ›´æ–°**ï¼šä¿æŒçŸ¥è¯†åº“æ—¶æ•ˆæ€§

## å»¶ä¼¸é˜…è¯»

- [LangChain RAG æ•™ç¨‹](https://python.langchain.com/docs/tutorials/rag/)
- [LlamaIndex æ–‡æ¡£](https://docs.llamaindex.ai/)
- [RAG è®ºæ–‡](https://arxiv.org/abs/2005.11401)
