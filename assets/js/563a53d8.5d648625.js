"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[17078],{48885:(n,e,l)=>{l.d(e,{R:()=>t,x:()=>r});var a=l(99378);const s={},i=a.createContext(s);function t(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),a.createElement(i.Provider,{value:e},n.children)}},51673:(n,e,l)=>{l.r(e),l.d(e,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"ai/local-llm","title":"\ud83c\udfe0 \u672c\u5730\u90e8\u7f72 LLM","description":"\u672c\u5730\u90e8\u7f72 LLM \u53ef\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3001\u964d\u4f4e\u6210\u672c\u3001\u51cf\u5c11\u5ef6\u8fdf\u3002\u672c\u6587\u4ecb\u7ecd\u4e3b\u6d41\u7684\u672c\u5730\u90e8\u7f72\u65b9\u6848\u3002","source":"@site/docs/ai/local-llm.md","sourceDirName":"ai","slug":"/ai/local-llm","permalink":"/docs/ai/local-llm","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/local-llm.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"sidebar_position":19,"title":"\ud83c\udfe0 \u672c\u5730\u90e8\u7f72 LLM"},"sidebar":"ai","previous":{"title":"\ud83d\uddc4\ufe0f \u5411\u91cf\u6570\u636e\u5e93\u5b9e\u6218","permalink":"/docs/ai/vector-database"},"next":{"title":"\ud83d\udcbb AI \u7f16\u7801\u52a9\u624b\u5f00\u53d1","permalink":"/docs/ai/coding-assistant"}}');var s=l(22714),i=l(48885);const t={sidebar_position:19,title:"\ud83c\udfe0 \u672c\u5730\u90e8\u7f72 LLM"},r="\u672c\u5730\u90e8\u7f72 LLM",d={},c=[{value:"\u90e8\u7f72\u65b9\u6848\u5bf9\u6bd4",id:"\u90e8\u7f72\u65b9\u6848\u5bf9\u6bd4",level:2},{value:"Ollama",id:"ollama",level:2},{value:"\u5b89\u88c5",id:"\u5b89\u88c5",level:3},{value:"\u57fa\u7840\u4f7f\u7528",id:"\u57fa\u7840\u4f7f\u7528",level:3},{value:"API \u8c03\u7528",id:"api-\u8c03\u7528",level:3},{value:"\u4f7f\u7528 OpenAI SDK",id:"\u4f7f\u7528-openai-sdk",level:3},{value:"\u6d41\u5f0f\u8f93\u51fa",id:"\u6d41\u5f0f\u8f93\u51fa",level:3},{value:"\u81ea\u5b9a\u4e49\u6a21\u578b",id:"\u81ea\u5b9a\u4e49\u6a21\u578b",level:3},{value:"vLLM",id:"vllm",level:2},{value:"\u5b89\u88c5",id:"\u5b89\u88c5-1",level:3},{value:"\u542f\u52a8\u670d\u52a1",id:"\u542f\u52a8\u670d\u52a1",level:3},{value:"API \u8c03\u7528",id:"api-\u8c03\u7528-1",level:3},{value:"\u79bb\u7ebf\u6279\u91cf\u63a8\u7406",id:"\u79bb\u7ebf\u6279\u91cf\u63a8\u7406",level:3},{value:"\u9ad8\u7ea7\u914d\u7f6e",id:"\u9ad8\u7ea7\u914d\u7f6e",level:3},{value:"llama.cpp",id:"llamacpp",level:2},{value:"\u5b89\u88c5 Python \u7ed1\u5b9a",id:"\u5b89\u88c5-python-\u7ed1\u5b9a",level:3},{value:"\u4e0b\u8f7d\u6a21\u578b",id:"\u4e0b\u8f7d\u6a21\u578b",level:3},{value:"\u57fa\u7840\u4f7f\u7528",id:"\u57fa\u7840\u4f7f\u7528-1",level:3},{value:"Chat \u683c\u5f0f",id:"chat-\u683c\u5f0f",level:3},{value:"\u542f\u52a8 OpenAI \u517c\u5bb9\u670d\u52a1",id:"\u542f\u52a8-openai-\u517c\u5bb9\u670d\u52a1",level:3},{value:"\u6a21\u578b\u91cf\u5316",id:"\u6a21\u578b\u91cf\u5316",level:2},{value:"\u5e38\u89c1\u91cf\u5316\u683c\u5f0f",id:"\u5e38\u89c1\u91cf\u5316\u683c\u5f0f",level:3},{value:"\u4f7f\u7528 AutoGPTQ \u91cf\u5316",id:"\u4f7f\u7528-autogptq-\u91cf\u5316",level:3},{value:"LangChain \u96c6\u6210",id:"langchain-\u96c6\u6210",level:2},{value:"Ollama",id:"ollama-1",level:3},{value:"llama.cpp",id:"llamacpp-1",level:3},{value:"Docker \u90e8\u7f72",id:"docker-\u90e8\u7f72",level:2},{value:"Ollama Docker",id:"ollama-docker",level:3},{value:"vLLM Docker",id:"vllm-docker",level:3},{value:"\u786c\u4ef6\u8981\u6c42",id:"\u786c\u4ef6\u8981\u6c42",level:2},{value:"GPU \u663e\u5b58\u9700\u6c42\uff08\u63a8\u7406\uff09",id:"gpu-\u663e\u5b58\u9700\u6c42\u63a8\u7406",level:3},{value:"\u63a8\u8350\u914d\u7f6e",id:"\u63a8\u8350\u914d\u7f6e",level:3},{value:"\u6027\u80fd\u4f18\u5316",id:"\u6027\u80fd\u4f18\u5316",level:2},{value:"1. \u6279\u5904\u7406",id:"1-\u6279\u5904\u7406",level:3},{value:"2. KV Cache \u4f18\u5316",id:"2-kv-cache-\u4f18\u5316",level:3},{value:"3. \u6295\u673a\u89e3\u7801",id:"3-\u6295\u673a\u89e3\u7801",level:3},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function o(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"\u672c\u5730\u90e8\u7f72-llm",children:"\u672c\u5730\u90e8\u7f72 LLM"})}),"\n",(0,s.jsx)(e.p,{children:"\u672c\u5730\u90e8\u7f72 LLM \u53ef\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3001\u964d\u4f4e\u6210\u672c\u3001\u51cf\u5c11\u5ef6\u8fdf\u3002\u672c\u6587\u4ecb\u7ecd\u4e3b\u6d41\u7684\u672c\u5730\u90e8\u7f72\u65b9\u6848\u3002"}),"\n",(0,s.jsx)(e.h2,{id:"\u90e8\u7f72\u65b9\u6848\u5bf9\u6bd4",children:"\u90e8\u7f72\u65b9\u6848\u5bf9\u6bd4"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"\u65b9\u6848"}),(0,s.jsx)(e.th,{children:"\u7279\u70b9"}),(0,s.jsx)(e.th,{children:"\u9002\u7528\u573a\u666f"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Ollama"})}),(0,s.jsx)(e.td,{children:"\u7b80\u5355\u6613\u7528\uff0c\u4e00\u952e\u90e8\u7f72"}),(0,s.jsx)(e.td,{children:"\u5f00\u53d1\u6d4b\u8bd5\u3001\u4e2a\u4eba\u4f7f\u7528"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"vLLM"})}),(0,s.jsx)(e.td,{children:"\u9ad8\u6027\u80fd\uff0c\u751f\u4ea7\u7ea7"}),(0,s.jsx)(e.td,{children:"\u751f\u4ea7\u73af\u5883\u3001\u9ad8\u5e76\u53d1"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"llama.cpp"})}),(0,s.jsx)(e.td,{children:"\u8f7b\u91cf\uff0cCPU \u53cb\u597d"}),(0,s.jsx)(e.td,{children:"\u8fb9\u7f18\u8bbe\u5907\u3001\u4f4e\u8d44\u6e90"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"TGI"})}),(0,s.jsx)(e.td,{children:"HuggingFace \u5b98\u65b9"}),(0,s.jsx)(e.td,{children:"\u4f01\u4e1a\u90e8\u7f72"})]})]})]}),"\n",(0,s.jsx)(e.h2,{id:"ollama",children:"Ollama"}),"\n",(0,s.jsx)(e.p,{children:"Ollama \u662f\u6700\u7b80\u5355\u7684\u672c\u5730 LLM \u90e8\u7f72\u65b9\u6848\uff0c\u652f\u6301 macOS\u3001Linux\u3001Windows\u3002"}),"\n",(0,s.jsx)(e.h3,{id:"\u5b89\u88c5",children:"\u5b89\u88c5"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# macOS / Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# \u6216\u4f7f\u7528 Homebrew (macOS)\nbrew install ollama\n"})}),"\n",(0,s.jsx)(e.h3,{id:"\u57fa\u7840\u4f7f\u7528",children:"\u57fa\u7840\u4f7f\u7528"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# \u542f\u52a8\u670d\u52a1\nollama serve\n\n# \u8fd0\u884c\u6a21\u578b\uff08\u81ea\u52a8\u4e0b\u8f7d\uff09\nollama run llama3.2\nollama run qwen2.5:7b\nollama run deepseek-coder:6.7b\n\n# \u5217\u51fa\u5df2\u4e0b\u8f7d\u6a21\u578b\nollama list\n\n# \u5220\u9664\u6a21\u578b\nollama rm llama3.2\n"})}),"\n",(0,s.jsx)(e.h3,{id:"api-\u8c03\u7528",children:"API \u8c03\u7528"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import requests\n\n# Ollama \u517c\u5bb9 OpenAI API \u683c\u5f0f\nresponse = requests.post(\n    "http://localhost:11434/api/chat",\n    json={\n        "model": "llama3.2",\n        "messages": [{"role": "user", "content": "\u4f60\u597d"}],\n        "stream": False\n    }\n)\n\nprint(response.json()["message"]["content"])\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u4f7f\u7528-openai-sdk",children:"\u4f7f\u7528 OpenAI SDK"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\n# \u6307\u5411 Ollama \u670d\u52a1\nclient = OpenAI(\n    base_url="http://localhost:11434/v1",\n    api_key="ollama"  # \u4efb\u610f\u503c\n)\n\nresponse = client.chat.completions.create(\n    model="llama3.2",\n    messages=[{"role": "user", "content": "\u89e3\u91ca\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60"}]\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u6d41\u5f0f\u8f93\u51fa",children:"\u6d41\u5f0f\u8f93\u51fa"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'stream = client.chat.completions.create(\n    model="llama3.2",\n    messages=[{"role": "user", "content": "\u5199\u4e00\u9996\u8bd7"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end="", flush=True)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u81ea\u5b9a\u4e49\u6a21\u578b",children:"\u81ea\u5b9a\u4e49\u6a21\u578b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'# \u521b\u5efa Modelfile\ncat > Modelfile << \'EOF\'\nFROM llama3.2\n\n# \u8bbe\u7f6e\u7cfb\u7edf\u63d0\u793a\nSYSTEM """\n\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u4ee3\u7801\u52a9\u624b\uff0c\u64c5\u957f Python \u548c JavaScript\u3002\n\u56de\u7b54\u8981\u7b80\u6d01\u3001\u51c6\u786e\uff0c\u5e76\u63d0\u4f9b\u4ee3\u7801\u793a\u4f8b\u3002\n"""\n\n# \u8c03\u6574\u53c2\u6570\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nEOF\n\n# \u521b\u5efa\u81ea\u5b9a\u4e49\u6a21\u578b\nollama create code-assistant -f Modelfile\n\n# \u8fd0\u884c\nollama run code-assistant\n'})}),"\n",(0,s.jsx)(e.h2,{id:"vllm",children:"vLLM"}),"\n",(0,s.jsx)(e.p,{children:"vLLM \u662f\u9ad8\u6027\u80fd\u7684 LLM \u63a8\u7406\u5f15\u64ce\uff0c\u652f\u6301 PagedAttention\u3001\u8fde\u7eed\u6279\u5904\u7406\u7b49\u4f18\u5316\u6280\u672f\u3002"}),"\n",(0,s.jsx)(e.h3,{id:"\u5b89\u88c5-1",children:"\u5b89\u88c5"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"pip install vllm\n"})}),"\n",(0,s.jsx)(e.h3,{id:"\u542f\u52a8\u670d\u52a1",children:"\u542f\u52a8\u670d\u52a1"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# \u542f\u52a8 OpenAI \u517c\u5bb9\u670d\u52a1\npython -m vllm.entrypoints.openai.api_server \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --tensor-parallel-size 1\n"})}),"\n",(0,s.jsx)(e.h3,{id:"api-\u8c03\u7528-1",children:"API \u8c03\u7528"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8000/v1",\n    api_key="vllm"\n)\n\nresponse = client.chat.completions.create(\n    model="Qwen/Qwen2.5-7B-Instruct",\n    messages=[{"role": "user", "content": "\u4f60\u597d"}]\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u79bb\u7ebf\u6279\u91cf\u63a8\u7406",children:"\u79bb\u7ebf\u6279\u91cf\u63a8\u7406"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\n# \u52a0\u8f7d\u6a21\u578b\nllm = LLM(model="Qwen/Qwen2.5-7B-Instruct")\n\n# \u8bbe\u7f6e\u91c7\u6837\u53c2\u6570\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=512\n)\n\n# \u6279\u91cf\u63a8\u7406\nprompts = [\n    "\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\uff1f",\n    "Python \u6709\u4ec0\u4e48\u4f18\u70b9\uff1f",\n    "\u89e3\u91ca RESTful API"\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    print(f"Prompt: {output.prompt}")\n    print(f"Output: {output.outputs[0].text}\\n")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u9ad8\u7ea7\u914d\u7f6e",children:"\u9ad8\u7ea7\u914d\u7f6e"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\nllm = LLM(\n    model="Qwen/Qwen2.5-7B-Instruct",\n    tensor_parallel_size=2,      # \u591a GPU \u5e76\u884c\n    gpu_memory_utilization=0.9,  # GPU \u5185\u5b58\u4f7f\u7528\u7387\n    max_model_len=8192,          # \u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\n    quantization="awq",          # \u91cf\u5316\u65b9\u5f0f\n    dtype="float16"              # \u6570\u636e\u7c7b\u578b\n)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"llamacpp",children:"llama.cpp"}),"\n",(0,s.jsx)(e.p,{children:"llama.cpp \u662f\u7eaf C/C++ \u5b9e\u73b0\u7684 LLM \u63a8\u7406\u5e93\uff0c\u652f\u6301 CPU \u63a8\u7406\uff0c\u8d44\u6e90\u5360\u7528\u4f4e\u3002"}),"\n",(0,s.jsx)(e.h3,{id:"\u5b89\u88c5-python-\u7ed1\u5b9a",children:"\u5b89\u88c5 Python \u7ed1\u5b9a"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'pip install llama-cpp-python\n\n# \u652f\u6301 GPU \u52a0\u901f\nCMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u4e0b\u8f7d\u6a21\u578b",children:"\u4e0b\u8f7d\u6a21\u578b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# \u4ece HuggingFace \u4e0b\u8f7d GGUF \u683c\u5f0f\u6a21\u578b\n# \u4f8b\u5982\uff1ahttps://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n"})}),"\n",(0,s.jsx)(e.h3,{id:"\u57fa\u7840\u4f7f\u7528-1",children:"\u57fa\u7840\u4f7f\u7528"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from llama_cpp import Llama\n\n# \u52a0\u8f7d\u6a21\u578b\nllm = Llama(\n    model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",\n    n_ctx=4096,      # \u4e0a\u4e0b\u6587\u957f\u5ea6\n    n_threads=8,     # CPU \u7ebf\u7a0b\u6570\n    n_gpu_layers=35  # GPU \u52a0\u901f\u5c42\u6570\uff080 = \u7eaf CPU\uff09\n)\n\n# \u751f\u6210\noutput = llm(\n    "Q: \u4ec0\u4e48\u662f\u4eba\u5de5\u667a\u80fd\uff1f\\nA:",\n    max_tokens=256,\n    temperature=0.7,\n    stop=["Q:", "\\n\\n"]\n)\n\nprint(output["choices"][0]["text"])\n'})}),"\n",(0,s.jsx)(e.h3,{id:"chat-\u683c\u5f0f",children:"Chat \u683c\u5f0f"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'output = llm.create_chat_completion(\n    messages=[\n        {"role": "system", "content": "\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002"},\n        {"role": "user", "content": "\u89e3\u91ca\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60"}\n    ],\n    temperature=0.7,\n    max_tokens=512\n)\n\nprint(output["choices"][0]["message"]["content"])\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u542f\u52a8-openai-\u517c\u5bb9\u670d\u52a1",children:"\u542f\u52a8 OpenAI \u517c\u5bb9\u670d\u52a1"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"python -m llama_cpp.server \\\n    --model ./models/llama-2-7b-chat.Q4_K_M.gguf \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --n_ctx 4096\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u6a21\u578b\u91cf\u5316",children:"\u6a21\u578b\u91cf\u5316"}),"\n",(0,s.jsx)(e.p,{children:"\u91cf\u5316\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u5185\u5b58\u5360\u7528\u3002"}),"\n",(0,s.jsx)(e.h3,{id:"\u5e38\u89c1\u91cf\u5316\u683c\u5f0f",children:"\u5e38\u89c1\u91cf\u5316\u683c\u5f0f"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"\u683c\u5f0f"}),(0,s.jsx)(e.th,{children:"\u8bf4\u660e"}),(0,s.jsx)(e.th,{children:"\u5927\u5c0f\uff087B\uff09"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"FP16"}),(0,s.jsx)(e.td,{children:"\u534a\u7cbe\u5ea6\u6d6e\u70b9"}),(0,s.jsx)(e.td,{children:"~14GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"INT8"}),(0,s.jsx)(e.td,{children:"8\u4f4d\u6574\u6570"}),(0,s.jsx)(e.td,{children:"~7GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"INT4"}),(0,s.jsx)(e.td,{children:"4\u4f4d\u6574\u6570"}),(0,s.jsx)(e.td,{children:"~4GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"GGUF Q4"}),(0,s.jsx)(e.td,{children:"llama.cpp 4\u4f4d\u91cf\u5316"}),(0,s.jsx)(e.td,{children:"~4GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"AWQ"}),(0,s.jsx)(e.td,{children:"\u6fc0\u6d3b\u611f\u77e5\u91cf\u5316"}),(0,s.jsx)(e.td,{children:"~4GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"GPTQ"}),(0,s.jsx)(e.td,{children:"\u540e\u8bad\u7ec3\u91cf\u5316"}),(0,s.jsx)(e.td,{children:"~4GB"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"\u4f7f\u7528-autogptq-\u91cf\u5316",children:"\u4f7f\u7528 AutoGPTQ \u91cf\u5316"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel_id = "Qwen/Qwen2.5-7B-Instruct"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# \u91cf\u5316\u914d\u7f6e\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    desc_act=False\n)\n\n# \u91cf\u5316\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_id,\n    quantize_config=quantize_config\n)\n\n# \u51c6\u5907\u6821\u51c6\u6570\u636e\nexamples = [tokenizer(text) for text in calibration_texts]\nmodel.quantize(examples)\n\n# \u4fdd\u5b58\nmodel.save_quantized("./qwen2.5-7b-gptq")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"langchain-\u96c6\u6210",children:"LangChain \u96c6\u6210"}),"\n",(0,s.jsx)(e.h3,{id:"ollama-1",children:"Ollama"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from langchain_community.llms import Ollama\nfrom langchain_community.chat_models import ChatOllama\n\n# LLM\nllm = Ollama(model="llama3.2")\nresponse = llm.invoke("\u4f60\u597d")\n\n# Chat Model\nchat = ChatOllama(model="llama3.2")\nresponse = chat.invoke([{"role": "user", "content": "\u4f60\u597d"}])\n'})}),"\n",(0,s.jsx)(e.h3,{id:"llamacpp-1",children:"llama.cpp"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from langchain_community.llms import LlamaCpp\n\nllm = LlamaCpp(\n    model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",\n    n_ctx=4096,\n    n_gpu_layers=35,\n    temperature=0.7\n)\n\nresponse = llm.invoke("\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\uff1f")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"docker-\u90e8\u7f72",children:"Docker \u90e8\u7f72"}),"\n",(0,s.jsx)(e.h3,{id:"ollama-docker",children:"Ollama Docker"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:"# docker-compose.yml\nversion: '3.8'\nservices:\n  ollama:\n    image: ollama/ollama\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_data:/root/.ollama\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\nvolumes:\n  ollama_data:\n"})}),"\n",(0,s.jsx)(e.h3,{id:"vllm-docker",children:"vLLM Docker"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:"version: '3.8'\nservices:\n  vllm:\n    image: vllm/vllm-openai:latest\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ~/.cache/huggingface:/root/.cache/huggingface\n    command: >\n      --model Qwen/Qwen2.5-7B-Instruct\n      --host 0.0.0.0\n      --port 8000\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u786c\u4ef6\u8981\u6c42",children:"\u786c\u4ef6\u8981\u6c42"}),"\n",(0,s.jsx)(e.h3,{id:"gpu-\u663e\u5b58\u9700\u6c42\u63a8\u7406",children:"GPU \u663e\u5b58\u9700\u6c42\uff08\u63a8\u7406\uff09"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"\u6a21\u578b\u5927\u5c0f"}),(0,s.jsx)(e.th,{children:"FP16"}),(0,s.jsx)(e.th,{children:"INT8"}),(0,s.jsx)(e.th,{children:"INT4"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"7B"}),(0,s.jsx)(e.td,{children:"14GB"}),(0,s.jsx)(e.td,{children:"7GB"}),(0,s.jsx)(e.td,{children:"4GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"13B"}),(0,s.jsx)(e.td,{children:"26GB"}),(0,s.jsx)(e.td,{children:"13GB"}),(0,s.jsx)(e.td,{children:"7GB"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"70B"}),(0,s.jsx)(e.td,{children:"140GB"}),(0,s.jsx)(e.td,{children:"70GB"}),(0,s.jsx)(e.td,{children:"35GB"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"\u63a8\u8350\u914d\u7f6e",children:"\u63a8\u8350\u914d\u7f6e"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"\u573a\u666f"}),(0,s.jsx)(e.th,{children:"GPU"}),(0,s.jsx)(e.th,{children:"\u5185\u5b58"}),(0,s.jsx)(e.th,{children:"\u63a8\u8350\u6a21\u578b"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"\u4e2a\u4eba\u5f00\u53d1"}),(0,s.jsx)(e.td,{children:"RTX 3060 12GB"}),(0,s.jsx)(e.td,{children:"16GB"}),(0,s.jsx)(e.td,{children:"7B INT4"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"\u5c0f\u56e2\u961f"}),(0,s.jsx)(e.td,{children:"RTX 4090 24GB"}),(0,s.jsx)(e.td,{children:"32GB"}),(0,s.jsx)(e.td,{children:"7B-13B"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"\u751f\u4ea7\u73af\u5883"}),(0,s.jsx)(e.td,{children:"A100 40GB/80GB"}),(0,s.jsx)(e.td,{children:"64GB+"}),(0,s.jsx)(e.td,{children:"13B-70B"})]})]})]}),"\n",(0,s.jsx)(e.h2,{id:"\u6027\u80fd\u4f18\u5316",children:"\u6027\u80fd\u4f18\u5316"}),"\n",(0,s.jsx)(e.h3,{id:"1-\u6279\u5904\u7406",children:"1. \u6279\u5904\u7406"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# vLLM \u81ea\u52a8\u6279\u5904\u7406\n# \u591a\u4e2a\u8bf7\u6c42\u4f1a\u88ab\u5408\u5e76\u5904\u7406\uff0c\u63d0\u9ad8\u541e\u5410\u91cf\n"})}),"\n",(0,s.jsx)(e.h3,{id:"2-kv-cache-\u4f18\u5316",children:"2. KV Cache \u4f18\u5316"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# vLLM PagedAttention\nllm = LLM(\n    model="...",\n    gpu_memory_utilization=0.9,  # \u66f4\u591a\u5185\u5b58\u7528\u4e8e KV cache\n)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"3-\u6295\u673a\u89e3\u7801",children:"3. \u6295\u673a\u89e3\u7801"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u4f7f\u7528\u5c0f\u6a21\u578b\u52a0\u901f\u5927\u6a21\u578b\nllm = LLM(\n    model="large-model",\n    speculative_model="small-model",\n    num_speculative_tokens=5\n)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://ollama.com/",children:"Ollama \u5b98\u7f51"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://docs.vllm.ai/",children:"vLLM \u6587\u6863"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp GitHub"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://huggingface.co/docs/text-generation-inference",children:"HuggingFace TGI"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(o,{...n})}):o(n)}}}]);