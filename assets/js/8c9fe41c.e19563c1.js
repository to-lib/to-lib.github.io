"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[33778],{39700:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ai/streaming","title":"\ud83c\udf0a \u6d41\u5f0f\u5904\u7406","description":"\u6d41\u5f0f\u5904\u7406\u662f\u8ba9 LLM \u9010\u6b65\u8fd4\u56de\u751f\u6210\u5185\u5bb9\u7684\u6280\u672f\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u54cd\u5e94\u3002\u8fd9\u80fd\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u66f4\u5feb\u770b\u5230\u8f93\u51fa\u3002","source":"@site/docs/ai/streaming.md","sourceDirName":"ai","slug":"/ai/streaming","permalink":"/docs/ai/streaming","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/streaming.md","tags":[],"version":"current","sidebarPosition":18,"frontMatter":{"sidebar_position":18,"title":"\ud83c\udf0a \u6d41\u5f0f\u5904\u7406"}}');var a=t(22714),r=t(48885);const i={sidebar_position:18,title:"\ud83c\udf0a \u6d41\u5f0f\u5904\u7406"},c="\u6d41\u5f0f\u5904\u7406 (Streaming)",o={},l=[{value:"\u4e3a\u4ec0\u4e48\u9700\u8981\u6d41\u5f0f\u5904\u7406\uff1f",id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u6d41\u5f0f\u5904\u7406",level:2},{value:"OpenAI \u6d41\u5f0f API",id:"openai-\u6d41\u5f0f-api",level:2},{value:"\u57fa\u7840\u7528\u6cd5",id:"\u57fa\u7840\u7528\u6cd5",level:3},{value:"\u5f02\u6b65\u6d41\u5f0f",id:"\u5f02\u6b65\u6d41\u5f0f",level:3},{value:"\u6536\u96c6\u5b8c\u6574\u54cd\u5e94",id:"\u6536\u96c6\u5b8c\u6574\u54cd\u5e94",level:3},{value:"Anthropic \u6d41\u5f0f API",id:"anthropic-\u6d41\u5f0f-api",level:2},{value:"\u57fa\u7840\u7528\u6cd5",id:"\u57fa\u7840\u7528\u6cd5-1",level:3},{value:"\u4e8b\u4ef6\u5904\u7406",id:"\u4e8b\u4ef6\u5904\u7406",level:3},{value:"\u83b7\u53d6\u6700\u7ec8\u6d88\u606f",id:"\u83b7\u53d6\u6700\u7ec8\u6d88\u606f",level:3},{value:"\u6d41\u5f0f Function Calling",id:"\u6d41\u5f0f-function-calling",level:2},{value:"OpenAI \u6d41\u5f0f\u5de5\u5177\u8c03\u7528",id:"openai-\u6d41\u5f0f\u5de5\u5177\u8c03\u7528",level:3},{value:"Web \u5e94\u7528\u96c6\u6210",id:"web-\u5e94\u7528\u96c6\u6210",level:2},{value:"FastAPI SSE",id:"fastapi-sse",level:3},{value:"\u524d\u7aef JavaScript \u6d88\u8d39",id:"\u524d\u7aef-javascript-\u6d88\u8d39",level:3},{value:"React Hook",id:"react-hook",level:3},{value:"LangChain \u6d41\u5f0f\u5904\u7406",id:"langchain-\u6d41\u5f0f\u5904\u7406",level:2},{value:"\u57fa\u7840\u6d41\u5f0f",id:"\u57fa\u7840\u6d41\u5f0f",level:3},{value:"\u6d41\u5f0f Chain",id:"\u6d41\u5f0f-chain",level:3},{value:"\u5f02\u6b65\u6d41\u5f0f",id:"\u5f02\u6b65\u6d41\u5f0f-1",level:3},{value:"\u6d41\u5f0f\u4e8b\u4ef6",id:"\u6d41\u5f0f\u4e8b\u4ef6",level:3},{value:"\u6d41\u5f0f\u5904\u7406\u6700\u4f73\u5b9e\u8df5",id:"\u6d41\u5f0f\u5904\u7406\u6700\u4f73\u5b9e\u8df5",level:2},{value:"1. \u8d85\u65f6\u5904\u7406",id:"1-\u8d85\u65f6\u5904\u7406",level:3},{value:"2. \u9519\u8bef\u6062\u590d",id:"2-\u9519\u8bef\u6062\u590d",level:3},{value:"3. \u53d6\u6d88\u5904\u7406",id:"3-\u53d6\u6d88\u5904\u7406",level:3},{value:"4. \u7f13\u51b2\u5904\u7406",id:"4-\u7f13\u51b2\u5904\u7406",level:3},{value:"\u6027\u80fd\u4f18\u5316",id:"\u6027\u80fd\u4f18\u5316",level:2},{value:"1. \u8fde\u63a5\u590d\u7528",id:"1-\u8fde\u63a5\u590d\u7528",level:3},{value:"2. \u5e76\u53d1\u6d41\u5f0f\u8bf7\u6c42",id:"2-\u5e76\u53d1\u6d41\u5f0f\u8bf7\u6c42",level:3},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"\u6d41\u5f0f\u5904\u7406-streaming",children:"\u6d41\u5f0f\u5904\u7406 (Streaming)"})}),"\n",(0,a.jsx)(e.p,{children:"\u6d41\u5f0f\u5904\u7406\u662f\u8ba9 LLM \u9010\u6b65\u8fd4\u56de\u751f\u6210\u5185\u5bb9\u7684\u6280\u672f\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u54cd\u5e94\u3002\u8fd9\u80fd\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u66f4\u5feb\u770b\u5230\u8f93\u51fa\u3002"}),"\n",(0,a.jsx)(e.h2,{id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u6d41\u5f0f\u5904\u7406",children:"\u4e3a\u4ec0\u4e48\u9700\u8981\u6d41\u5f0f\u5904\u7406\uff1f"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"\u573a\u666f"}),(0,a.jsx)(e.th,{children:"\u975e\u6d41\u5f0f"}),(0,a.jsx)(e.th,{children:"\u6d41\u5f0f"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"\u9996\u5b57\u8282\u65f6\u95f4"}),(0,a.jsx)(e.td,{children:"\u7b49\u5f85\u5b8c\u6574\u751f\u6210"}),(0,a.jsx)(e.td,{children:"\u51e0\u767e\u6beb\u79d2"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"\u7528\u6237\u4f53\u9a8c"}),(0,a.jsx)(e.td,{children:"\u957f\u65f6\u95f4\u7b49\u5f85"}),(0,a.jsx)(e.td,{children:"\u5b9e\u65f6\u53cd\u9988"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"\u957f\u6587\u672c\u751f\u6210"}),(0,a.jsx)(e.td,{children:"\u53ef\u80fd\u8d85\u65f6"}),(0,a.jsx)(e.td,{children:"\u6301\u7eed\u8f93\u51fa"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"\u8d44\u6e90\u5229\u7528"}),(0,a.jsx)(e.td,{children:"\u4e00\u6b21\u6027\u52a0\u8f7d"}),(0,a.jsx)(e.td,{children:"\u6e10\u8fdb\u5f0f\u5904\u7406"})]})]})]}),"\n",(0,a.jsx)(e.h2,{id:"openai-\u6d41\u5f0f-api",children:"OpenAI \u6d41\u5f0f API"}),"\n",(0,a.jsx)(e.h3,{id:"\u57fa\u7840\u7528\u6cd5",children:"\u57fa\u7840\u7528\u6cd5"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[{"role": "user", "content": "\u5199\u4e00\u9996\u5173\u4e8e\u6625\u5929\u7684\u8bd7"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end="", flush=True)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u5f02\u6b65\u6d41\u5f0f",children:"\u5f02\u6b65\u6d41\u5f0f"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from openai import AsyncOpenAI\nimport asyncio\n\nclient = AsyncOpenAI()\n\nasync def stream_chat(prompt: str):\n    stream = await client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        stream=True\n    )\n    \n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end="", flush=True)\n\nasyncio.run(stream_chat("\u89e3\u91ca\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60"))\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u6536\u96c6\u5b8c\u6574\u54cd\u5e94",children:"\u6536\u96c6\u5b8c\u6574\u54cd\u5e94"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def stream_with_full_response(prompt: str) -> tuple[str, dict]:\n    """\u6d41\u5f0f\u8f93\u51fa\u540c\u65f6\u6536\u96c6\u5b8c\u6574\u54cd\u5e94"""\n    stream = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        stream=True,\n        stream_options={"include_usage": True}  # \u5305\u542b token \u4f7f\u7528\u91cf\n    )\n    \n    full_content = ""\n    usage = None\n    \n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content:\n            content = chunk.choices[0].delta.content\n            full_content += content\n            print(content, end="", flush=True)\n        \n        # \u6700\u540e\u4e00\u4e2a chunk \u5305\u542b usage\n        if chunk.usage:\n            usage = {\n                "prompt_tokens": chunk.usage.prompt_tokens,\n                "completion_tokens": chunk.usage.completion_tokens,\n                "total_tokens": chunk.usage.total_tokens\n            }\n    \n    print()  # \u6362\u884c\n    return full_content, usage\n\ncontent, usage = stream_with_full_response("\u4f60\u597d")\nprint(f"Token \u4f7f\u7528: {usage}")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"anthropic-\u6d41\u5f0f-api",children:"Anthropic \u6d41\u5f0f API"}),"\n",(0,a.jsx)(e.h3,{id:"\u57fa\u7840\u7528\u6cd5-1",children:"\u57fa\u7840\u7528\u6cd5"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import anthropic\n\nclient = anthropic.Anthropic()\n\nwith client.messages.stream(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    messages=[{"role": "user", "content": "\u5199\u4e00\u4e2a Python \u5feb\u901f\u6392\u5e8f"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end="", flush=True)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u4e8b\u4ef6\u5904\u7406",children:"\u4e8b\u4ef6\u5904\u7406"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'with client.messages.stream(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    messages=[{"role": "user", "content": "\u4f60\u597d"}]\n) as stream:\n    for event in stream:\n        if event.type == "content_block_delta":\n            print(event.delta.text, end="", flush=True)\n        elif event.type == "message_stop":\n            print("\\n[\u5b8c\u6210]")\n        elif event.type == "message_delta":\n            print(f"\\n[\u505c\u6b62\u539f\u56e0: {event.delta.stop_reason}]")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u83b7\u53d6\u6700\u7ec8\u6d88\u606f",children:"\u83b7\u53d6\u6700\u7ec8\u6d88\u606f"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'with client.messages.stream(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    messages=[{"role": "user", "content": "\u4f60\u597d"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end="", flush=True)\n    \n    # \u83b7\u53d6\u5b8c\u6574\u6d88\u606f\u5bf9\u8c61\n    final_message = stream.get_final_message()\n    print(f"\\nToken \u4f7f\u7528: {final_message.usage}")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"\u6d41\u5f0f-function-calling",children:"\u6d41\u5f0f Function Calling"}),"\n",(0,a.jsx)(e.h3,{id:"openai-\u6d41\u5f0f\u5de5\u5177\u8c03\u7528",children:"OpenAI \u6d41\u5f0f\u5de5\u5177\u8c03\u7528"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import json\n\ndef stream_with_tools(prompt: str, tools: list):\n    """\u6d41\u5f0f\u5904\u7406\u5e26\u5de5\u5177\u8c03\u7528"""\n    stream = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        tools=tools,\n        stream=True\n    )\n    \n    tool_calls = {}\n    content = ""\n    \n    for chunk in stream:\n        delta = chunk.choices[0].delta\n        \n        # \u5904\u7406\u6587\u672c\u5185\u5bb9\n        if delta.content:\n            content += delta.content\n            print(delta.content, end="", flush=True)\n        \n        # \u5904\u7406\u5de5\u5177\u8c03\u7528\n        if delta.tool_calls:\n            for tc in delta.tool_calls:\n                idx = tc.index\n                if idx not in tool_calls:\n                    tool_calls[idx] = {\n                        "id": tc.id,\n                        "name": tc.function.name if tc.function else "",\n                        "arguments": ""\n                    }\n                if tc.function and tc.function.arguments:\n                    tool_calls[idx]["arguments"] += tc.function.arguments\n    \n    return content, list(tool_calls.values())\n\n# \u4f7f\u7528\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "get_weather",\n        "description": "\u83b7\u53d6\u5929\u6c14",\n        "parameters": {\n            "type": "object",\n            "properties": {"city": {"type": "string"}},\n            "required": ["city"]\n        }\n    }\n}]\n\ncontent, tool_calls = stream_with_tools("\u5317\u4eac\u5929\u6c14\u600e\u4e48\u6837\uff1f", tools)\nif tool_calls:\n    print(f"\\n\u5de5\u5177\u8c03\u7528: {tool_calls}")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"web-\u5e94\u7528\u96c6\u6210",children:"Web \u5e94\u7528\u96c6\u6210"}),"\n",(0,a.jsx)(e.h3,{id:"fastapi-sse",children:"FastAPI SSE"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom openai import OpenAI\n\napp = FastAPI()\nclient = OpenAI()\n\nasync def generate_stream(prompt: str):\n    """\u751f\u6210 SSE \u6d41"""\n    stream = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        stream=True\n    )\n    \n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            content = chunk.choices[0].delta.content\n            # SSE \u683c\u5f0f\n            yield f"data: {json.dumps({\'content\': content})}\\n\\n"\n    \n    yield "data: [DONE]\\n\\n"\n\n@app.get("/chat/stream")\nasync def chat_stream(prompt: str):\n    return StreamingResponse(\n        generate_stream(prompt),\n        media_type="text/event-stream",\n        headers={\n            "Cache-Control": "no-cache",\n            "Connection": "keep-alive",\n        }\n    )\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u524d\u7aef-javascript-\u6d88\u8d39",children:"\u524d\u7aef JavaScript \u6d88\u8d39"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-javascript",children:"async function streamChat(prompt) {\n    const response = await fetch(`/chat/stream?prompt=${encodeURIComponent(prompt)}`);\n    const reader = response.body.getReader();\n    const decoder = new TextDecoder();\n    \n    while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        \n        const text = decoder.decode(value);\n        const lines = text.split('\\n');\n        \n        for (const line of lines) {\n            if (line.startsWith('data: ')) {\n                const data = line.slice(6);\n                if (data === '[DONE]') {\n                    console.log('Stream completed');\n                    return;\n                }\n                const parsed = JSON.parse(data);\n                // \u66f4\u65b0 UI\n                document.getElementById('output').textContent += parsed.content;\n            }\n        }\n    }\n}\n"})}),"\n",(0,a.jsx)(e.h3,{id:"react-hook",children:"React Hook"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-typescript",children:"import { useState, useCallback } from 'react';\n\nfunction useStreamChat() {\n    const [content, setContent] = useState('');\n    const [isLoading, setIsLoading] = useState(false);\n    \n    const sendMessage = useCallback(async (prompt: string) => {\n        setIsLoading(true);\n        setContent('');\n        \n        try {\n            const response = await fetch('/chat/stream', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ prompt })\n            });\n            \n            const reader = response.body?.getReader();\n            if (!reader) return;\n            \n            const decoder = new TextDecoder();\n            \n            while (true) {\n                const { done, value } = await reader.read();\n                if (done) break;\n                \n                const text = decoder.decode(value);\n                // \u89e3\u6790 SSE \u6570\u636e\n                const matches = text.matchAll(/data: ({.*?})\\n/g);\n                for (const match of matches) {\n                    const data = JSON.parse(match[1]);\n                    setContent(prev => prev + data.content);\n                }\n            }\n        } finally {\n            setIsLoading(false);\n        }\n    }, []);\n    \n    return { content, isLoading, sendMessage };\n}\n"})}),"\n",(0,a.jsx)(e.h2,{id:"langchain-\u6d41\u5f0f\u5904\u7406",children:"LangChain \u6d41\u5f0f\u5904\u7406"}),"\n",(0,a.jsx)(e.h3,{id:"\u57fa\u7840\u6d41\u5f0f",children:"\u57fa\u7840\u6d41\u5f0f"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nllm = ChatOpenAI(model="gpt-4o", streaming=True)\n\nfor chunk in llm.stream([HumanMessage(content="\u5199\u4e00\u9996\u8bd7")]):\n    print(chunk.content, end="", flush=True)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u6d41\u5f0f-chain",children:"\u6d41\u5f0f Chain"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_template("\u7528 {language} \u89e3\u91ca {topic}")\nchain = prompt | llm | StrOutputParser()\n\nfor chunk in chain.stream({"language": "\u7b80\u5355\u7684\u8bed\u8a00", "topic": "\u91cf\u5b50\u8ba1\u7b97"}):\n    print(chunk, end="", flush=True)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u5f02\u6b65\u6d41\u5f0f-1",children:"\u5f02\u6b65\u6d41\u5f0f"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'async def async_stream():\n    async for chunk in llm.astream([HumanMessage(content="\u4f60\u597d")]):\n        print(chunk.content, end="", flush=True)\n\nimport asyncio\nasyncio.run(async_stream())\n'})}),"\n",(0,a.jsx)(e.h3,{id:"\u6d41\u5f0f\u4e8b\u4ef6",children:"\u6d41\u5f0f\u4e8b\u4ef6"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'async def stream_events():\n    """\u83b7\u53d6\u8be6\u7ec6\u7684\u6d41\u5f0f\u4e8b\u4ef6"""\n    chain = prompt | llm | StrOutputParser()\n    \n    async for event in chain.astream_events(\n        {"language": "\u4e2d\u6587", "topic": "AI"},\n        version="v2"\n    ):\n        kind = event["event"]\n        \n        if kind == "on_chat_model_stream":\n            content = event["data"]["chunk"].content\n            if content:\n                print(content, end="", flush=True)\n        elif kind == "on_chain_end":\n            print("\\n[Chain \u5b8c\u6210]")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"\u6d41\u5f0f\u5904\u7406\u6700\u4f73\u5b9e\u8df5",children:"\u6d41\u5f0f\u5904\u7406\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,a.jsx)(e.h3,{id:"1-\u8d85\u65f6\u5904\u7406",children:"1. \u8d85\u65f6\u5904\u7406"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def stream_with_timeout(prompt: str, timeout: float = 30.0):\n    """\u5e26\u8d85\u65f6\u7684\u6d41\u5f0f\u5904\u7406"""\n    try:\n        stream = await asyncio.wait_for(\n            client.chat.completions.create(\n                model="gpt-4o",\n                messages=[{"role": "user", "content": prompt}],\n                stream=True\n            ),\n            timeout=timeout\n        )\n        \n        async for chunk in stream:\n            if chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n                \n    except asyncio.TimeoutError:\n        yield "\\n[\u8d85\u65f6]"\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-\u9519\u8bef\u6062\u590d",children:"2. \u9519\u8bef\u6062\u590d"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'async def resilient_stream(prompt: str, max_retries: int = 3):\n    """\u5e26\u91cd\u8bd5\u7684\u6d41\u5f0f\u5904\u7406"""\n    for attempt in range(max_retries):\n        try:\n            stream = await client.chat.completions.create(\n                model="gpt-4o",\n                messages=[{"role": "user", "content": prompt}],\n                stream=True\n            )\n            \n            async for chunk in stream:\n                if chunk.choices[0].delta.content:\n                    yield chunk.choices[0].delta.content\n            return\n            \n        except Exception as e:\n            if attempt == max_retries - 1:\n                yield f"\\n[\u9519\u8bef: {e}]"\n            else:\n                await asyncio.sleep(2 ** attempt)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"3-\u53d6\u6d88\u5904\u7406",children:"3. \u53d6\u6d88\u5904\u7406"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import asyncio\n\nasync def cancellable_stream(prompt: str):\n    """\u53ef\u53d6\u6d88\u7684\u6d41\u5f0f\u5904\u7406"""\n    stream = await client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        stream=True\n    )\n    \n    try:\n        async for chunk in stream:\n            if chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n    except asyncio.CancelledError:\n        print("\\n[\u5df2\u53d6\u6d88]")\n        raise\n\n# \u4f7f\u7528\nasync def main():\n    task = asyncio.create_task(consume_stream())\n    await asyncio.sleep(2)\n    task.cancel()  # 2\u79d2\u540e\u53d6\u6d88\n'})}),"\n",(0,a.jsx)(e.h3,{id:"4-\u7f13\u51b2\u5904\u7406",children:"4. \u7f13\u51b2\u5904\u7406"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'async def buffered_stream(prompt: str, buffer_size: int = 10):\n    """\u7f13\u51b2\u6d41\u5f0f\u8f93\u51fa\uff0c\u51cf\u5c11 UI \u66f4\u65b0\u9891\u7387"""\n    buffer = ""\n    \n    stream = await client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        stream=True\n    )\n    \n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            buffer += chunk.choices[0].delta.content\n            \n            if len(buffer) >= buffer_size:\n                yield buffer\n                buffer = ""\n    \n    if buffer:\n        yield buffer\n'})}),"\n",(0,a.jsx)(e.h2,{id:"\u6027\u80fd\u4f18\u5316",children:"\u6027\u80fd\u4f18\u5316"}),"\n",(0,a.jsx)(e.h3,{id:"1-\u8fde\u63a5\u590d\u7528",children:"1. \u8fde\u63a5\u590d\u7528"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from openai import OpenAI\nimport httpx\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49 HTTP \u5ba2\u6237\u7aef\nhttp_client = httpx.Client(\n    limits=httpx.Limits(max_keepalive_connections=10),\n    timeout=httpx.Timeout(60.0, connect=5.0)\n)\n\nclient = OpenAI(http_client=http_client)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"2-\u5e76\u53d1\u6d41\u5f0f\u8bf7\u6c42",children:"2. \u5e76\u53d1\u6d41\u5f0f\u8bf7\u6c42"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'async def parallel_streams(prompts: list[str]):\n    """\u5e76\u53d1\u5904\u7406\u591a\u4e2a\u6d41\u5f0f\u8bf7\u6c42"""\n    async def process_one(prompt: str, index: int):\n        result = ""\n        stream = await client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": prompt}],\n            stream=True\n        )\n        async for chunk in stream:\n            if chunk.choices[0].delta.content:\n                result += chunk.choices[0].delta.content\n        return index, result\n    \n    tasks = [process_one(p, i) for i, p in enumerate(prompts)]\n    results = await asyncio.gather(*tasks)\n    return dict(results)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://platform.openai.com/docs/api-reference/streaming",children:"OpenAI Streaming"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://docs.anthropic.com/claude/reference/messages-streaming",children:"Anthropic Streaming"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://python.langchain.com/docs/expression_language/streaming",children:"LangChain Streaming"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},48885:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>c});var s=t(99378);const a={},r=s.createContext(a);function i(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);