"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[4894],{17716:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"ai/rlhf","title":"\ud83c\udfaf RLHF \u4e0e DPO","description":"RLHF\uff08Reinforcement Learning from Human Feedback\uff09\u548c DPO\uff08Direct Preference Optimization\uff09\u662f\u8ba9 LLM \u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6838\u5fc3\u6280\u672f\u3002","source":"@site/docs/ai/rlhf.md","sourceDirName":"ai","slug":"/ai/rlhf","permalink":"/docs/ai/rlhf","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/rlhf.md","tags":[],"version":"current","sidebarPosition":37,"frontMatter":{"sidebar_position":37,"title":"\ud83c\udfaf RLHF \u4e0e DPO"}}');var s=r(22714),i=r(48885);const o={sidebar_position:37,title:"\ud83c\udfaf RLHF \u4e0e DPO"},d="RLHF \u4e0e DPO",l={},a=[{value:"\u4e3a\u4ec0\u4e48\u9700\u8981\u5bf9\u9f50\uff1f",id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u5bf9\u9f50",level:2},{value:"RLHF \u6d41\u7a0b",id:"rlhf-\u6d41\u7a0b",level:2},{value:"\u504f\u597d\u6570\u636e\u683c\u5f0f",id:"\u504f\u597d\u6570\u636e\u683c\u5f0f",level:2},{value:"\u5956\u52b1\u6a21\u578b\u8bad\u7ec3",id:"\u5956\u52b1\u6a21\u578b\u8bad\u7ec3",level:2},{value:"DPO\uff08Direct Preference Optimization\uff09",id:"dpodirect-preference-optimization",level:2},{value:"DPO \u539f\u7406",id:"dpo-\u539f\u7406",level:3},{value:"DPO \u635f\u5931\u51fd\u6570",id:"dpo-\u635f\u5931\u51fd\u6570",level:3},{value:"\u4f7f\u7528 TRL \u5e93\u8bad\u7ec3 DPO",id:"\u4f7f\u7528-trl-\u5e93\u8bad\u7ec3-dpo",level:3},{value:"ORPO\uff08\u65e0\u9700\u53c2\u8003\u6a21\u578b\uff09",id:"orpo\u65e0\u9700\u53c2\u8003\u6a21\u578b",level:2},{value:"\u504f\u597d\u6570\u636e\u6536\u96c6",id:"\u504f\u597d\u6570\u636e\u6536\u96c6",level:2},{value:"\u4eba\u5de5\u6807\u6ce8",id:"\u4eba\u5de5\u6807\u6ce8",level:3},{value:"AI \u8f85\u52a9\u6807\u6ce8",id:"ai-\u8f85\u52a9\u6807\u6ce8",level:3},{value:"\u65b9\u6cd5\u5bf9\u6bd4",id:"\u65b9\u6cd5\u5bf9\u6bd4",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"rlhf-\u4e0e-dpo",children:"RLHF \u4e0e DPO"})}),"\n",(0,s.jsx)(n.p,{children:"RLHF\uff08Reinforcement Learning from Human Feedback\uff09\u548c DPO\uff08Direct Preference Optimization\uff09\u662f\u8ba9 LLM \u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6838\u5fc3\u6280\u672f\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u5bf9\u9f50",children:"\u4e3a\u4ec0\u4e48\u9700\u8981\u5bf9\u9f50\uff1f"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u9884\u8bad\u7ec3\u6a21\u578b\uff1a\u9884\u6d4b\u4e0b\u4e00\u4e2a token\uff08\u53ef\u80fd\u751f\u6210\u6709\u5bb3/\u65e0\u7528\u5185\u5bb9\uff09\n     \u2502\n     \u25bc\n\u5bf9\u9f50\u540e\u6a21\u578b\uff1a\u751f\u6210\u6709\u5e2e\u52a9\u3001\u8bda\u5b9e\u3001\u65e0\u5bb3\u7684\u5185\u5bb9\n"})}),"\n",(0,s.jsx)(n.h2,{id:"rlhf-\u6d41\u7a0b",children:"RLHF \u6d41\u7a0b"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    RLHF \u4e09\u9636\u6bb5                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  \u9636\u6bb51: SFT\uff08\u76d1\u7763\u5fae\u8c03\uff09                                 \u2502\n\u2502  \u2514\u2500> \u7528\u9ad8\u8d28\u91cf\u6570\u636e\u5fae\u8c03\u57fa\u7840\u6a21\u578b                           \u2502\n\u2502                                                         \u2502\n\u2502  \u9636\u6bb52: \u8bad\u7ec3\u5956\u52b1\u6a21\u578b                                    \u2502\n\u2502  \u2514\u2500> \u4eba\u7c7b\u6807\u6ce8\u504f\u597d\u6570\u636e\uff0c\u8bad\u7ec3 RM                          \u2502\n\u2502                                                         \u2502\n\u2502  \u9636\u6bb53: PPO \u5f3a\u5316\u5b66\u4e60                                    \u2502\n\u2502  \u2514\u2500> \u7528 RM \u6307\u5bfc\u6a21\u578b\u4f18\u5316                                 \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"\u504f\u597d\u6570\u636e\u683c\u5f0f",children:"\u504f\u597d\u6570\u636e\u683c\u5f0f"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "prompt": "\u5199\u4e00\u9996\u5173\u4e8e\u6625\u5929\u7684\u8bd7",\n  "chosen": "\u6625\u98ce\u62c2\u9762\u6696\u610f\u6d53\uff0c\\n\u6843\u82b1\u6735\u6735\u6620\u65e5\u7ea2\u3002\\n\u71d5\u5b50\u5f52\u6765\u5bfb\u65e7\u5de2\uff0c\\n\u67f3\u7d6e\u98d8\u98d8\u821e\u4e1c\u98ce\u3002",\n  "rejected": "\u6625\u5929\u6765\u4e86\uff0c\u82b1\u5f00\u4e86\uff0c\u5f88\u6f02\u4eae\u3002"\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u5956\u52b1\u6a21\u578b\u8bad\u7ec3",children:"\u5956\u52b1\u6a21\u578b\u8bad\u7ec3"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n\nclass RewardModel(nn.Module):\n    """\u5956\u52b1\u6a21\u578b"""\n    \n    def __init__(self, model_name: str):\n        super().__init__()\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=1\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\ndef reward_loss(chosen_rewards, rejected_rewards):\n    """\u5956\u52b1\u6a21\u578b\u635f\u5931\u51fd\u6570"""\n    # \u5e0c\u671b chosen \u7684\u5956\u52b1\u9ad8\u4e8e rejected\n    return -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n\n# \u8bad\u7ec3\u5faa\u73af\ndef train_reward_model(model, dataloader, optimizer, epochs=3):\n    model.train()\n    \n    for epoch in range(epochs):\n        for batch in dataloader:\n            # \u8ba1\u7b97 chosen \u548c rejected \u7684\u5956\u52b1\n            chosen_rewards = model(\n                batch["chosen_input_ids"],\n                batch["chosen_attention_mask"]\n            )\n            rejected_rewards = model(\n                batch["rejected_input_ids"],\n                batch["rejected_attention_mask"]\n            )\n            \n            loss = reward_loss(chosen_rewards, rejected_rewards)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"dpodirect-preference-optimization",children:"DPO\uff08Direct Preference Optimization\uff09"}),"\n",(0,s.jsx)(n.p,{children:"DPO \u76f4\u63a5\u4ece\u504f\u597d\u6570\u636e\u4f18\u5316\u7b56\u7565\uff0c\u65e0\u9700\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002"}),"\n",(0,s.jsx)(n.h3,{id:"dpo-\u539f\u7406",children:"DPO \u539f\u7406"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"RLHF: \u504f\u597d\u6570\u636e \u2192 \u5956\u52b1\u6a21\u578b \u2192 PPO \u8bad\u7ec3 \u2192 \u5bf9\u9f50\u6a21\u578b\nDPO:  \u504f\u597d\u6570\u636e \u2192 \u76f4\u63a5\u4f18\u5316 \u2192 \u5bf9\u9f50\u6a21\u578b\uff08\u66f4\u7b80\u5355\uff01\uff09\n"})}),"\n",(0,s.jsx)(n.h3,{id:"dpo-\u635f\u5931\u51fd\u6570",children:"DPO \u635f\u5931\u51fd\u6570"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch.nn.functional as F\n\ndef dpo_loss(\n    policy_chosen_logps: torch.Tensor,\n    policy_rejected_logps: torch.Tensor,\n    reference_chosen_logps: torch.Tensor,\n    reference_rejected_logps: torch.Tensor,\n    beta: float = 0.1\n) -> torch.Tensor:\n    """DPO \u635f\u5931\u51fd\u6570"""\n    # \u8ba1\u7b97 log ratio\n    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n    \n    # DPO \u635f\u5931\n    logits = beta * (chosen_logratios - rejected_logratios)\n    loss = -F.logsigmoid(logits).mean()\n    \n    return loss\n'})}),"\n",(0,s.jsx)(n.h3,{id:"\u4f7f\u7528-trl-\u5e93\u8bad\u7ec3-dpo",children:"\u4f7f\u7528 TRL \u5e93\u8bad\u7ec3 DPO"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install trl\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from trl import DPOTrainer, DPOConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B")\nref_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B")\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B")\n\n# \u52a0\u8f7d\u504f\u597d\u6570\u636e\ndataset = load_dataset("json", data_files="preferences.jsonl")\n\n# DPO \u914d\u7f6e\ntraining_args = DPOConfig(\n    output_dir="./dpo_model",\n    beta=0.1,\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    learning_rate=5e-7,\n    logging_steps=10\n)\n\n# \u8bad\u7ec3\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=training_args,\n    train_dataset=dataset["train"],\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"orpo\u65e0\u9700\u53c2\u8003\u6a21\u578b",children:"ORPO\uff08\u65e0\u9700\u53c2\u8003\u6a21\u578b\uff09"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from trl import ORPOTrainer, ORPOConfig\n\n# ORPO \u4e0d\u9700\u8981\u53c2\u8003\u6a21\u578b\ntraining_args = ORPOConfig(\n    output_dir="./orpo_model",\n    beta=0.1,\n    per_device_train_batch_size=4,\n    num_train_epochs=3\n)\n\ntrainer = ORPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset["train"],\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u504f\u597d\u6570\u636e\u6536\u96c6",children:"\u504f\u597d\u6570\u636e\u6536\u96c6"}),"\n",(0,s.jsx)(n.h3,{id:"\u4eba\u5de5\u6807\u6ce8",children:"\u4eba\u5de5\u6807\u6ce8"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def collect_preferences(prompts: list, model) -> list:\n    """\u6536\u96c6\u4eba\u5de5\u504f\u597d\u6807\u6ce8"""\n    preferences = []\n    \n    for prompt in prompts:\n        # \u751f\u6210\u591a\u4e2a\u5019\u9009\u56de\u590d\n        responses = []\n        for _ in range(3):\n            response = model.generate(prompt, temperature=0.8)\n            responses.append(response)\n        \n        # \u4eba\u5de5\u9009\u62e9\u6700\u4f73\u548c\u6700\u5dee\n        print(f"Prompt: {prompt}")\n        for i, r in enumerate(responses):\n            print(f"{i}: {r}")\n        \n        chosen_idx = int(input("Best response: "))\n        rejected_idx = int(input("Worst response: "))\n        \n        preferences.append({\n            "prompt": prompt,\n            "chosen": responses[chosen_idx],\n            "rejected": responses[rejected_idx]\n        })\n    \n    return preferences\n'})}),"\n",(0,s.jsx)(n.h3,{id:"ai-\u8f85\u52a9\u6807\u6ce8",children:"AI \u8f85\u52a9\u6807\u6ce8"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\ndef ai_preference_labeling(prompt: str, response_a: str, response_b: str) -> dict:\n    """\u4f7f\u7528 GPT-4 \u8fdb\u884c\u504f\u597d\u6807\u6ce8"""\n    judge_prompt = f"""\n\u6bd4\u8f83\u4ee5\u4e0b\u4e24\u4e2a\u56de\u590d\uff0c\u9009\u62e9\u66f4\u597d\u7684\u4e00\u4e2a\u3002\n\n\u95ee\u9898\uff1a{prompt}\n\n\u56de\u590d A\uff1a{response_a}\n\n\u56de\u590d B\uff1a{response_b}\n\n\u54ea\u4e2a\u56de\u590d\u66f4\u597d\uff1f\u53ea\u56de\u7b54 "A" \u6216 "B"\u3002\n"""\n    \n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": judge_prompt}],\n        max_tokens=1\n    )\n    \n    choice = response.choices[0].message.content.strip()\n    \n    if choice == "A":\n        return {"prompt": prompt, "chosen": response_a, "rejected": response_b}\n    else:\n        return {"prompt": prompt, "chosen": response_b, "rejected": response_a}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u65b9\u6cd5\u5bf9\u6bd4",children:"\u65b9\u6cd5\u5bf9\u6bd4"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"\u65b9\u6cd5"}),(0,s.jsx)(n.th,{children:"\u590d\u6742\u5ea6"}),(0,s.jsx)(n.th,{children:"\u6548\u679c"}),(0,s.jsx)(n.th,{children:"\u9002\u7528\u573a\u666f"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RLHF"}),(0,s.jsx)(n.td,{children:"\u9ad8"}),(0,s.jsx)(n.td,{children:"\u6700\u597d"}),(0,s.jsx)(n.td,{children:"\u5927\u89c4\u6a21\u5bf9\u9f50"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DPO"}),(0,s.jsx)(n.td,{children:"\u4e2d"}),(0,s.jsx)(n.td,{children:"\u5f88\u597d"}),(0,s.jsx)(n.td,{children:"\u63a8\u8350\u9996\u9009"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ORPO"}),(0,s.jsx)(n.td,{children:"\u4f4e"}),(0,s.jsx)(n.td,{children:"\u597d"}),(0,s.jsx)(n.td,{children:"\u8d44\u6e90\u6709\u9650"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"KTO"}),(0,s.jsx)(n.td,{children:"\u4f4e"}),(0,s.jsx)(n.td,{children:"\u597d"}),(0,s.jsx)(n.td,{children:"\u53ea\u6709\u6b63/\u8d1f\u6837\u672c"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u6570\u636e\u8d28\u91cf\u4f18\u5148"}),"\uff1a\u504f\u597d\u6570\u636e\u8d28\u91cf\u51b3\u5b9a\u5bf9\u9f50\u6548\u679c"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u4ece DPO \u5f00\u59cb"}),"\uff1a\u6bd4 RLHF \u7b80\u5355\uff0c\u6548\u679c\u63a5\u8fd1"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u591a\u6837\u5316\u6570\u636e"}),"\uff1a\u8986\u76d6\u5404\u79cd\u573a\u666f\u548c\u8fb9\u754c\u60c5\u51b5"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u8fed\u4ee3\u4f18\u5316"}),"\uff1a\u591a\u8f6e\u5bf9\u9f50\u9010\u6b65\u63d0\u5347"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u8bc4\u4f30\u9a8c\u8bc1"}),"\uff1a\u7528\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u5bf9\u9f50\u6548\u679c"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2203.02155",children:"InstructGPT Paper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2305.18290",children:"DPO Paper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/huggingface/trl",children:"TRL Library"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},48885:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>d});var t=r(99378);const s={},i=t.createContext(s);function o(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);