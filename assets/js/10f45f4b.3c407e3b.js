"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[98513],{48568:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"ai/speculative-decoding","title":"\u26a1 Speculative Decoding","description":"Speculative Decoding \u662f\u4e00\u79cd\u52a0\u901f LLM \u63a8\u7406\u7684\u6280\u672f\uff0c\u4f7f\u7528\u5c0f\u6a21\u578b\u5feb\u901f\u751f\u6210\u5019\u9009 token\uff0c\u518d\u7531\u5927\u6a21\u578b\u9a8c\u8bc1\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u901f\u5ea6\u3002","source":"@site/docs/ai/speculative-decoding.md","sourceDirName":"ai","slug":"/ai/speculative-decoding","permalink":"/docs/ai/speculative-decoding","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/speculative-decoding.md","tags":[],"version":"current","sidebarPosition":39,"frontMatter":{"sidebar_position":39,"title":"\u26a1 Speculative Decoding"}}');var s=t(22714),d=t(48885);const a={sidebar_position:39,title:"\u26a1 Speculative Decoding"},i="Speculative Decoding\uff08\u63a8\u6d4b\u89e3\u7801\uff09",l={},o=[{value:"\u539f\u7406",id:"\u539f\u7406",level:2},{value:"\u4e3a\u4ec0\u4e48\u6709\u6548\uff1f",id:"\u4e3a\u4ec0\u4e48\u6709\u6548",level:2},{value:"\u57fa\u7840\u5b9e\u73b0",id:"\u57fa\u7840\u5b9e\u73b0",level:2},{value:"\u5e26\u91c7\u6837\u7684\u63a8\u6d4b\u89e3\u7801",id:"\u5e26\u91c7\u6837\u7684\u63a8\u6d4b\u89e3\u7801",level:2},{value:"vLLM \u63a8\u6d4b\u89e3\u7801",id:"vllm-\u63a8\u6d4b\u89e3\u7801",level:2},{value:"\u81ea\u63a8\u6d4b\u89e3\u7801\uff08Self-Speculative\uff09",id:"\u81ea\u63a8\u6d4b\u89e3\u7801self-speculative",level:2},{value:"Medusa\uff08\u591a\u5934\u63a8\u6d4b\uff09",id:"medusa\u591a\u5934\u63a8\u6d4b",level:2},{value:"\u52a0\u901f\u6548\u679c",id:"\u52a0\u901f\u6548\u679c",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"speculative-decoding\u63a8\u6d4b\u89e3\u7801",children:"Speculative Decoding\uff08\u63a8\u6d4b\u89e3\u7801\uff09"})}),"\n",(0,s.jsx)(n.p,{children:"Speculative Decoding \u662f\u4e00\u79cd\u52a0\u901f LLM \u63a8\u7406\u7684\u6280\u672f\uff0c\u4f7f\u7528\u5c0f\u6a21\u578b\u5feb\u901f\u751f\u6210\u5019\u9009 token\uff0c\u518d\u7531\u5927\u6a21\u578b\u9a8c\u8bc1\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u901f\u5ea6\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"\u539f\u7406",children:"\u539f\u7406"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u4f20\u7edf\u89e3\u7801\uff1a\n\u5927\u6a21\u578b \u2500\u2500> token1 \u2500\u2500> token2 \u2500\u2500> token3 \u2500\u2500> token4\n         (\u6162)      (\u6162)      (\u6162)      (\u6162)\n\n\u63a8\u6d4b\u89e3\u7801\uff1a\n\u5c0f\u6a21\u578b \u2500\u2500> [token1, token2, token3, token4] (\u5feb\u901f\u751f\u6210)\n              \u2502\n              \u25bc\n\u5927\u6a21\u578b \u2500\u2500> \u9a8c\u8bc1 \u2500\u2500> [\u2713token1, \u2713token2, \u2713token3, \u2717token4]\n              \u2502\n              \u25bc\n         \u63a5\u53d7\u524d3\u4e2a\uff0c\u4ece token4 \u91cd\u65b0\u751f\u6210\n"})}),"\n",(0,s.jsx)(n.h2,{id:"\u4e3a\u4ec0\u4e48\u6709\u6548",children:"\u4e3a\u4ec0\u4e48\u6709\u6548\uff1f"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u5927\u6a21\u578b\u63a8\u7406\u74f6\u9888\u5728\u5185\u5b58\u5e26\u5bbd\uff0c\u4e0d\u5728\u8ba1\u7b97"}),"\n",(0,s.jsx)(n.li,{children:"\u9a8c\u8bc1\u591a\u4e2a token \u7684\u6210\u672c \u2248 \u751f\u6210 1 \u4e2a token"}),"\n",(0,s.jsx)(n.li,{children:"\u5c0f\u6a21\u578b\u751f\u6210\u7684 token \u5927\u90e8\u5206\u80fd\u88ab\u5927\u6a21\u578b\u63a5\u53d7"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\u57fa\u7840\u5b9e\u73b0",children:"\u57fa\u7840\u5b9e\u73b0"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass SpeculativeDecoder:\n    """\u63a8\u6d4b\u89e3\u7801\u5668"""\n    \n    def __init__(\n        self,\n        target_model_name: str,  # \u5927\u6a21\u578b\n        draft_model_name: str,   # \u5c0f\u6a21\u578b\n        gamma: int = 4           # \u6bcf\u6b21\u63a8\u6d4b\u7684 token \u6570\n    ):\n        self.target = AutoModelForCausalLM.from_pretrained(target_model_name)\n        self.draft = AutoModelForCausalLM.from_pretrained(draft_model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n        self.gamma = gamma\n    \n    def generate(self, prompt: str, max_tokens: int = 100) -> str:\n        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")\n        \n        generated = input_ids.clone()\n        \n        while generated.shape[1] - input_ids.shape[1] < max_tokens:\n            # 1. \u5c0f\u6a21\u578b\u751f\u6210 gamma \u4e2a\u5019\u9009 token\n            draft_tokens = self._draft_generate(generated, self.gamma)\n            \n            # 2. \u5927\u6a21\u578b\u9a8c\u8bc1\n            accepted, next_token = self._verify(generated, draft_tokens)\n            \n            # 3. \u63a5\u53d7\u9a8c\u8bc1\u901a\u8fc7\u7684 token\n            generated = torch.cat([generated, accepted, next_token], dim=1)\n            \n            # \u68c0\u67e5\u662f\u5426\u751f\u6210\u4e86 EOS\n            if next_token.item() == self.tokenizer.eos_token_id:\n                break\n        \n        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n    \n    def _draft_generate(self, input_ids: torch.Tensor, num_tokens: int) -> torch.Tensor:\n        """\u5c0f\u6a21\u578b\u751f\u6210\u5019\u9009 token"""\n        draft_ids = input_ids.clone()\n        \n        for _ in range(num_tokens):\n            with torch.no_grad():\n                outputs = self.draft(draft_ids)\n                next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n                draft_ids = torch.cat([draft_ids, next_token], dim=1)\n        \n        # \u8fd4\u56de\u65b0\u751f\u6210\u7684 token\n        return draft_ids[:, input_ids.shape[1]:]\n    \n    def _verify(\n        self,\n        input_ids: torch.Tensor,\n        draft_tokens: torch.Tensor\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        """\u5927\u6a21\u578b\u9a8c\u8bc1\u5019\u9009 token"""\n        # \u62fc\u63a5\u8f93\u5165\u548c\u5019\u9009\n        full_ids = torch.cat([input_ids, draft_tokens], dim=1)\n        \n        with torch.no_grad():\n            outputs = self.target(full_ids)\n            logits = outputs.logits\n        \n        accepted = []\n        \n        for i in range(draft_tokens.shape[1]):\n            pos = input_ids.shape[1] + i - 1\n            target_probs = torch.softmax(logits[:, pos, :], dim=-1)\n            draft_token = draft_tokens[:, i]\n            \n            # \u68c0\u67e5\u5927\u6a21\u578b\u662f\u5426\u540c\u610f\u8fd9\u4e2a token\n            target_token = logits[:, pos, :].argmax(dim=-1)\n            \n            if target_token.item() == draft_token.item():\n                accepted.append(draft_token)\n            else:\n                # \u62d2\u7edd\uff0c\u8fd4\u56de\u5927\u6a21\u578b\u7684\u9009\u62e9\n                return (\n                    torch.stack(accepted, dim=1) if accepted else torch.tensor([[]]),\n                    target_token.unsqueeze(0)\n                )\n        \n        # \u5168\u90e8\u63a5\u53d7\uff0c\u751f\u6210\u4e0b\u4e00\u4e2a token\n        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n        return torch.stack(accepted, dim=1), next_token\n\n# \u4f7f\u7528\ndecoder = SpeculativeDecoder(\n    target_model_name="meta-llama/Llama-2-70b-hf",\n    draft_model_name="meta-llama/Llama-2-7b-hf",\n    gamma=4\n)\n\noutput = decoder.generate("\u5199\u4e00\u9996\u5173\u4e8e\u6625\u5929\u7684\u8bd7\uff1a")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u5e26\u91c7\u6837\u7684\u63a8\u6d4b\u89e3\u7801",children:"\u5e26\u91c7\u6837\u7684\u63a8\u6d4b\u89e3\u7801"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def speculative_sampling(\n    target_probs: torch.Tensor,\n    draft_probs: torch.Tensor,\n    draft_token: int\n) -> tuple[bool, int]:\n    """\u63a8\u6d4b\u91c7\u6837\uff08\u652f\u6301\u968f\u673a\u91c7\u6837\uff09"""\n    p = target_probs[draft_token].item()\n    q = draft_probs[draft_token].item()\n    \n    # \u4ee5 min(1, p/q) \u7684\u6982\u7387\u63a5\u53d7\n    if torch.rand(1).item() < min(1, p / q):\n        return True, draft_token\n    else:\n        # \u4ece\u4fee\u6b63\u5206\u5e03\u4e2d\u91c7\u6837\n        adjusted_probs = torch.clamp(target_probs - draft_probs, min=0)\n        adjusted_probs = adjusted_probs / adjusted_probs.sum()\n        new_token = torch.multinomial(adjusted_probs, 1).item()\n        return False, new_token\n'})}),"\n",(0,s.jsx)(n.h2,{id:"vllm-\u63a8\u6d4b\u89e3\u7801",children:"vLLM \u63a8\u6d4b\u89e3\u7801"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\n# vLLM \u5185\u7f6e\u63a8\u6d4b\u89e3\u7801\u652f\u6301\nllm = LLM(\n    model="meta-llama/Llama-2-70b-hf",\n    speculative_model="meta-llama/Llama-2-7b-hf",\n    num_speculative_tokens=5,\n    use_v2_block_manager=True\n)\n\nsampling_params = SamplingParams(temperature=0.7, max_tokens=256)\noutputs = llm.generate(["\u5199\u4e00\u9996\u8bd7"], sampling_params)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u81ea\u63a8\u6d4b\u89e3\u7801self-speculative",children:"\u81ea\u63a8\u6d4b\u89e3\u7801\uff08Self-Speculative\uff09"}),"\n",(0,s.jsx)(n.p,{children:"\u4f7f\u7528\u540c\u4e00\u6a21\u578b\u7684\u65e9\u671f\u5c42\u4f5c\u4e3a draft model\u3002"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SelfSpeculativeDecoder:\n    """\u81ea\u63a8\u6d4b\u89e3\u7801"""\n    \n    def __init__(self, model_name: str, draft_layers: int = 8):\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.draft_layers = draft_layers\n        self.total_layers = len(self.model.model.layers)\n    \n    def draft_forward(self, input_ids):\n        """\u53ea\u4f7f\u7528\u524d\u51e0\u5c42\u8fdb\u884c\u63a8\u6d4b"""\n        hidden = self.model.model.embed_tokens(input_ids)\n        \n        for i in range(self.draft_layers):\n            hidden = self.model.model.layers[i](hidden)[0]\n        \n        # \u76f4\u63a5\u7528 LM head \u9884\u6d4b\n        logits = self.model.lm_head(hidden)\n        return logits\n    \n    def full_forward(self, input_ids):\n        """\u5b8c\u6574\u524d\u5411\u4f20\u64ad"""\n        return self.model(input_ids).logits\n'})}),"\n",(0,s.jsx)(n.h2,{id:"medusa\u591a\u5934\u63a8\u6d4b",children:"Medusa\uff08\u591a\u5934\u63a8\u6d4b\uff09"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MedusaHead(torch.nn.Module):\n    """Medusa \u591a\u5934\u9884\u6d4b"""\n    \n    def __init__(self, hidden_size: int, vocab_size: int, num_heads: int = 4):\n        super().__init__()\n        self.heads = torch.nn.ModuleList([\n            torch.nn.Linear(hidden_size, vocab_size)\n            for _ in range(num_heads)\n        ])\n    \n    def forward(self, hidden_states):\n        # \u6bcf\u4e2a\u5934\u9884\u6d4b\u672a\u6765\u7684\u4e00\u4e2a token\n        return [head(hidden_states) for head in self.heads]\n\n# Medusa \u53ef\u4ee5\u4e00\u6b21\u9884\u6d4b\u591a\u4e2a\u672a\u6765 token\n# \u7136\u540e\u7528\u6811\u72b6\u9a8c\u8bc1\u627e\u5230\u6700\u957f\u7684\u6709\u6548\u5e8f\u5217\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u52a0\u901f\u6548\u679c",children:"\u52a0\u901f\u6548\u679c"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"\u914d\u7f6e"}),(0,s.jsx)(n.th,{children:"\u52a0\u901f\u6bd4"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Llama-70B + Llama-7B"}),(0,s.jsx)(n.td,{children:"2-3x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"\u81ea\u63a8\u6d4b\uff088\u5c42\uff09"}),(0,s.jsx)(n.td,{children:"1.5-2x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Medusa"}),(0,s.jsx)(n.td,{children:"2-3x"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u9009\u62e9\u5408\u9002\u7684 draft \u6a21\u578b"}),"\uff1a\u540c\u7cfb\u5217\u5c0f\u6a21\u578b\u6548\u679c\u6700\u597d"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u8c03\u6574 gamma"}),"\uff1a\u901a\u5e38 4-8 \u662f\u597d\u7684\u9009\u62e9"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u76d1\u63a7\u63a5\u53d7\u7387"}),"\uff1a\u63a5\u53d7\u7387\u592a\u4f4e\u8bf4\u660e draft \u6a21\u578b\u4e0d\u5339\u914d"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u6279\u5904\u7406\u4f18\u5316"}),"\uff1a\u63a8\u6d4b\u89e3\u7801\u5bf9\u6279\u5904\u7406\u4e0d\u592a\u53cb\u597d"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\u4f7f\u7528 vLLM"}),"\uff1a\u751f\u4ea7\u73af\u5883\u63a8\u8350\u4f7f\u7528 vLLM \u7684\u5b9e\u73b0"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2211.17192",children:"Speculative Decoding Paper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2401.10774",children:"Medusa Paper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/models/spec_decode.html",children:"vLLM Speculative Decoding"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},48885:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var r=t(99378);const s={},d=r.createContext(s);function a(e){const n=r.useContext(d);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(d.Provider,{value:n},e.children)}}}]);