"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[91711],{34836:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>f,frontMatter:()=>o,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"ai/memory","title":"\ud83e\udde0 \u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406","description":"\u5bf9\u8bdd\u8bb0\u5fc6\u662f\u8ba9 AI \u52a9\u624b\u80fd\u591f\u8bb0\u4f4f\u4e0a\u4e0b\u6587\u3001\u4fdd\u6301\u8fde\u8d2f\u5bf9\u8bdd\u7684\u5173\u952e\u6280\u672f\u3002\u672c\u6587\u4ecb\u7ecd\u5404\u79cd\u8bb0\u5fc6\u7ba1\u7406\u7b56\u7565\u3002","source":"@site/docs/ai/memory.md","sourceDirName":"ai","slug":"/ai/memory","permalink":"/docs/ai/memory","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/memory.md","tags":[],"version":"current","sidebarPosition":24,"frontMatter":{"sidebar_position":24,"title":"\ud83e\udde0 \u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406"},"sidebar":"ai","previous":{"title":"\ud83e\udd16 AI Agent (\u667a\u80fd\u4f53)","permalink":"/docs/ai/agent"},"next":{"title":"\ud83d\udd0c MCP (\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae)","permalink":"/docs/ai/mcp"}}');var r=s(22714),i=s(48885);const o={sidebar_position:24,title:"\ud83e\udde0 \u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406"},a="\u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406",l={},m=[{value:"\u8bb0\u5fc6\u7c7b\u578b",id:"\u8bb0\u5fc6\u7c7b\u578b",level:2},{value:"\u77ed\u671f\u8bb0\u5fc6\uff1a\u6d88\u606f\u5386\u53f2",id:"\u77ed\u671f\u8bb0\u5fc6\u6d88\u606f\u5386\u53f2",level:2},{value:"\u57fa\u7840\u5b9e\u73b0",id:"\u57fa\u7840\u5b9e\u73b0",level:3},{value:"\u7a97\u53e3\u8bb0\u5fc6\uff08\u9650\u5236\u6d88\u606f\u6570\u91cf\uff09",id:"\u7a97\u53e3\u8bb0\u5fc6\u9650\u5236\u6d88\u606f\u6570\u91cf",level:3},{value:"Token \u9650\u5236\u8bb0\u5fc6",id:"token-\u9650\u5236\u8bb0\u5fc6",level:3},{value:"\u6458\u8981\u8bb0\u5fc6",id:"\u6458\u8981\u8bb0\u5fc6",level:2},{value:"\u5411\u91cf\u8bb0\u5fc6",id:"\u5411\u91cf\u8bb0\u5fc6",level:2},{value:"\u5b9e\u4f53\u8bb0\u5fc6",id:"\u5b9e\u4f53\u8bb0\u5fc6",level:2},{value:"\u7ec4\u5408\u8bb0\u5fc6",id:"\u7ec4\u5408\u8bb0\u5fc6",level:2},{value:"LangChain \u8bb0\u5fc6",id:"langchain-\u8bb0\u5fc6",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"\u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406",children:"\u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406"})}),"\n",(0,r.jsx)(n.p,{children:"\u5bf9\u8bdd\u8bb0\u5fc6\u662f\u8ba9 AI \u52a9\u624b\u80fd\u591f\u8bb0\u4f4f\u4e0a\u4e0b\u6587\u3001\u4fdd\u6301\u8fde\u8d2f\u5bf9\u8bdd\u7684\u5173\u952e\u6280\u672f\u3002\u672c\u6587\u4ecb\u7ecd\u5404\u79cd\u8bb0\u5fc6\u7ba1\u7406\u7b56\u7565\u3002"}),"\n",(0,r.jsx)(n.h2,{id:"\u8bb0\u5fc6\u7c7b\u578b",children:"\u8bb0\u5fc6\u7c7b\u578b"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     \u8bb0\u5fc6\u7cfb\u7edf                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  \u77ed\u671f\u8bb0\u5fc6 (Short-term)                                  \u2502\n\u2502  \u2514\u2500> \u5f53\u524d\u5bf9\u8bdd\u7684\u6d88\u606f\u5386\u53f2                                 \u2502\n\u2502                                                         \u2502\n\u2502  \u957f\u671f\u8bb0\u5fc6 (Long-term)                                   \u2502\n\u2502  \u251c\u2500> \u5411\u91cf\u8bb0\u5fc6\uff1a\u8bed\u4e49\u68c0\u7d22\u5386\u53f2\u4fe1\u606f                         \u2502\n\u2502  \u251c\u2500> \u6458\u8981\u8bb0\u5fc6\uff1a\u538b\u7f29\u5386\u53f2\u5bf9\u8bdd                             \u2502\n\u2502  \u2514\u2500> \u5b9e\u4f53\u8bb0\u5fc6\uff1a\u8bb0\u4f4f\u5173\u952e\u5b9e\u4f53\u4fe1\u606f                         \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"\u77ed\u671f\u8bb0\u5fc6\u6d88\u606f\u5386\u53f2",children:"\u77ed\u671f\u8bb0\u5fc6\uff1a\u6d88\u606f\u5386\u53f2"}),"\n",(0,r.jsx)(n.h3,{id:"\u57fa\u7840\u5b9e\u73b0",children:"\u57fa\u7840\u5b9e\u73b0"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nfrom typing import List, Dict\n\nclass ConversationMemory:\n    """\u57fa\u7840\u5bf9\u8bdd\u8bb0\u5fc6"""\n    \n    def __init__(self, system_prompt: str = "\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002"):\n        self.client = OpenAI()\n        self.system_prompt = system_prompt\n        self.messages: List[Dict] = []\n    \n    def add_user_message(self, content: str):\n        self.messages.append({"role": "user", "content": content})\n    \n    def add_assistant_message(self, content: str):\n        self.messages.append({"role": "assistant", "content": content})\n    \n    def get_messages(self) -> List[Dict]:\n        return [{"role": "system", "content": self.system_prompt}] + self.messages\n    \n    def chat(self, user_input: str) -> str:\n        self.add_user_message(user_input)\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=self.get_messages()\n        )\n        \n        assistant_message = response.choices[0].message.content\n        self.add_assistant_message(assistant_message)\n        \n        return assistant_message\n    \n    def clear(self):\n        self.messages = []\n\n# \u4f7f\u7528\nmemory = ConversationMemory()\nprint(memory.chat("\u6211\u53eb\u5f20\u4e09"))\nprint(memory.chat("\u6211\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f"))  # \u80fd\u8bb0\u4f4f\u540d\u5b57\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u7a97\u53e3\u8bb0\u5fc6\u9650\u5236\u6d88\u606f\u6570\u91cf",children:"\u7a97\u53e3\u8bb0\u5fc6\uff08\u9650\u5236\u6d88\u606f\u6570\u91cf\uff09"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class WindowMemory:\n    """\u6ed1\u52a8\u7a97\u53e3\u8bb0\u5fc6"""\n    \n    def __init__(self, window_size: int = 10):\n        self.window_size = window_size\n        self.messages: List[Dict] = []\n        self.client = OpenAI()\n    \n    def add_message(self, role: str, content: str):\n        self.messages.append({"role": role, "content": content})\n        # \u4fdd\u6301\u7a97\u53e3\u5927\u5c0f\n        if len(self.messages) > self.window_size * 2:  # user + assistant\n            self.messages = self.messages[-self.window_size * 2:]\n    \n    def chat(self, user_input: str) -> str:\n        self.add_message("user", user_input)\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": "\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002"},\n                *self.messages\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        self.add_message("assistant", assistant_message)\n        \n        return assistant_message\n'})}),"\n",(0,r.jsx)(n.h3,{id:"token-\u9650\u5236\u8bb0\u5fc6",children:"Token \u9650\u5236\u8bb0\u5fc6"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import tiktoken\n\nclass TokenLimitMemory:\n    """\u57fa\u4e8e Token \u9650\u5236\u7684\u8bb0\u5fc6"""\n    \n    def __init__(self, max_tokens: int = 4000, model: str = "gpt-4o"):\n        self.max_tokens = max_tokens\n        self.model = model\n        self.messages: List[Dict] = []\n        self.client = OpenAI()\n        self.encoder = tiktoken.encoding_for_model(model)\n    \n    def _count_tokens(self, messages: List[Dict]) -> int:\n        """\u8ba1\u7b97\u6d88\u606f\u7684 token \u6570"""\n        total = 0\n        for msg in messages:\n            total += len(self.encoder.encode(msg["content"])) + 4  # \u89d2\u8272\u6807\u8bb0\n        return total\n    \n    def _trim_messages(self):\n        """\u88c1\u526a\u6d88\u606f\u4ee5\u7b26\u5408 token \u9650\u5236"""\n        while self._count_tokens(self.messages) > self.max_tokens and len(self.messages) > 2:\n            # \u4fdd\u7559\u6700\u65b0\u7684\u6d88\u606f\uff0c\u5220\u9664\u6700\u65e7\u7684\n            self.messages.pop(0)\n    \n    def chat(self, user_input: str) -> str:\n        self.messages.append({"role": "user", "content": user_input})\n        self._trim_messages()\n        \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": "\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002"},\n                *self.messages\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        self.messages.append({"role": "assistant", "content": assistant_message})\n        self._trim_messages()\n        \n        return assistant_message\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u6458\u8981\u8bb0\u5fc6",children:"\u6458\u8981\u8bb0\u5fc6"}),"\n",(0,r.jsx)(n.p,{children:"\u5f53\u5bf9\u8bdd\u8fc7\u957f\u65f6\uff0c\u81ea\u52a8\u751f\u6210\u6458\u8981\u6765\u538b\u7f29\u5386\u53f2\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class SummaryMemory:\n    """\u6458\u8981\u8bb0\u5fc6"""\n    \n    def __init__(self, summary_threshold: int = 10):\n        self.summary_threshold = summary_threshold\n        self.messages: List[Dict] = []\n        self.summary: str = ""\n        self.client = OpenAI()\n    \n    def _generate_summary(self) -> str:\n        """\u751f\u6210\u5bf9\u8bdd\u6458\u8981"""\n        conversation = "\\n".join([\n            f"{msg[\'role\']}: {msg[\'content\']}"\n            for msg in self.messages\n        ])\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {\n                    "role": "system",\n                    "content": "\u8bf7\u7b80\u6d01\u5730\u603b\u7ed3\u4ee5\u4e0b\u5bf9\u8bdd\u7684\u8981\u70b9\uff0c\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff08\u4eba\u540d\u3001\u6570\u5b57\u3001\u51b3\u5b9a\u7b49\uff09\u3002"\n                },\n                {"role": "user", "content": conversation}\n            ],\n            max_tokens=500\n        )\n        \n        return response.choices[0].message.content\n    \n    def _maybe_summarize(self):\n        """\u68c0\u67e5\u662f\u5426\u9700\u8981\u751f\u6210\u6458\u8981"""\n        if len(self.messages) >= self.summary_threshold:\n            # \u751f\u6210\u6458\u8981\n            new_summary = self._generate_summary()\n            \n            # \u5408\u5e76\u65e7\u6458\u8981\n            if self.summary:\n                self.summary = f"\u4e4b\u524d\u7684\u5bf9\u8bdd\u6458\u8981\uff1a{self.summary}\\n\\n\u6700\u8fd1\u7684\u5bf9\u8bdd\u6458\u8981\uff1a{new_summary}"\n            else:\n                self.summary = new_summary\n            \n            # \u6e05\u7a7a\u6d88\u606f\uff0c\u53ea\u4fdd\u7559\u6700\u8fd1\u51e0\u6761\n            self.messages = self.messages[-4:]\n    \n    def get_context(self) -> str:\n        """\u83b7\u53d6\u5b8c\u6574\u4e0a\u4e0b\u6587"""\n        context_parts = []\n        \n        if self.summary:\n            context_parts.append(f"\u5bf9\u8bdd\u5386\u53f2\u6458\u8981\uff1a\\n{self.summary}")\n        \n        if self.messages:\n            recent = "\\n".join([\n                f"{msg[\'role\']}: {msg[\'content\']}"\n                for msg in self.messages\n            ])\n            context_parts.append(f"\u6700\u8fd1\u7684\u5bf9\u8bdd\uff1a\\n{recent}")\n        \n        return "\\n\\n".join(context_parts)\n    \n    def chat(self, user_input: str) -> str:\n        self.messages.append({"role": "user", "content": user_input})\n        self._maybe_summarize()\n        \n        system_prompt = "\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002"\n        if self.summary:\n            system_prompt += f"\\n\\n{self.get_context()}"\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                *self.messages\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        self.messages.append({"role": "assistant", "content": assistant_message})\n        \n        return assistant_message\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u5411\u91cf\u8bb0\u5fc6",children:"\u5411\u91cf\u8bb0\u5fc6"}),"\n",(0,r.jsx)(n.p,{children:"\u4f7f\u7528\u5411\u91cf\u6570\u636e\u5e93\u5b58\u50a8\u548c\u68c0\u7d22\u5386\u53f2\u5bf9\u8bdd\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom datetime import datetime\nimport uuid\n\nclass VectorMemory:\n    """\u5411\u91cf\u8bb0\u5fc6"""\n    \n    def __init__(self, collection_name: str = "conversation_memory"):\n        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")\n        self.vectorstore = Chroma(\n            collection_name=collection_name,\n            embedding_function=self.embeddings,\n            persist_directory="./memory_db"\n        )\n        self.client = OpenAI()\n        self.session_id = str(uuid.uuid4())\n    \n    def _store_exchange(self, user_input: str, assistant_response: str):\n        """\u5b58\u50a8\u5bf9\u8bdd\u4ea4\u6362"""\n        exchange = f"\u7528\u6237: {user_input}\\n\u52a9\u624b: {assistant_response}"\n        \n        self.vectorstore.add_texts(\n            texts=[exchange],\n            metadatas=[{\n                "session_id": self.session_id,\n                "timestamp": datetime.now().isoformat(),\n                "user_input": user_input\n            }]\n        )\n    \n    def _retrieve_relevant(self, query: str, k: int = 3) -> List[str]:\n        """\u68c0\u7d22\u76f8\u5173\u5386\u53f2"""\n        docs = self.vectorstore.similarity_search(query, k=k)\n        return [doc.page_content for doc in docs]\n    \n    def chat(self, user_input: str) -> str:\n        # \u68c0\u7d22\u76f8\u5173\u5386\u53f2\n        relevant_history = self._retrieve_relevant(user_input)\n        \n        # \u6784\u5efa\u4e0a\u4e0b\u6587\n        context = ""\n        if relevant_history:\n            context = "\u76f8\u5173\u7684\u5386\u53f2\u5bf9\u8bdd\uff1a\\n" + "\\n---\\n".join(relevant_history)\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002{context}"\n                },\n                {"role": "user", "content": user_input}\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        \n        # \u5b58\u50a8\u8fd9\u6b21\u5bf9\u8bdd\n        self._store_exchange(user_input, assistant_message)\n        \n        return assistant_message\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u5b9e\u4f53\u8bb0\u5fc6",children:"\u5b9e\u4f53\u8bb0\u5fc6"}),"\n",(0,r.jsx)(n.p,{children:"\u8bb0\u4f4f\u5bf9\u8bdd\u4e2d\u63d0\u5230\u7684\u5173\u952e\u5b9e\u4f53\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from typing import Dict, Any\nimport json\n\nclass EntityMemory:\n    """\u5b9e\u4f53\u8bb0\u5fc6"""\n    \n    def __init__(self):\n        self.entities: Dict[str, Any] = {}\n        self.client = OpenAI()\n    \n    def _extract_entities(self, text: str) -> Dict[str, Any]:\n        """\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u5b9e\u4f53"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {\n                    "role": "system",\n                    "content": """\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u952e\u5b9e\u4f53\u4fe1\u606f\uff0c\u8fd4\u56de JSON \u683c\u5f0f\uff1a\n{\n  "\u4eba\u7269": {"\u59d3\u540d": "...", "\u7279\u5f81": "..."},\n  "\u5730\u70b9": ["..."],\n  "\u65f6\u95f4": ["..."],\n  "\u4e8b\u4ef6": ["..."],\n  "\u504f\u597d": {"...": "..."}\n}\n\u53ea\u8fd4\u56de JSON\uff0c\u4e0d\u8981\u5176\u4ed6\u5185\u5bb9\u3002\u5982\u679c\u6ca1\u6709\u76f8\u5173\u4fe1\u606f\uff0c\u5bf9\u5e94\u5b57\u6bb5\u4e3a\u7a7a\u3002"""\n                },\n                {"role": "user", "content": text}\n            ],\n            response_format={"type": "json_object"}\n        )\n        \n        try:\n            return json.loads(response.choices[0].message.content)\n        except:\n            return {}\n    \n    def _update_entities(self, new_entities: Dict[str, Any]):\n        """\u66f4\u65b0\u5b9e\u4f53\u5b58\u50a8"""\n        for key, value in new_entities.items():\n            if key not in self.entities:\n                self.entities[key] = value\n            elif isinstance(value, dict) and isinstance(self.entities[key], dict):\n                self.entities[key].update(value)\n            elif isinstance(value, list) and isinstance(self.entities[key], list):\n                self.entities[key].extend(value)\n                self.entities[key] = list(set(self.entities[key]))  # \u53bb\u91cd\n            else:\n                self.entities[key] = value\n    \n    def get_entity_context(self) -> str:\n        """\u83b7\u53d6\u5b9e\u4f53\u4e0a\u4e0b\u6587"""\n        if not self.entities:\n            return ""\n        return f"\u5df2\u77e5\u4fe1\u606f\uff1a\\n{json.dumps(self.entities, ensure_ascii=False, indent=2)}"\n    \n    def chat(self, user_input: str) -> str:\n        # \u63d0\u53d6\u5b9e\u4f53\n        new_entities = self._extract_entities(user_input)\n        self._update_entities(new_entities)\n        \n        # \u6784\u5efa\u4e0a\u4e0b\u6587\n        entity_context = self.get_entity_context()\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002\\n\\n{entity_context}"\n                },\n                {"role": "user", "content": user_input}\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        \n        # \u4ece\u56de\u590d\u4e2d\u4e5f\u63d0\u53d6\u5b9e\u4f53\n        response_entities = self._extract_entities(assistant_message)\n        self._update_entities(response_entities)\n        \n        return assistant_message\n\n# \u4f7f\u7528\nmemory = EntityMemory()\nprint(memory.chat("\u6211\u53eb\u5f20\u4e09\uff0c\u4eca\u5e74 30 \u5c81\uff0c\u4f4f\u5728\u5317\u4eac"))\nprint(memory.chat("\u6211\u559c\u6b22\u5403\u5ddd\u83dc"))\nprint(memory.chat("\u4f60\u8fd8\u8bb0\u5f97\u6211\u7684\u4fe1\u606f\u5417\uff1f"))\nprint(f"\u5b58\u50a8\u7684\u5b9e\u4f53\uff1a{memory.entities}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u7ec4\u5408\u8bb0\u5fc6",children:"\u7ec4\u5408\u8bb0\u5fc6"}),"\n",(0,r.jsx)(n.p,{children:"\u7ed3\u5408\u591a\u79cd\u8bb0\u5fc6\u7c7b\u578b\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class CombinedMemory:\n    """\u7ec4\u5408\u8bb0\u5fc6\u7cfb\u7edf"""\n    \n    def __init__(self):\n        self.short_term = []  # \u77ed\u671f\uff1a\u6700\u8fd1\u6d88\u606f\n        self.summary = ""     # \u4e2d\u671f\uff1a\u6458\u8981\n        self.entities = {}    # \u957f\u671f\uff1a\u5b9e\u4f53\n        self.vector_memory = VectorMemory()  # \u957f\u671f\uff1a\u5411\u91cf\u68c0\u7d22\n        self.client = OpenAI()\n        self.message_count = 0\n    \n    def _build_context(self, user_input: str) -> str:\n        """\u6784\u5efa\u5b8c\u6574\u4e0a\u4e0b\u6587"""\n        parts = []\n        \n        # \u5b9e\u4f53\u4fe1\u606f\n        if self.entities:\n            parts.append(f"\u7528\u6237\u4fe1\u606f\uff1a{json.dumps(self.entities, ensure_ascii=False)}")\n        \n        # \u5386\u53f2\u6458\u8981\n        if self.summary:\n            parts.append(f"\u5bf9\u8bdd\u6458\u8981\uff1a{self.summary}")\n        \n        # \u76f8\u5173\u5386\u53f2\uff08\u5411\u91cf\u68c0\u7d22\uff09\n        relevant = self.vector_memory._retrieve_relevant(user_input, k=2)\n        if relevant:\n            parts.append(f"\u76f8\u5173\u5386\u53f2\uff1a\\n" + "\\n".join(relevant))\n        \n        return "\\n\\n".join(parts)\n    \n    def chat(self, user_input: str) -> str:\n        self.message_count += 1\n        \n        # \u6784\u5efa\u4e0a\u4e0b\u6587\n        context = self._build_context(user_input)\n        \n        # \u77ed\u671f\u8bb0\u5fc6\n        self.short_term.append({"role": "user", "content": user_input})\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": f"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002\\n\\n{context}"},\n                *self.short_term[-10:]  # \u6700\u8fd1 10 \u6761\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        self.short_term.append({"role": "assistant", "content": assistant_message})\n        \n        # \u66f4\u65b0\u5404\u79cd\u8bb0\u5fc6\n        self._update_memories(user_input, assistant_message)\n        \n        return assistant_message\n    \n    def _update_memories(self, user_input: str, response: str):\n        """\u66f4\u65b0\u5404\u79cd\u8bb0\u5fc6"""\n        # \u5b58\u50a8\u5230\u5411\u91cf\u8bb0\u5fc6\n        self.vector_memory._store_exchange(user_input, response)\n        \n        # \u6bcf 10 \u8f6e\u66f4\u65b0\u6458\u8981\n        if self.message_count % 10 == 0:\n            self._update_summary()\n        \n        # \u63d0\u53d6\u5b9e\u4f53\n        self._extract_and_update_entities(user_input)\n    \n    def _update_summary(self):\n        """\u66f4\u65b0\u6458\u8981"""\n        if len(self.short_term) < 4:\n            return\n        \n        conversation = "\\n".join([\n            f"{m[\'role\']}: {m[\'content\']}" for m in self.short_term\n        ])\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {"role": "system", "content": "\u7b80\u6d01\u603b\u7ed3\u5bf9\u8bdd\u8981\u70b9\u3002"},\n                {"role": "user", "content": conversation}\n            ],\n            max_tokens=300\n        )\n        \n        new_summary = response.choices[0].message.content\n        \n        if self.summary:\n            self.summary = f"{self.summary}\\n\\n{new_summary}"\n        else:\n            self.summary = new_summary\n        \n        # \u6e05\u7406\u77ed\u671f\u8bb0\u5fc6\n        self.short_term = self.short_term[-4:]\n    \n    def _extract_and_update_entities(self, text: str):\n        """\u63d0\u53d6\u5e76\u66f4\u65b0\u5b9e\u4f53"""\n        # \u7b80\u5316\u7248\u5b9e\u4f53\u63d0\u53d6\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {\n                    "role": "system",\n                    "content": "\u63d0\u53d6\u6587\u672c\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff08\u59d3\u540d\u3001\u504f\u597d\u7b49\uff09\uff0c\u8fd4\u56de JSON\u3002"\n                },\n                {"role": "user", "content": text}\n            ],\n            response_format={"type": "json_object"}\n        )\n        \n        try:\n            new_entities = json.loads(response.choices[0].message.content)\n            self.entities.update(new_entities)\n        except:\n            pass\n'})}),"\n",(0,r.jsx)(n.h2,{id:"langchain-\u8bb0\u5fc6",children:"LangChain \u8bb0\u5fc6"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain.memory import (\n    ConversationBufferMemory,\n    ConversationBufferWindowMemory,\n    ConversationSummaryMemory,\n    ConversationSummaryBufferMemory,\n    VectorStoreRetrieverMemory\n)\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import ConversationChain\n\nllm = ChatOpenAI(model="gpt-4o")\n\n# \u7f13\u51b2\u8bb0\u5fc6\nbuffer_memory = ConversationBufferMemory()\n\n# \u7a97\u53e3\u8bb0\u5fc6\nwindow_memory = ConversationBufferWindowMemory(k=5)\n\n# \u6458\u8981\u8bb0\u5fc6\nsummary_memory = ConversationSummaryMemory(llm=llm)\n\n# \u6458\u8981\u7f13\u51b2\u8bb0\u5fc6\uff08\u7ed3\u5408\u4e24\u8005\uff09\nsummary_buffer_memory = ConversationSummaryBufferMemory(\n    llm=llm,\n    max_token_limit=2000\n)\n\n# \u4f7f\u7528\u8bb0\u5fc6\u7684\u5bf9\u8bdd\u94fe\nconversation = ConversationChain(\n    llm=llm,\n    memory=summary_buffer_memory,\n    verbose=True\n)\n\nresponse = conversation.predict(input="\u4f60\u597d\uff0c\u6211\u53eb\u5f20\u4e09")\nresponse = conversation.predict(input="\u6211\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u9009\u62e9\u5408\u9002\u7684\u8bb0\u5fc6\u7c7b\u578b"}),"\uff1a\u7b80\u5355\u573a\u666f\u7528\u7a97\u53e3\u8bb0\u5fc6\uff0c\u590d\u6742\u573a\u666f\u7528\u7ec4\u5408\u8bb0\u5fc6"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u63a7\u5236\u4e0a\u4e0b\u6587\u957f\u5ea6"}),"\uff1a\u907f\u514d\u8d85\u51fa\u6a21\u578b\u9650\u5236"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u5b9a\u671f\u6e05\u7406"}),"\uff1a\u9632\u6b62\u8bb0\u5fc6\u65e0\u9650\u589e\u957f"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u6301\u4e45\u5316\u5b58\u50a8"}),"\uff1a\u91cd\u8981\u4fe1\u606f\u5b58\u5165\u6570\u636e\u5e93"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u9690\u79c1\u4fdd\u62a4"}),"\uff1a\u654f\u611f\u4fe1\u606f\u8131\u654f\u5904\u7406"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/memory/",children:"LangChain Memory"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/",children:"LlamaIndex Chat Engine"})}),"\n"]})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},48885:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var t=s(99378);const r={},i=t.createContext(r);function o(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);