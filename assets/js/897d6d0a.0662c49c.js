"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[83760],{37841:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>i,default:()=>p,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ai/observability","title":"\ud83d\udcca AI \u53ef\u89c2\u6d4b\u6027","description":"AI \u53ef\u89c2\u6d4b\u6027\u662f\u6307\u5bf9 AI \u5e94\u7528\u8fdb\u884c\u76d1\u63a7\u3001\u8ffd\u8e2a\u548c\u8c03\u8bd5\u7684\u80fd\u529b\uff0c\u5e2e\u52a9\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u3001\u5b9a\u4f4d\u95ee\u9898\u3001\u4f18\u5316\u6027\u80fd\u3002","source":"@site/docs/ai/observability.md","sourceDirName":"ai","slug":"/ai/observability","permalink":"/docs/ai/observability","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/observability.md","tags":[],"version":"current","sidebarPosition":28,"frontMatter":{"sidebar_position":28,"title":"\ud83d\udcca AI \u53ef\u89c2\u6d4b\u6027"},"sidebar":"ai","previous":{"title":"\ud83d\ude80 Production\uff08\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72\uff09","permalink":"/docs/ai/production"},"next":{"title":"\ud83d\udcb0 \u6210\u672c\u4f18\u5316","permalink":"/docs/ai/cost-optimization"}}');var r=t(22714),a=t(48885);const l={sidebar_position:28,title:"\ud83d\udcca AI \u53ef\u89c2\u6d4b\u6027"},i="AI \u53ef\u89c2\u6d4b\u6027",o={},c=[{value:"\u53ef\u89c2\u6d4b\u6027\u4e09\u652f\u67f1",id:"\u53ef\u89c2\u6d4b\u6027\u4e09\u652f\u67f1",level:2},{value:"\u57fa\u7840\u5b9e\u73b0",id:"\u57fa\u7840\u5b9e\u73b0",level:2},{value:"\u7b80\u5355\u65e5\u5fd7",id:"\u7b80\u5355\u65e5\u5fd7",level:3},{value:"\u8ffd\u8e2a\u7cfb\u7edf",id:"\u8ffd\u8e2a\u7cfb\u7edf",level:3},{value:"LangSmith",id:"langsmith",level:2},{value:"\u914d\u7f6e",id:"\u914d\u7f6e",level:3},{value:"\u81ea\u52a8\u8ffd\u8e2a",id:"\u81ea\u52a8\u8ffd\u8e2a",level:3},{value:"\u624b\u52a8\u8ffd\u8e2a",id:"\u624b\u52a8\u8ffd\u8e2a",level:3},{value:"\u8bc4\u4f30",id:"\u8bc4\u4f30",level:3},{value:"OpenTelemetry \u96c6\u6210",id:"opentelemetry-\u96c6\u6210",level:2},{value:"\u6307\u6807\u6536\u96c6",id:"\u6307\u6807\u6536\u96c6",level:2},{value:"\u8d28\u91cf\u76d1\u63a7",id:"\u8d28\u91cf\u76d1\u63a7",level:2},{value:"\u544a\u8b66\u7cfb\u7edf",id:"\u544a\u8b66\u7cfb\u7edf",level:2},{value:"\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u5bf9\u6bd4",id:"\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u5bf9\u6bd4",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ai-\u53ef\u89c2\u6d4b\u6027",children:"AI \u53ef\u89c2\u6d4b\u6027"})}),"\n",(0,r.jsx)(n.p,{children:"AI \u53ef\u89c2\u6d4b\u6027\u662f\u6307\u5bf9 AI \u5e94\u7528\u8fdb\u884c\u76d1\u63a7\u3001\u8ffd\u8e2a\u548c\u8c03\u8bd5\u7684\u80fd\u529b\uff0c\u5e2e\u52a9\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u3001\u5b9a\u4f4d\u95ee\u9898\u3001\u4f18\u5316\u6027\u80fd\u3002"}),"\n",(0,r.jsx)(n.h2,{id:"\u53ef\u89c2\u6d4b\u6027\u4e09\u652f\u67f1",children:"\u53ef\u89c2\u6d4b\u6027\u4e09\u652f\u67f1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    AI \u53ef\u89c2\u6d4b\u6027                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  Tracing\uff08\u8ffd\u8e2a\uff09                                        \u2502\n\u2502  \u2514\u2500> \u5b8c\u6574\u8c03\u7528\u94fe\u8def\u3001\u6bcf\u6b65\u8017\u65f6\u3001Token \u4f7f\u7528                 \u2502\n\u2502                                                         \u2502\n\u2502  Logging\uff08\u65e5\u5fd7\uff09                                        \u2502\n\u2502  \u2514\u2500> \u8f93\u5165\u8f93\u51fa\u8bb0\u5f55\u3001\u9519\u8bef\u4fe1\u606f\u3001\u8c03\u8bd5\u6570\u636e                   \u2502\n\u2502                                                         \u2502\n\u2502  Metrics\uff08\u6307\u6807\uff09                                        \u2502\n\u2502  \u2514\u2500> \u5ef6\u8fdf\u3001\u6210\u529f\u7387\u3001\u6210\u672c\u3001\u8d28\u91cf\u8bc4\u5206                       \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"\u57fa\u7840\u5b9e\u73b0",children:"\u57fa\u7840\u5b9e\u73b0"}),"\n",(0,r.jsx)(n.h3,{id:"\u7b80\u5355\u65e5\u5fd7",children:"\u7b80\u5355\u65e5\u5fd7"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nimport json\nfrom datetime import datetime\nfrom functools import wraps\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger("ai_app")\n\ndef log_llm_call(func):\n    """LLM \u8c03\u7528\u65e5\u5fd7\u88c5\u9970\u5668"""\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = datetime.now()\n        \n        # \u8bb0\u5f55\u8f93\u5165\n        logger.info(json.dumps({\n            "event": "llm_call_start",\n            "function": func.__name__,\n            "timestamp": start_time.isoformat(),\n            "kwargs": {k: str(v)[:200] for k, v in kwargs.items()}\n        }, ensure_ascii=False))\n        \n        try:\n            result = func(*args, **kwargs)\n            duration = (datetime.now() - start_time).total_seconds()\n            \n            # \u8bb0\u5f55\u6210\u529f\n            logger.info(json.dumps({\n                "event": "llm_call_success",\n                "function": func.__name__,\n                "duration_seconds": duration,\n                "output_preview": str(result)[:200]\n            }, ensure_ascii=False))\n            \n            return result\n            \n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            \n            # \u8bb0\u5f55\u9519\u8bef\n            logger.error(json.dumps({\n                "event": "llm_call_error",\n                "function": func.__name__,\n                "duration_seconds": duration,\n                "error": str(e)\n            }, ensure_ascii=False))\n            \n            raise\n    \n    return wrapper\n\n@log_llm_call\ndef chat(message: str) -> str:\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": message}]\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u8ffd\u8e2a\u7cfb\u7edf",children:"\u8ffd\u8e2a\u7cfb\u7edf"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import uuid\nfrom dataclasses import dataclass, field\nfrom typing import Any\nfrom contextlib import contextmanager\n\n@dataclass\nclass Span:\n    """\u8ffd\u8e2a Span"""\n    name: str\n    trace_id: str\n    span_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n    parent_id: str | None = None\n    start_time: datetime = field(default_factory=datetime.now)\n    end_time: datetime | None = None\n    attributes: dict = field(default_factory=dict)\n    events: list = field(default_factory=list)\n    \n    def set_attribute(self, key: str, value: Any):\n        self.attributes[key] = value\n    \n    def add_event(self, name: str, attributes: dict = None):\n        self.events.append({\n            "name": name,\n            "timestamp": datetime.now().isoformat(),\n            "attributes": attributes or {}\n        })\n    \n    def end(self):\n        self.end_time = datetime.now()\n    \n    @property\n    def duration_ms(self) -> float:\n        if self.end_time:\n            return (self.end_time - self.start_time).total_seconds() * 1000\n        return 0\n\nclass Tracer:\n    """\u7b80\u5355\u8ffd\u8e2a\u5668"""\n    \n    def __init__(self):\n        self.spans: list[Span] = []\n        self._current_trace_id: str | None = None\n        self._current_span: Span | None = None\n    \n    @contextmanager\n    def start_trace(self, name: str):\n        """\u5f00\u59cb\u65b0\u7684\u8ffd\u8e2a"""\n        self._current_trace_id = str(uuid.uuid4())\n        span = Span(name=name, trace_id=self._current_trace_id)\n        self._current_span = span\n        \n        try:\n            yield span\n        finally:\n            span.end()\n            self.spans.append(span)\n            self._current_span = None\n    \n    @contextmanager\n    def start_span(self, name: str):\n        """\u5f00\u59cb\u5b50 Span"""\n        parent = self._current_span\n        span = Span(\n            name=name,\n            trace_id=self._current_trace_id,\n            parent_id=parent.span_id if parent else None\n        )\n        self._current_span = span\n        \n        try:\n            yield span\n        finally:\n            span.end()\n            self.spans.append(span)\n            self._current_span = parent\n    \n    def export(self) -> list[dict]:\n        """\u5bfc\u51fa\u8ffd\u8e2a\u6570\u636e"""\n        return [\n            {\n                "name": s.name,\n                "trace_id": s.trace_id,\n                "span_id": s.span_id,\n                "parent_id": s.parent_id,\n                "duration_ms": s.duration_ms,\n                "attributes": s.attributes,\n                "events": s.events\n            }\n            for s in self.spans\n        ]\n\ntracer = Tracer()\n\n# \u4f7f\u7528\u793a\u4f8b\ndef rag_query(question: str) -> str:\n    with tracer.start_trace("rag_query") as trace:\n        trace.set_attribute("question", question)\n        \n        # \u68c0\u7d22\n        with tracer.start_span("retrieval") as span:\n            docs = retrieve_documents(question)\n            span.set_attribute("num_docs", len(docs))\n        \n        # \u751f\u6210\n        with tracer.start_span("generation") as span:\n            response = generate_answer(question, docs)\n            span.set_attribute("response_length", len(response))\n        \n        return response\n'})}),"\n",(0,r.jsx)(n.h2,{id:"langsmith",children:"LangSmith"}),"\n",(0,r.jsx)(n.p,{children:"LangSmith \u662f LangChain \u5b98\u65b9\u7684\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u3002"}),"\n",(0,r.jsx)(n.h3,{id:"\u914d\u7f6e",children:"\u914d\u7f6e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install langsmith\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=your_api_key\nexport LANGCHAIN_PROJECT=my_project\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\u81ea\u52a8\u8ffd\u8e2a",children:"\u81ea\u52a8\u8ffd\u8e2a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# \u914d\u7f6e\u540e\u81ea\u52a8\u8ffd\u8e2a\u6240\u6709 LangChain \u8c03\u7528\nllm = ChatOpenAI(model="gpt-4o")\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002"),\n    ("user", "{input}")\n])\n\nchain = prompt | llm\nresponse = chain.invoke({"input": "\u4f60\u597d"})\n# \u81ea\u52a8\u8bb0\u5f55\u5230 LangSmith\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u624b\u52a8\u8ffd\u8e2a",children:"\u624b\u52a8\u8ffd\u8e2a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langsmith import traceable\n\n@traceable(name="my_function")\ndef process_query(query: str) -> str:\n    # \u51fd\u6570\u5185\u7684\u6240\u6709 LLM \u8c03\u7528\u90fd\u4f1a\u88ab\u8ffd\u8e2a\n    response = llm.invoke(query)\n    return response.content\n\n@traceable(run_type="retriever")\ndef search_documents(query: str) -> list:\n    # \u6807\u8bb0\u4e3a\u68c0\u7d22\u5668\u7c7b\u578b\n    return vector_store.similarity_search(query)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u8bc4\u4f30",children:"\u8bc4\u4f30"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langsmith import Client\nfrom langsmith.evaluation import evaluate\n\nclient = Client()\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = client.create_dataset("qa_dataset")\nclient.create_examples(\n    inputs=[{"question": "\u4ec0\u4e48\u662f RAG\uff1f"}],\n    outputs=[{"answer": "RAG \u662f\u68c0\u7d22\u589e\u5f3a\u751f\u6210..."}],\n    dataset_id=dataset.id\n)\n\n# \u5b9a\u4e49\u8bc4\u4f30\u51fd\u6570\ndef correctness(run, example):\n    # \u6bd4\u8f83\u9884\u6d4b\u548c\u53c2\u8003\u7b54\u6848\n    prediction = run.outputs["output"]\n    reference = example.outputs["answer"]\n    # \u8fd4\u56de\u8bc4\u5206\n    return {"score": 0.8}\n\n# \u8fd0\u884c\u8bc4\u4f30\nresults = evaluate(\n    lambda x: chain.invoke(x),\n    data=dataset.name,\n    evaluators=[correctness]\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"opentelemetry-\u96c6\u6210",children:"OpenTelemetry \u96c6\u6210"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\n# \u914d\u7f6e OpenTelemetry\nprovider = TracerProvider()\nprocessor = BatchSpanProcessor(OTLPSpanExporter(endpoint="http://localhost:4317"))\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n\ntracer = trace.get_tracer(__name__)\n\ndef llm_call_with_otel(prompt: str) -> str:\n    with tracer.start_as_current_span("llm_call") as span:\n        span.set_attribute("prompt", prompt[:100])\n        span.set_attribute("model", "gpt-4o")\n        \n        response = client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": prompt}]\n        )\n        \n        result = response.choices[0].message.content\n        span.set_attribute("response_length", len(result))\n        span.set_attribute("tokens_used", response.usage.total_tokens)\n        \n        return result\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u6307\u6807\u6536\u96c6",children:"\u6307\u6807\u6536\u96c6"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass, field\nfrom collections import defaultdict\nimport time\n\n@dataclass\nclass MetricsCollector:\n    """\u6307\u6807\u6536\u96c6\u5668"""\n    \n    latencies: list = field(default_factory=list)\n    token_counts: list = field(default_factory=list)\n    error_counts: dict = field(default_factory=lambda: defaultdict(int))\n    success_count: int = 0\n    total_cost: float = 0.0\n    \n    def record_latency(self, latency_ms: float):\n        self.latencies.append(latency_ms)\n    \n    def record_tokens(self, input_tokens: int, output_tokens: int):\n        self.token_counts.append({\n            "input": input_tokens,\n            "output": output_tokens\n        })\n    \n    def record_error(self, error_type: str):\n        self.error_counts[error_type] += 1\n    \n    def record_success(self):\n        self.success_count += 1\n    \n    def record_cost(self, cost: float):\n        self.total_cost += cost\n    \n    def get_stats(self) -> dict:\n        total_requests = self.success_count + sum(self.error_counts.values())\n        \n        return {\n            "total_requests": total_requests,\n            "success_rate": self.success_count / total_requests if total_requests > 0 else 0,\n            "avg_latency_ms": sum(self.latencies) / len(self.latencies) if self.latencies else 0,\n            "p95_latency_ms": sorted(self.latencies)[int(len(self.latencies) * 0.95)] if self.latencies else 0,\n            "total_tokens": sum(t["input"] + t["output"] for t in self.token_counts),\n            "total_cost": self.total_cost,\n            "error_breakdown": dict(self.error_counts)\n        }\n\nmetrics = MetricsCollector()\n\ndef monitored_chat(message: str) -> str:\n    start = time.time()\n    \n    try:\n        response = client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": message}]\n        )\n        \n        latency = (time.time() - start) * 1000\n        metrics.record_latency(latency)\n        metrics.record_tokens(\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens\n        )\n        metrics.record_success()\n        \n        return response.choices[0].message.content\n        \n    except Exception as e:\n        metrics.record_error(type(e).__name__)\n        raise\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u8d28\u91cf\u76d1\u63a7",children:"\u8d28\u91cf\u76d1\u63a7"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class QualityMonitor:\n    """\u8d28\u91cf\u76d1\u63a7"""\n    \n    def __init__(self):\n        self.client = OpenAI()\n        self.scores = []\n    \n    def evaluate_response(\n        self,\n        question: str,\n        response: str,\n        context: str = ""\n    ) -> dict:\n        """\u8bc4\u4f30\u56de\u590d\u8d28\u91cf"""\n        eval_prompt = f"""\n\u8bc4\u4f30\u4ee5\u4e0b AI \u56de\u590d\u7684\u8d28\u91cf\uff081-5 \u5206\uff09\uff1a\n\n\u95ee\u9898\uff1a{question}\n{"\u4e0a\u4e0b\u6587\uff1a" + context if context else ""}\n\u56de\u590d\uff1a{response}\n\n\u8bc4\u4f30\u7ef4\u5ea6\uff1a\n1. \u76f8\u5173\u6027\uff1a\u56de\u590d\u662f\u5426\u5207\u9898\n2. \u51c6\u786e\u6027\uff1a\u4fe1\u606f\u662f\u5426\u6b63\u786e\n3. \u5b8c\u6574\u6027\uff1a\u662f\u5426\u5b8c\u6574\u56de\u7b54\u95ee\u9898\n4. \u6e05\u6670\u5ea6\uff1a\u8868\u8fbe\u662f\u5426\u6e05\u6670\n\n\u8fd4\u56de JSON\uff1a{{"relevance": 1-5, "accuracy": 1-5, "completeness": 1-5, "clarity": 1-5}}\n"""\n        \n        eval_response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[{"role": "user", "content": eval_prompt}],\n            response_format={"type": "json_object"}\n        )\n        \n        scores = json.loads(eval_response.choices[0].message.content)\n        scores["overall"] = sum(scores.values()) / len(scores)\n        self.scores.append(scores)\n        \n        return scores\n    \n    def get_average_scores(self) -> dict:\n        if not self.scores:\n            return {}\n        \n        keys = self.scores[0].keys()\n        return {\n            k: sum(s[k] for s in self.scores) / len(self.scores)\n            for k in keys\n        }\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u544a\u8b66\u7cfb\u7edf",children:"\u544a\u8b66\u7cfb\u7edf"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from enum import Enum\n\nclass AlertLevel(Enum):\n    INFO = "info"\n    WARNING = "warning"\n    CRITICAL = "critical"\n\nclass AlertSystem:\n    """\u544a\u8b66\u7cfb\u7edf"""\n    \n    def __init__(self):\n        self.thresholds = {\n            "latency_p95_ms": 5000,\n            "error_rate": 0.05,\n            "cost_daily": 100.0\n        }\n        self.handlers = []\n    \n    def add_handler(self, handler):\n        self.handlers.append(handler)\n    \n    def check(self, metrics: dict):\n        """\u68c0\u67e5\u6307\u6807\u5e76\u89e6\u53d1\u544a\u8b66"""\n        alerts = []\n        \n        # \u5ef6\u8fdf\u544a\u8b66\n        if metrics.get("p95_latency_ms", 0) > self.thresholds["latency_p95_ms"]:\n            alerts.append({\n                "level": AlertLevel.WARNING,\n                "message": f"P95 \u5ef6\u8fdf\u8fc7\u9ad8\uff1a{metrics[\'p95_latency_ms\']:.0f}ms"\n            })\n        \n        # \u9519\u8bef\u7387\u544a\u8b66\n        error_rate = 1 - metrics.get("success_rate", 1)\n        if error_rate > self.thresholds["error_rate"]:\n            alerts.append({\n                "level": AlertLevel.CRITICAL,\n                "message": f"\u9519\u8bef\u7387\u8fc7\u9ad8\uff1a{error_rate:.2%}"\n            })\n        \n        # \u6210\u672c\u544a\u8b66\n        if metrics.get("total_cost", 0) > self.thresholds["cost_daily"]:\n            alerts.append({\n                "level": AlertLevel.WARNING,\n                "message": f"\u65e5\u6210\u672c\u8d85\u9650\uff1a${metrics[\'total_cost\']:.2f}"\n            })\n        \n        # \u89e6\u53d1\u5904\u7406\u5668\n        for alert in alerts:\n            for handler in self.handlers:\n                handler(alert)\n        \n        return alerts\n\n# \u4f7f\u7528\nalert_system = AlertSystem()\nalert_system.add_handler(lambda a: print(f"[{a[\'level\'].value}] {a[\'message\']}"))\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u5bf9\u6bd4",children:"\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u5bf9\u6bd4"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"\u5e73\u53f0"}),(0,r.jsx)(n.th,{children:"\u7279\u70b9"}),(0,r.jsx)(n.th,{children:"\u9002\u7528\u573a\u666f"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LangSmith"}),(0,r.jsx)(n.td,{children:"LangChain \u539f\u751f\u652f\u6301"}),(0,r.jsx)(n.td,{children:"LangChain \u9879\u76ee"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Arize"}),(0,r.jsx)(n.td,{children:"\u5f3a\u5927\u7684\u6570\u636e\u5206\u6790"}),(0,r.jsx)(n.td,{children:"\u751f\u4ea7\u73af\u5883\u76d1\u63a7"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Weights & Biases"}),(0,r.jsx)(n.td,{children:"ML \u5b9e\u9a8c\u8ffd\u8e2a"}),(0,r.jsx)(n.td,{children:"\u6a21\u578b\u8bad\u7ec3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Helicone"}),(0,r.jsx)(n.td,{children:"\u7b80\u5355\u6613\u7528"}),(0,r.jsx)(n.td,{children:"\u5feb\u901f\u96c6\u6210"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"OpenTelemetry"}),(0,r.jsx)(n.td,{children:"\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55"}),(0,r.jsx)(n.td,{children:"\u4f01\u4e1a\u7ea7\u5e94\u7528"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u5168\u94fe\u8def\u8ffd\u8e2a"}),"\uff1a\u8bb0\u5f55\u4ece\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5b8c\u6574\u94fe\u8def"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u91c7\u6837\u7b56\u7565"}),"\uff1a\u9ad8\u6d41\u91cf\u65f6\u4f7f\u7528\u91c7\u6837\u51cf\u5c11\u5f00\u9500"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u654f\u611f\u4fe1\u606f\u8131\u654f"}),"\uff1a\u65e5\u5fd7\u4e2d\u4e0d\u8bb0\u5f55\u654f\u611f\u6570\u636e"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u5b9e\u65f6\u544a\u8b66"}),"\uff1a\u5173\u952e\u6307\u6807\u5f02\u5e38\u65f6\u53ca\u65f6\u901a\u77e5"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u5b9a\u671f\u56de\u987e"}),"\uff1a\u5206\u6790\u5386\u53f2\u6570\u636e\u4f18\u5316\u7cfb\u7edf"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.smith.langchain.com/",children:"LangSmith \u6587\u6863"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://opentelemetry.io/",children:"OpenTelemetry"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arize.com/",children:"Arize AI"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},48885:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>i});var s=t(99378);const r={},a=s.createContext(r);function l(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);