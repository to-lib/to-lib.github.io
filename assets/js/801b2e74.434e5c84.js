"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[28932],{48885:(n,e,s)=>{s.d(e,{R:()=>l,x:()=>i});var t=s(99378);const r={},c=t.createContext(r);function l(n){const e=t.useContext(c);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:l(n.components),t.createElement(c.Provider,{value:e},n.children)}},89416:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>o,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"ai/prompt-caching","title":"\ud83d\udcbe Prompt Caching","description":"Prompt Caching \u662f OpenAI \u548c Anthropic \u63d0\u4f9b\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u7f13\u5b58\u91cd\u590d\u4f7f\u7528\u7684\u63d0\u793a\u524d\u7f00\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\u3002","source":"@site/docs/ai/prompt-caching.md","sourceDirName":"ai","slug":"/ai/prompt-caching","permalink":"/docs/ai/prompt-caching","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/prompt-caching.md","tags":[],"version":"current","sidebarPosition":22,"frontMatter":{"sidebar_position":22,"title":"\ud83d\udcbe Prompt Caching"}}');var r=s(22714),c=s(48885);const l={sidebar_position:22,title:"\ud83d\udcbe Prompt Caching"},i="Prompt Caching\uff08\u63d0\u793a\u7f13\u5b58\uff09",o={},a=[{value:"\u4e3a\u4ec0\u4e48\u9700\u8981 Prompt Caching\uff1f",id:"\u4e3a\u4ec0\u4e48\u9700\u8981-prompt-caching",level:2},{value:"\u6210\u672c\u8282\u7701",id:"\u6210\u672c\u8282\u7701",level:2},{value:"OpenAI",id:"openai",level:3},{value:"Anthropic",id:"anthropic",level:3},{value:"OpenAI Prompt Caching",id:"openai-prompt-caching",level:2},{value:"\u5de5\u4f5c\u539f\u7406",id:"\u5de5\u4f5c\u539f\u7406",level:3},{value:"\u7f13\u5b58\u6761\u4ef6",id:"\u7f13\u5b58\u6761\u4ef6",level:3},{value:"\u67e5\u770b\u7f13\u5b58\u547d\u4e2d",id:"\u67e5\u770b\u7f13\u5b58\u547d\u4e2d",level:3},{value:"\u4f18\u5316\u7f13\u5b58\u547d\u4e2d\u7387",id:"\u4f18\u5316\u7f13\u5b58\u547d\u4e2d\u7387",level:3},{value:"\u6279\u91cf\u8bf7\u6c42\u4f18\u5316",id:"\u6279\u91cf\u8bf7\u6c42\u4f18\u5316",level:3},{value:"Anthropic Prompt Caching",id:"anthropic-prompt-caching",level:2},{value:"\u57fa\u7840\u7528\u6cd5",id:"\u57fa\u7840\u7528\u6cd5",level:3},{value:"\u591a\u4e2a\u7f13\u5b58\u65ad\u70b9",id:"\u591a\u4e2a\u7f13\u5b58\u65ad\u70b9",level:3},{value:"\u7f13\u5b58\u6761\u4ef6",id:"\u7f13\u5b58\u6761\u4ef6-1",level:3},{value:"\u5de5\u5177\u5b9a\u4e49\u7f13\u5b58",id:"\u5de5\u5177\u5b9a\u4e49\u7f13\u5b58",level:3},{value:"\u5b9e\u6218\u5e94\u7528",id:"\u5b9e\u6218\u5e94\u7528",level:2},{value:"1. RAG \u573a\u666f\u4f18\u5316",id:"1-rag-\u573a\u666f\u4f18\u5316",level:3},{value:"2. \u591a\u8f6e\u5bf9\u8bdd\u4f18\u5316",id:"2-\u591a\u8f6e\u5bf9\u8bdd\u4f18\u5316",level:3},{value:"3. Few-shot \u5b66\u4e60\u4f18\u5316",id:"3-few-shot-\u5b66\u4e60\u4f18\u5316",level:3},{value:"\u76d1\u63a7\u4e0e\u4f18\u5316",id:"\u76d1\u63a7\u4e0e\u4f18\u5316",level:2},{value:"\u7f13\u5b58\u547d\u4e2d\u7387\u76d1\u63a7",id:"\u7f13\u5b58\u547d\u4e2d\u7387\u76d1\u63a7",level:3},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function h(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,c.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"prompt-caching\u63d0\u793a\u7f13\u5b58",children:"Prompt Caching\uff08\u63d0\u793a\u7f13\u5b58\uff09"})}),"\n",(0,r.jsx)(e.p,{children:"Prompt Caching \u662f OpenAI \u548c Anthropic \u63d0\u4f9b\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u7f13\u5b58\u91cd\u590d\u4f7f\u7528\u7684\u63d0\u793a\u524d\u7f00\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\u3002"}),"\n",(0,r.jsx)(e.h2,{id:"\u4e3a\u4ec0\u4e48\u9700\u8981-prompt-caching",children:"\u4e3a\u4ec0\u4e48\u9700\u8981 Prompt Caching\uff1f"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"\u573a\u666f"}),(0,r.jsx)(e.th,{children:"\u95ee\u9898"}),(0,r.jsx)(e.th,{children:"Prompt Caching \u6548\u679c"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u957f\u7cfb\u7edf\u63d0\u793a"}),(0,r.jsx)(e.td,{children:"\u6bcf\u6b21\u90fd\u8981\u5904\u7406\u76f8\u540c\u5185\u5bb9"}),(0,r.jsx)(e.td,{children:"\u7f13\u5b58\u540e\u53ea\u5904\u7406\u4e00\u6b21"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"RAG \u56fa\u5b9a\u4e0a\u4e0b\u6587"}),(0,r.jsx)(e.td,{children:"\u76f8\u540c\u6587\u6863\u91cd\u590d\u53d1\u9001"}),(0,r.jsx)(e.td,{children:"\u7f13\u5b58\u6587\u6863\u5185\u5bb9"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u591a\u8f6e\u5bf9\u8bdd"}),(0,r.jsx)(e.td,{children:"\u5386\u53f2\u6d88\u606f\u91cd\u590d\u5904\u7406"}),(0,r.jsx)(e.td,{children:"\u7f13\u5b58\u5386\u53f2\u90e8\u5206"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Few-shot \u793a\u4f8b"}),(0,r.jsx)(e.td,{children:"\u76f8\u540c\u793a\u4f8b\u91cd\u590d\u53d1\u9001"}),(0,r.jsx)(e.td,{children:"\u7f13\u5b58\u793a\u4f8b"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"\u6210\u672c\u8282\u7701",children:"\u6210\u672c\u8282\u7701"}),"\n",(0,r.jsx)(e.h3,{id:"openai",children:"OpenAI"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"\u6a21\u578b"}),(0,r.jsx)(e.th,{children:"\u6b63\u5e38\u8f93\u5165\u4ef7\u683c"}),(0,r.jsx)(e.th,{children:"\u7f13\u5b58\u8f93\u5165\u4ef7\u683c"}),(0,r.jsx)(e.th,{children:"\u8282\u7701\u6bd4\u4f8b"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"GPT-4o"}),(0,r.jsx)(e.td,{children:"$2.50/1M"}),(0,r.jsx)(e.td,{children:"$1.25/1M"}),(0,r.jsx)(e.td,{children:"50%"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"o1"}),(0,r.jsx)(e.td,{children:"$15.00/1M"}),(0,r.jsx)(e.td,{children:"$7.50/1M"}),(0,r.jsx)(e.td,{children:"50%"})]})]})]}),"\n",(0,r.jsx)(e.h3,{id:"anthropic",children:"Anthropic"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"\u6a21\u578b"}),(0,r.jsx)(e.th,{children:"\u6b63\u5e38\u8f93\u5165\u4ef7\u683c"}),(0,r.jsx)(e.th,{children:"\u7f13\u5b58\u5199\u5165\u4ef7\u683c"}),(0,r.jsx)(e.th,{children:"\u7f13\u5b58\u8bfb\u53d6\u4ef7\u683c"}),(0,r.jsx)(e.th,{children:"\u8282\u7701\u6bd4\u4f8b"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Claude 3.5 Sonnet"}),(0,r.jsx)(e.td,{children:"$3.00/1M"}),(0,r.jsx)(e.td,{children:"$3.75/1M"}),(0,r.jsx)(e.td,{children:"$0.30/1M"}),(0,r.jsx)(e.td,{children:"90%"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Claude 3.5 Haiku"}),(0,r.jsx)(e.td,{children:"$0.80/1M"}),(0,r.jsx)(e.td,{children:"$1.00/1M"}),(0,r.jsx)(e.td,{children:"$0.08/1M"}),(0,r.jsx)(e.td,{children:"90%"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"openai-prompt-caching",children:"OpenAI Prompt Caching"}),"\n",(0,r.jsxs)(e.p,{children:["OpenAI \u7684 Prompt Caching \u662f",(0,r.jsx)(e.strong,{children:"\u81ea\u52a8"}),"\u7684\uff0c\u65e0\u9700\u989d\u5916\u914d\u7f6e\u3002"]}),"\n",(0,r.jsx)(e.h3,{id:"\u5de5\u4f5c\u539f\u7406",children:"\u5de5\u4f5c\u539f\u7406"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"\u8bf7\u6c42 1: [\u7cfb\u7edf\u63d0\u793a 2000 tokens] + [\u7528\u6237\u6d88\u606f 100 tokens]\n        \u2193 \u81ea\u52a8\u7f13\u5b58\u524d\u7f00\n\u8bf7\u6c42 2: [\u7cfb\u7edf\u63d0\u793a 2000 tokens] + [\u7528\u6237\u6d88\u606f 200 tokens]\n        \u2193 \u547d\u4e2d\u7f13\u5b58\uff0c\u53ea\u5904\u7406\u65b0\u589e\u90e8\u5206\n"})}),"\n",(0,r.jsx)(e.h3,{id:"\u7f13\u5b58\u6761\u4ef6",children:"\u7f13\u5b58\u6761\u4ef6"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\u63d0\u793a\u524d\u7f00\u5fc5\u987b",(0,r.jsx)(e.strong,{children:"\u5b8c\u5168\u76f8\u540c"}),"\uff08\u9010\u5b57\u7b26\u5339\u914d\uff09"]}),"\n",(0,r.jsx)(e.li,{children:"\u6700\u5c0f\u7f13\u5b58\u957f\u5ea6\uff1a1024 tokens"}),"\n",(0,r.jsx)(e.li,{children:"\u7f13\u5b58\u6709\u6548\u671f\uff1a5-10 \u5206\u949f\uff08\u4f4e\u6d41\u91cf\u65f6\u66f4\u77ed\uff09"}),"\n",(0,r.jsx)(e.li,{children:"\u76f8\u540c\u7ec4\u7ec7\u5185\u7684\u8bf7\u6c42\u5171\u4eab\u7f13\u5b58"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"\u67e5\u770b\u7f13\u5b58\u547d\u4e2d",children:"\u67e5\u770b\u7f13\u5b58\u547d\u4e2d"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[\n        {"role": "system", "content": "\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u52a9\u624b..." * 100},  # \u957f\u7cfb\u7edf\u63d0\u793a\n        {"role": "user", "content": "\u4f60\u597d"}\n    ]\n)\n\n# \u67e5\u770b\u7f13\u5b58\u4fe1\u606f\nusage = response.usage\nprint(f"\u603b\u8f93\u5165 tokens: {usage.prompt_tokens}")\nprint(f"\u7f13\u5b58\u547d\u4e2d tokens: {usage.prompt_tokens_details.cached_tokens}")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"\u4f18\u5316\u7f13\u5b58\u547d\u4e2d\u7387",children:"\u4f18\u5316\u7f13\u5b58\u547d\u4e2d\u7387"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# \u2705 \u597d\u7684\u505a\u6cd5\uff1a\u56fa\u5b9a\u524d\u7f00\uff0c\u53d8\u5316\u90e8\u5206\u653e\u540e\u9762\nmessages = [\n    {"role": "system", "content": FIXED_SYSTEM_PROMPT},  # \u56fa\u5b9a\n    {"role": "user", "content": FIXED_EXAMPLES},         # \u56fa\u5b9a\u793a\u4f8b\n    {"role": "user", "content": user_input}              # \u53d8\u5316\u90e8\u5206\n]\n\n# \u274c \u4e0d\u597d\u7684\u505a\u6cd5\uff1a\u53d8\u5316\u90e8\u5206\u5728\u524d\u9762\nmessages = [\n    {"role": "system", "content": f"\u5f53\u524d\u65f6\u95f4\uff1a{datetime.now()}"},  # \u6bcf\u6b21\u90fd\u53d8\n    {"role": "system", "content": FIXED_SYSTEM_PROMPT},\n    {"role": "user", "content": user_input}\n]\n'})}),"\n",(0,r.jsx)(e.h3,{id:"\u6279\u91cf\u8bf7\u6c42\u4f18\u5316",children:"\u6279\u91cf\u8bf7\u6c42\u4f18\u5316"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# \u76f8\u540c\u524d\u7f00\u7684\u8bf7\u6c42\u4f1a\u5171\u4eab\u7f13\u5b58\nasync def batch_with_cache(prompts: list[str], system_prompt: str):\n    """\u6279\u91cf\u8bf7\u6c42\uff0c\u5171\u4eab\u7cfb\u7edf\u63d0\u793a\u7f13\u5b58"""\n    tasks = []\n    for prompt in prompts:\n        task = client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": prompt}\n            ]\n        )\n        tasks.append(task)\n    \n    return await asyncio.gather(*tasks)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"anthropic-prompt-caching",children:"Anthropic Prompt Caching"}),"\n",(0,r.jsxs)(e.p,{children:["Anthropic \u7684 Prompt Caching \u9700\u8981",(0,r.jsx)(e.strong,{children:"\u663e\u5f0f\u6807\u8bb0"}),"\u7f13\u5b58\u65ad\u70b9\u3002"]}),"\n",(0,r.jsx)(e.h3,{id:"\u57fa\u7840\u7528\u6cd5",children:"\u57fa\u7840\u7528\u6cd5"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import anthropic\n\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    system=[\n        {\n            "type": "text",\n            "text": "\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u52a9\u624b\uff0c\u4ee5\u4e0b\u662f\u4f60\u9700\u8981\u53c2\u8003\u7684\u6587\u6863\uff1a\\n" + long_document,\n            "cache_control": {"type": "ephemeral"}  # \u6807\u8bb0\u7f13\u5b58\u70b9\n        }\n    ],\n    messages=[\n        {"role": "user", "content": "\u603b\u7ed3\u6587\u6863\u7684\u4e3b\u8981\u5185\u5bb9"}\n    ]\n)\n\n# \u67e5\u770b\u7f13\u5b58\u4fe1\u606f\nprint(f"\u8f93\u5165 tokens: {response.usage.input_tokens}")\nprint(f"\u7f13\u5b58\u521b\u5efa tokens: {response.usage.cache_creation_input_tokens}")\nprint(f"\u7f13\u5b58\u8bfb\u53d6 tokens: {response.usage.cache_read_input_tokens}")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"\u591a\u4e2a\u7f13\u5b58\u65ad\u70b9",children:"\u591a\u4e2a\u7f13\u5b58\u65ad\u70b9"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'response = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    system=[\n        {\n            "type": "text",\n            "text": "\u4f60\u662f\u4e00\u4e2a\u4ee3\u7801\u52a9\u624b\u3002",\n        },\n        {\n            "type": "text",\n            "text": code_documentation,  # \u4ee3\u7801\u6587\u6863\n            "cache_control": {"type": "ephemeral"}  # \u7f13\u5b58\u70b9 1\n        }\n    ],\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "text",\n                    "text": few_shot_examples,  # Few-shot \u793a\u4f8b\n                    "cache_control": {"type": "ephemeral"}  # \u7f13\u5b58\u70b9 2\n                },\n                {\n                    "type": "text",\n                    "text": "\u8bf7\u5e2e\u6211\u5199\u4e00\u4e2a\u6392\u5e8f\u51fd\u6570"\n                }\n            ]\n        }\n    ]\n)\n'})}),"\n",(0,r.jsx)(e.h3,{id:"\u7f13\u5b58\u6761\u4ef6-1",children:"\u7f13\u5b58\u6761\u4ef6"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\u6700\u5c0f\u7f13\u5b58\u957f\u5ea6\uff1a\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Claude 3.5 Sonnet/Opus: 1024 tokens"}),"\n",(0,r.jsx)(e.li,{children:"Claude 3.5 Haiku: 2048 tokens"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.li,{children:"\u6700\u591a 4 \u4e2a\u7f13\u5b58\u65ad\u70b9"}),"\n",(0,r.jsx)(e.li,{children:"\u7f13\u5b58\u6709\u6548\u671f\uff1a5 \u5206\u949f"}),"\n",(0,r.jsxs)(e.li,{children:["\u5fc5\u987b\u4f7f\u7528 ",(0,r.jsx)(e.code,{children:"cache_control"})," \u663e\u5f0f\u6807\u8bb0"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"\u5de5\u5177\u5b9a\u4e49\u7f13\u5b58",children:"\u5de5\u5177\u5b9a\u4e49\u7f13\u5b58"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'tools = [\n    {\n        "name": "search",\n        "description": "\u641c\u7d22\u6587\u6863",\n        "input_schema": {\n            "type": "object",\n            "properties": {\n                "query": {"type": "string"}\n            }\n        }\n    },\n    # ... \u66f4\u591a\u5de5\u5177\n]\n\n# \u7f13\u5b58\u5de5\u5177\u5b9a\u4e49\nresponse = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    tools=tools,\n    extra_headers={\n        "anthropic-beta": "prompt-caching-2024-07-31"\n    },\n    system=[\n        {\n            "type": "text",\n            "text": "\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\u3002",\n            "cache_control": {"type": "ephemeral"}\n        }\n    ],\n    messages=[{"role": "user", "content": "\u641c\u7d22\u76f8\u5173\u6587\u6863"}]\n)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u5b9e\u6218\u5e94\u7528",children:"\u5b9e\u6218\u5e94\u7528"}),"\n",(0,r.jsx)(e.h3,{id:"1-rag-\u573a\u666f\u4f18\u5316",children:"1. RAG \u573a\u666f\u4f18\u5316"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class CachedRAG:\n    """\u5e26\u7f13\u5b58\u7684 RAG \u7cfb\u7edf"""\n    \n    def __init__(self, documents: str):\n        self.documents = documents\n        self.client = anthropic.Anthropic()\n    \n    def query(self, question: str) -> str:\n        """\u67e5\u8be2\u65f6\u590d\u7528\u6587\u6863\u7f13\u5b58"""\n        response = self.client.messages.create(\n            model="claude-3-5-sonnet-20241022",\n            max_tokens=1024,\n            system=[\n                {\n                    "type": "text",\n                    "text": f"\u4f60\u662f\u4e00\u4e2a\u95ee\u7b54\u52a9\u624b\u3002\u8bf7\u6839\u636e\u4ee5\u4e0b\u6587\u6863\u56de\u7b54\u95ee\u9898\uff1a\\n\\n{self.documents}",\n                    "cache_control": {"type": "ephemeral"}\n                }\n            ],\n            messages=[\n                {"role": "user", "content": question}\n            ]\n        )\n        return response.content[0].text\n\n# \u4f7f\u7528\nrag = CachedRAG(long_document)\n# \u7b2c\u4e00\u6b21\u8bf7\u6c42\uff1a\u521b\u5efa\u7f13\u5b58\nanswer1 = rag.query("\u6587\u6863\u7684\u4e3b\u9898\u662f\u4ec0\u4e48\uff1f")\n# \u540e\u7eed\u8bf7\u6c42\uff1a\u547d\u4e2d\u7f13\u5b58\uff0c\u6210\u672c\u964d\u4f4e 90%\nanswer2 = rag.query("\u4f5c\u8005\u662f\u8c01\uff1f")\nanswer3 = rag.query("\u4e3b\u8981\u7ed3\u8bba\u662f\u4ec0\u4e48\uff1f")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"2-\u591a\u8f6e\u5bf9\u8bdd\u4f18\u5316",children:"2. \u591a\u8f6e\u5bf9\u8bdd\u4f18\u5316"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class CachedChat:\n    """\u5e26\u7f13\u5b58\u7684\u591a\u8f6e\u5bf9\u8bdd"""\n    \n    def __init__(self, system_prompt: str):\n        self.system_prompt = system_prompt\n        self.messages = []\n        self.client = anthropic.Anthropic()\n    \n    def chat(self, user_message: str) -> str:\n        self.messages.append({"role": "user", "content": user_message})\n        \n        # \u6784\u5efa\u5e26\u7f13\u5b58\u7684\u6d88\u606f\n        cached_messages = []\n        for i, msg in enumerate(self.messages[:-1]):  # \u5386\u53f2\u6d88\u606f\n            if i == len(self.messages) - 2:  # \u6700\u540e\u4e00\u6761\u5386\u53f2\u6d88\u606f\u52a0\u7f13\u5b58\n                cached_messages.append({\n                    "role": msg["role"],\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": msg["content"],\n                            "cache_control": {"type": "ephemeral"}\n                        }\n                    ]\n                })\n            else:\n                cached_messages.append(msg)\n        \n        # \u6dfb\u52a0\u5f53\u524d\u6d88\u606f\n        cached_messages.append(self.messages[-1])\n        \n        response = self.client.messages.create(\n            model="claude-3-5-sonnet-20241022",\n            max_tokens=1024,\n            system=[\n                {\n                    "type": "text",\n                    "text": self.system_prompt,\n                    "cache_control": {"type": "ephemeral"}\n                }\n            ],\n            messages=cached_messages\n        )\n        \n        assistant_message = response.content[0].text\n        self.messages.append({"role": "assistant", "content": assistant_message})\n        \n        return assistant_message\n'})}),"\n",(0,r.jsx)(e.h3,{id:"3-few-shot-\u5b66\u4e60\u4f18\u5316",children:"3. Few-shot \u5b66\u4e60\u4f18\u5316"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def few_shot_with_cache(examples: str, query: str) -> str:\n    """\u7f13\u5b58 Few-shot \u793a\u4f8b"""\n    response = client.messages.create(\n        model="claude-3-5-sonnet-20241022",\n        max_tokens=1024,\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": f"\u4ee5\u4e0b\u662f\u4e00\u4e9b\u793a\u4f8b\uff1a\\n\\n{examples}",\n                        "cache_control": {"type": "ephemeral"}\n                    },\n                    {\n                        "type": "text",\n                        "text": f"\\n\\n\u73b0\u5728\u8bf7\u5904\u7406\uff1a{query}"\n                    }\n                ]\n            }\n        ]\n    )\n    return response.content[0].text\n\n# \u793a\u4f8b\nexamples = """\n\u8f93\u5165\uff1a\u4eca\u5929\u5929\u6c14\u771f\u597d\n\u8f93\u51fa\uff1apositive\n\n\u8f93\u5165\uff1a\u8fd9\u4e2a\u4ea7\u54c1\u592a\u5dee\u4e86\n\u8f93\u51fa\uff1anegative\n\n\u8f93\u5165\uff1a\u8fd8\u884c\u5427\n\u8f93\u51fa\uff1aneutral\n"""\n\n# \u591a\u6b21\u8c03\u7528\uff0c\u793a\u4f8b\u90e8\u5206\u88ab\u7f13\u5b58\nresult1 = few_shot_with_cache(examples, "\u6211\u5f88\u559c\u6b22\u8fd9\u4e2a")\nresult2 = few_shot_with_cache(examples, "\u592a\u7cdf\u7cd5\u4e86")\nresult3 = few_shot_with_cache(examples, "\u4e00\u822c\u822c")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u76d1\u63a7\u4e0e\u4f18\u5316",children:"\u76d1\u63a7\u4e0e\u4f18\u5316"}),"\n",(0,r.jsx)(e.h3,{id:"\u7f13\u5b58\u547d\u4e2d\u7387\u76d1\u63a7",children:"\u7f13\u5b58\u547d\u4e2d\u7387\u76d1\u63a7"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class CacheMonitor:\n    """\u7f13\u5b58\u76d1\u63a7"""\n    \n    def __init__(self):\n        self.total_input_tokens = 0\n        self.cached_tokens = 0\n        self.requests = 0\n    \n    def record(self, usage):\n        self.requests += 1\n        self.total_input_tokens += usage.input_tokens\n        \n        # Anthropic\n        if hasattr(usage, \'cache_read_input_tokens\'):\n            self.cached_tokens += usage.cache_read_input_tokens\n        # OpenAI\n        elif hasattr(usage, \'prompt_tokens_details\'):\n            self.cached_tokens += usage.prompt_tokens_details.cached_tokens\n    \n    def get_stats(self) -> dict:\n        cache_rate = self.cached_tokens / self.total_input_tokens if self.total_input_tokens > 0 else 0\n        return {\n            "requests": self.requests,\n            "total_input_tokens": self.total_input_tokens,\n            "cached_tokens": self.cached_tokens,\n            "cache_hit_rate": f"{cache_rate:.2%}",\n            "estimated_savings": f"${self.cached_tokens * 0.0000025:.4f}"  # \u5047\u8bbe\u8282\u7701 $2.5/1M\n        }\n\nmonitor = CacheMonitor()\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u56fa\u5b9a\u524d\u7f00"}),"\uff1a\u628a\u4e0d\u53d8\u7684\u5185\u5bb9\u653e\u5728\u6d88\u606f\u5f00\u5934"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u5408\u7406\u5206\u7ec4"}),"\uff1a\u76f8\u4f3c\u8bf7\u6c42\u653e\u5728\u4e00\u8d77\u53d1\u9001\uff0c\u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u76d1\u63a7\u547d\u4e2d\u7387"}),"\uff1a\u5b9a\u671f\u68c0\u67e5\u7f13\u5b58\u6548\u679c"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u907f\u514d\u52a8\u6001\u5185\u5bb9"}),"\uff1a\u65f6\u95f4\u6233\u3001\u968f\u673a\u6570\u7b49\u653e\u5728\u6d88\u606f\u672b\u5c3e"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u6279\u91cf\u5904\u7406"}),"\uff1a\u76f8\u540c\u4e0a\u4e0b\u6587\u7684\u8bf7\u6c42\u6279\u91cf\u53d1\u9001"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/prompt-caching",children:"OpenAI Prompt Caching"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://docs.anthropic.com/claude/docs/prompt-caching",children:"Anthropic Prompt Caching"})}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,c.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}}}]);