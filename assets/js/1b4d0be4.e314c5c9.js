"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[80911],{48885:(n,e,s)=>{s.d(e,{R:()=>l,x:()=>c});var t=s(99378);const r={},i=t.createContext(r);function l(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:l(n.components),t.createElement(i.Provider,{value:e},n.children)}},89368:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"ai/long-context","title":"\ud83d\udcdc \u957f\u4e0a\u4e0b\u6587\u5904\u7406","description":"\u5904\u7406\u8d85\u957f\u6587\u6863\u662f LLM \u5e94\u7528\u7684\u5e38\u89c1\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u5404\u79cd\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7b56\u7565\u3002","source":"@site/docs/ai/long-context.md","sourceDirName":"ai","slug":"/ai/long-context","permalink":"/docs/ai/long-context","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/long-context.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"sidebar_position":23,"title":"\ud83d\udcdc \u957f\u4e0a\u4e0b\u6587\u5904\u7406"},"sidebar":"ai","previous":{"title":"\ud83d\uddbc\ufe0f \u591a\u6a21\u6001 AI","permalink":"/docs/ai/multimodal"},"next":{"title":"\ud83e\udde9 Mixture of Experts","permalink":"/docs/ai/moe"}}');var r=s(22714),i=s(48885);const l={sidebar_position:23,title:"\ud83d\udcdc \u957f\u4e0a\u4e0b\u6587\u5904\u7406"},c="\u957f\u4e0a\u4e0b\u6587\u5904\u7406",o={},d=[{value:"\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6",id:"\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6",level:2},{value:"\u5904\u7406\u7b56\u7565\u6982\u89c8",id:"\u5904\u7406\u7b56\u7565\u6982\u89c8",level:2},{value:"\u7b56\u7565 1: \u76f4\u63a5\u5904\u7406\uff08\u77ed\u6587\u6863\uff09",id:"\u7b56\u7565-1-\u76f4\u63a5\u5904\u7406\u77ed\u6587\u6863",level:2},{value:"\u7b56\u7565 2: Map-Reduce",id:"\u7b56\u7565-2-map-reduce",level:2},{value:"\u7b56\u7565 3: Refine\uff08\u8fed\u4ee3\u7cbe\u70bc\uff09",id:"\u7b56\u7565-3-refine\u8fed\u4ee3\u7cbe\u70bc",level:2},{value:"\u7b56\u7565 4: \u5c42\u6b21\u5316\u6458\u8981",id:"\u7b56\u7565-4-\u5c42\u6b21\u5316\u6458\u8981",level:2},{value:"\u7b56\u7565 5: RAG \u68c0\u7d22",id:"\u7b56\u7565-5-rag-\u68c0\u7d22",level:2},{value:"\u7b56\u7565 6: \u6ed1\u52a8\u7a97\u53e3",id:"\u7b56\u7565-6-\u6ed1\u52a8\u7a97\u53e3",level:2},{value:"LangChain \u96c6\u6210",id:"langchain-\u96c6\u6210",level:2},{value:"\u7b56\u7565\u9009\u62e9\u6307\u5357",id:"\u7b56\u7565\u9009\u62e9\u6307\u5357",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function a(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"\u957f\u4e0a\u4e0b\u6587\u5904\u7406",children:"\u957f\u4e0a\u4e0b\u6587\u5904\u7406"})}),"\n",(0,r.jsx)(e.p,{children:"\u5904\u7406\u8d85\u957f\u6587\u6863\u662f LLM \u5e94\u7528\u7684\u5e38\u89c1\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u5404\u79cd\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7b56\u7565\u3002"}),"\n",(0,r.jsx)(e.h2,{id:"\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6",children:"\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"\u6a21\u578b"}),(0,r.jsx)(e.th,{children:"\u4e0a\u4e0b\u6587\u957f\u5ea6"}),(0,r.jsx)(e.th,{children:"\u7ea6\u7b49\u4e8e"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"GPT-4o"}),(0,r.jsx)(e.td,{children:"128K"}),(0,r.jsx)(e.td,{children:"~300 \u9875\u6587\u6863"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"GPT-4o-mini"}),(0,r.jsx)(e.td,{children:"128K"}),(0,r.jsx)(e.td,{children:"~300 \u9875\u6587\u6863"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Claude 3.5 Sonnet"}),(0,r.jsx)(e.td,{children:"200K"}),(0,r.jsx)(e.td,{children:"~500 \u9875\u6587\u6863"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Gemini 1.5 Pro"}),(0,r.jsx)(e.td,{children:"2M"}),(0,r.jsx)(e.td,{children:"~5000 \u9875\u6587\u6863"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Qwen2.5"}),(0,r.jsx)(e.td,{children:"128K"}),(0,r.jsx)(e.td,{children:"~300 \u9875\u6587\u6863"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"\u5904\u7406\u7b56\u7565\u6982\u89c8",children:"\u5904\u7406\u7b56\u7565\u6982\u89c8"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   \u957f\u6587\u6863\u5904\u7406\u7b56\u7565                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  \u6587\u6863\u957f\u5ea6 < \u4e0a\u4e0b\u6587\u7a97\u53e3                                   \u2502\n\u2502  \u2514\u2500> \u76f4\u63a5\u5904\u7406                                           \u2502\n\u2502                                                         \u2502\n\u2502  \u6587\u6863\u957f\u5ea6 > \u4e0a\u4e0b\u6587\u7a97\u53e3                                   \u2502\n\u2502  \u251c\u2500> \u7b56\u75651: \u5207\u5206 + RAG \u68c0\u7d22                             \u2502\n\u2502  \u251c\u2500> \u7b56\u75652: Map-Reduce \u6458\u8981                             \u2502\n\u2502  \u251c\u2500> \u7b56\u75653: Refine \u8fed\u4ee3\u7cbe\u70bc                             \u2502\n\u2502  \u2514\u2500> \u7b56\u75654: \u5c42\u6b21\u5316\u6458\u8981                                  \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565-1-\u76f4\u63a5\u5904\u7406\u77ed\u6587\u6863",children:"\u7b56\u7565 1: \u76f4\u63a5\u5904\u7406\uff08\u77ed\u6587\u6863\uff09"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\ndef process_short_document(document: str, question: str) -> str:\n    """\u76f4\u63a5\u5904\u7406\u77ed\u6587\u6863"""\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {\n                "role": "system",\n                "content": "\u4f60\u662f\u4e00\u4e2a\u6587\u6863\u5206\u6790\u52a9\u624b\u3002\u8bf7\u6839\u636e\u63d0\u4f9b\u7684\u6587\u6863\u56de\u7b54\u95ee\u9898\u3002"\n            },\n            {\n                "role": "user",\n                "content": f"\u6587\u6863\u5185\u5bb9\uff1a\\n\\n{document}\\n\\n\u95ee\u9898\uff1a{question}"\n            }\n        ]\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565-2-map-reduce",children:"\u7b56\u7565 2: Map-Reduce"}),"\n",(0,r.jsx)(e.p,{children:"\u5c06\u957f\u6587\u6863\u5207\u5206\uff0c\u5206\u522b\u5904\u7406\u540e\u5408\u5e76\u7ed3\u679c\u3002\u9002\u5408\u6458\u8981\u3001\u4fe1\u606f\u63d0\u53d6\u7b49\u4efb\u52a1\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom concurrent.futures import ThreadPoolExecutor\nimport tiktoken\n\nclass MapReduceProcessor:\n    """Map-Reduce \u6587\u6863\u5904\u7406\u5668"""\n    \n    def __init__(self, chunk_size: int = 4000, chunk_overlap: int = 200):\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=self._count_tokens\n        )\n        self.client = OpenAI()\n    \n    def _count_tokens(self, text: str) -> int:\n        enc = tiktoken.encoding_for_model("gpt-4o")\n        return len(enc.encode(text))\n    \n    def _map_chunk(self, chunk: str, task: str) -> str:\n        """Map \u9636\u6bb5\uff1a\u5904\u7406\u5355\u4e2a\u5757"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",  # Map \u9636\u6bb5\u7528\u5c0f\u6a21\u578b\n            messages=[\n                {"role": "system", "content": f"\u8bf7\u5bf9\u4ee5\u4e0b\u6587\u672c\u6267\u884c\u4efb\u52a1\uff1a{task}"},\n                {"role": "user", "content": chunk}\n            ],\n            temperature=0\n        )\n        return response.choices[0].message.content\n    \n    def _reduce(self, results: list[str], task: str) -> str:\n        """Reduce \u9636\u6bb5\uff1a\u5408\u5e76\u7ed3\u679c"""\n        combined = "\\n\\n---\\n\\n".join(results)\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",  # Reduce \u9636\u6bb5\u7528\u5927\u6a21\u578b\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"\u4ee5\u4e0b\u662f\u5bf9\u6587\u6863\u5404\u90e8\u5206\u6267\u884c\'{task}\'\u7684\u7ed3\u679c\u3002\u8bf7\u7efc\u5408\u8fd9\u4e9b\u7ed3\u679c\uff0c\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002"\n                },\n                {"role": "user", "content": combined}\n            ],\n            temperature=0\n        )\n        return response.choices[0].message.content\n    \n    def process(self, document: str, task: str, parallel: bool = True) -> str:\n        """\u5904\u7406\u957f\u6587\u6863"""\n        # \u5207\u5206\u6587\u6863\n        chunks = self.splitter.split_text(document)\n        print(f"\u6587\u6863\u88ab\u5207\u5206\u4e3a {len(chunks)} \u4e2a\u5757")\n        \n        # Map \u9636\u6bb5\n        if parallel:\n            with ThreadPoolExecutor(max_workers=5) as executor:\n                results = list(executor.map(\n                    lambda c: self._map_chunk(c, task),\n                    chunks\n                ))\n        else:\n            results = [self._map_chunk(c, task) for c in chunks]\n        \n        # Reduce \u9636\u6bb5\n        if len(results) == 1:\n            return results[0]\n        \n        return self._reduce(results, task)\n\n# \u4f7f\u7528\u793a\u4f8b\nprocessor = MapReduceProcessor()\nsummary = processor.process(\n    long_document,\n    task="\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u751f\u6210\u6458\u8981"\n)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565-3-refine\u8fed\u4ee3\u7cbe\u70bc",children:"\u7b56\u7565 3: Refine\uff08\u8fed\u4ee3\u7cbe\u70bc\uff09"}),"\n",(0,r.jsx)(e.p,{children:"\u9010\u5757\u5904\u7406\uff0c\u6bcf\u6b21\u57fa\u4e8e\u524d\u4e00\u6b21\u7684\u7ed3\u679c\u8fdb\u884c\u7cbe\u70bc\u3002\u9002\u5408\u9700\u8981\u8fde\u8d2f\u6027\u7684\u4efb\u52a1\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class RefineProcessor:\n    """Refine \u8fed\u4ee3\u7cbe\u70bc\u5904\u7406\u5668"""\n    \n    def __init__(self, chunk_size: int = 4000):\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=200\n        )\n        self.client = OpenAI()\n    \n    def process(self, document: str, task: str) -> str:\n        """\u8fed\u4ee3\u7cbe\u70bc\u5904\u7406"""\n        chunks = self.splitter.split_text(document)\n        \n        # \u5904\u7406\u7b2c\u4e00\u4e2a\u5757\n        current_result = self._initial_process(chunks[0], task)\n        \n        # \u8fed\u4ee3\u7cbe\u70bc\u540e\u7eed\u5757\n        for i, chunk in enumerate(chunks[1:], 2):\n            print(f"\u5904\u7406\u7b2c {i}/{len(chunks)} \u5757...")\n            current_result = self._refine(current_result, chunk, task)\n        \n        return current_result\n    \n    def _initial_process(self, chunk: str, task: str) -> str:\n        """\u5904\u7406\u7b2c\u4e00\u4e2a\u5757"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": f"\u8bf7\u5bf9\u4ee5\u4e0b\u6587\u672c\u6267\u884c\u4efb\u52a1\uff1a{task}"},\n                {"role": "user", "content": chunk}\n            ]\n        )\n        return response.choices[0].message.content\n    \n    def _refine(self, current_result: str, new_chunk: str, task: str) -> str:\n        """\u57fa\u4e8e\u65b0\u5185\u5bb9\u7cbe\u70bc\u7ed3\u679c"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"""\u4f60\u4e4b\u524d\u5bf9\u6587\u6863\u90e8\u5206\u5185\u5bb9\u6267\u884c\u4e86\u4efb\u52a1\uff1a{task}\n                    \n\u5f53\u524d\u7ed3\u679c\uff1a\n{current_result}\n\n\u73b0\u5728\u6709\u65b0\u7684\u6587\u6863\u5185\u5bb9\u3002\u8bf7\u6839\u636e\u65b0\u5185\u5bb9\u66f4\u65b0\u548c\u5b8c\u5584\u4f60\u7684\u7ed3\u679c\u3002\n\u5982\u679c\u65b0\u5185\u5bb9\u5305\u542b\u91cd\u8981\u4fe1\u606f\uff0c\u8bf7\u6dfb\u52a0\u5230\u7ed3\u679c\u4e2d\u3002\n\u5982\u679c\u65b0\u5185\u5bb9\u4e0e\u73b0\u6709\u7ed3\u679c\u77db\u76fe\uff0c\u8bf7\u8fdb\u884c\u4fee\u6b63\u3002"""\n                },\n                {"role": "user", "content": f"\u65b0\u5185\u5bb9\uff1a\\n{new_chunk}"}\n            ]\n        )\n        return response.choices[0].message.content\n\n# \u4f7f\u7528\nrefiner = RefineProcessor()\nresult = refiner.process(long_document, "\u751f\u6210\u8be6\u7ec6\u6458\u8981")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565-4-\u5c42\u6b21\u5316\u6458\u8981",children:"\u7b56\u7565 4: \u5c42\u6b21\u5316\u6458\u8981"}),"\n",(0,r.jsx)(e.p,{children:"\u6784\u5efa\u6458\u8981\u6811\uff0c\u9002\u5408\u8d85\u957f\u6587\u6863\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class HierarchicalSummarizer:\n    """\u5c42\u6b21\u5316\u6458\u8981"""\n    \n    def __init__(self, leaf_size: int = 2000, branch_factor: int = 4):\n        self.leaf_size = leaf_size\n        self.branch_factor = branch_factor\n        self.client = OpenAI()\n    \n    def summarize(self, document: str) -> dict:\n        """\u751f\u6210\u5c42\u6b21\u5316\u6458\u8981"""\n        # \u5207\u5206\u4e3a\u53f6\u5b50\u8282\u70b9\n        splitter = RecursiveCharacterTextSplitter(chunk_size=self.leaf_size)\n        chunks = splitter.split_text(document)\n        \n        # \u6784\u5efa\u6458\u8981\u6811\n        tree = self._build_tree(chunks)\n        \n        return {\n            "final_summary": tree["summary"],\n            "tree": tree\n        }\n    \n    def _build_tree(self, chunks: list[str], level: int = 0) -> dict:\n        """\u9012\u5f52\u6784\u5efa\u6458\u8981\u6811"""\n        if len(chunks) == 1:\n            summary = self._summarize_chunk(chunks[0])\n            return {"level": level, "summary": summary, "children": None}\n        \n        # \u5206\u7ec4\n        groups = [\n            chunks[i:i + self.branch_factor]\n            for i in range(0, len(chunks), self.branch_factor)\n        ]\n        \n        # \u9012\u5f52\u5904\u7406\u6bcf\u7ec4\n        children = []\n        child_summaries = []\n        \n        for group in groups:\n            if len(group) == 1:\n                child_summary = self._summarize_chunk(group[0])\n            else:\n                # \u5148\u5408\u5e76\u7ec4\u5185\u5185\u5bb9\u518d\u6458\u8981\n                combined = "\\n\\n".join(group)\n                child_summary = self._summarize_chunk(combined)\n            \n            children.append({\n                "level": level + 1,\n                "summary": child_summary,\n                "original_chunks": group\n            })\n            child_summaries.append(child_summary)\n        \n        # \u5982\u679c\u5b50\u6458\u8981\u6570\u91cf\u4ecd\u7136\u5f88\u591a\uff0c\u7ee7\u7eed\u9012\u5f52\n        if len(child_summaries) > self.branch_factor:\n            return self._build_tree(child_summaries, level)\n        \n        # \u5408\u5e76\u5b50\u6458\u8981\n        final_summary = self._merge_summaries(child_summaries)\n        \n        return {\n            "level": level,\n            "summary": final_summary,\n            "children": children\n        }\n    \n    def _summarize_chunk(self, text: str) -> str:\n        """\u6458\u8981\u5355\u4e2a\u5757"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {"role": "system", "content": "\u8bf7\u751f\u6210\u7b80\u6d01\u7684\u6458\u8981\uff0c\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002"},\n                {"role": "user", "content": text}\n            ],\n            max_tokens=500\n        )\n        return response.choices[0].message.content\n    \n    def _merge_summaries(self, summaries: list[str]) -> str:\n        """\u5408\u5e76\u591a\u4e2a\u6458\u8981"""\n        combined = "\\n\\n".join([f"\u90e8\u5206 {i+1}\uff1a{s}" for i, s in enumerate(summaries)])\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": "\u8bf7\u5c06\u4ee5\u4e0b\u591a\u4e2a\u6458\u8981\u5408\u5e76\u4e3a\u4e00\u4e2a\u8fde\u8d2f\u3001\u5168\u9762\u7684\u6458\u8981\u3002"\n                },\n                {"role": "user", "content": combined}\n            ]\n        )\n        return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565-5-rag-\u68c0\u7d22",children:"\u7b56\u7565 5: RAG \u68c0\u7d22"}),"\n",(0,r.jsx)(e.p,{children:"\u5bf9\u4e8e\u95ee\u7b54\u573a\u666f\uff0c\u53ea\u68c0\u7d22\u76f8\u5173\u90e8\u5206\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nclass LongDocumentQA:\n    """\u957f\u6587\u6863\u95ee\u7b54\u7cfb\u7edf"""\n    \n    def __init__(self, document: str):\n        # \u5207\u5206\u6587\u6863\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        chunks = splitter.split_text(document)\n        \n        # \u521b\u5efa\u5411\u91cf\u7d22\u5f15\n        self.vectorstore = Chroma.from_texts(\n            texts=chunks,\n            embedding=OpenAIEmbeddings(model="text-embedding-3-small")\n        )\n        \n        self.client = OpenAI()\n    \n    def query(self, question: str, k: int = 5) -> str:\n        """\u67e5\u8be2"""\n        # \u68c0\u7d22\u76f8\u5173\u5757\n        docs = self.vectorstore.similarity_search(question, k=k)\n        context = "\\n\\n".join([doc.page_content for doc in docs])\n        \n        # \u751f\u6210\u56de\u7b54\n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": "\u6839\u636e\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u56de\u7b54\u95ee\u9898\u3002\u5982\u679c\u4e0a\u4e0b\u6587\u4e2d\u6ca1\u6709\u76f8\u5173\u4fe1\u606f\uff0c\u8bf7\u8bf4\u660e\u3002"\n                },\n                {\n                    "role": "user",\n                    "content": f"\u4e0a\u4e0b\u6587\uff1a\\n{context}\\n\\n\u95ee\u9898\uff1a{question}"\n                }\n            ]\n        )\n        \n        return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565-6-\u6ed1\u52a8\u7a97\u53e3",children:"\u7b56\u7565 6: \u6ed1\u52a8\u7a97\u53e3"}),"\n",(0,r.jsx)(e.p,{children:"\u9002\u5408\u9700\u8981\u5904\u7406\u6574\u4e2a\u6587\u6863\u4f46\u4e0a\u4e0b\u6587\u6709\u9650\u7684\u573a\u666f\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class SlidingWindowProcessor:\n    """\u6ed1\u52a8\u7a97\u53e3\u5904\u7406\u5668"""\n    \n    def __init__(self, window_size: int = 4000, stride: int = 2000):\n        self.window_size = window_size\n        self.stride = stride\n        self.client = OpenAI()\n    \n    def process(self, document: str, task: str) -> list[dict]:\n        """\u6ed1\u52a8\u7a97\u53e3\u5904\u7406"""\n        results = []\n        \n        # \u6309\u5b57\u7b26\u6ed1\u52a8\uff08\u5b9e\u9645\u5e94\u7528\u4e2d\u5e94\u6309 token\uff09\n        for i in range(0, len(document), self.stride):\n            window = document[i:i + self.window_size]\n            \n            if len(window) < 100:  # \u8df3\u8fc7\u592a\u77ed\u7684\u7a97\u53e3\n                continue\n            \n            result = self._process_window(window, task, i)\n            results.append({\n                "start": i,\n                "end": i + len(window),\n                "result": result\n            })\n        \n        return results\n    \n    def _process_window(self, window: str, task: str, position: int) -> str:\n        """\u5904\u7406\u5355\u4e2a\u7a97\u53e3"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"\u8fd9\u662f\u6587\u6863\u7684\u4e00\u90e8\u5206\uff08\u4f4d\u7f6e\uff1a{position}\uff09\u3002\u8bf7\u6267\u884c\u4efb\u52a1\uff1a{task}"\n                },\n                {"role": "user", "content": window}\n            ]\n        )\n        return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"langchain-\u96c6\u6210",children:"LangChain \u96c6\u6210"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain.chains.summarize import load_summarize_chain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\n\nllm = ChatOpenAI(model="gpt-4o", temperature=0)\n\n# Map-Reduce \u6458\u8981\ndef langchain_map_reduce(text: str) -> str:\n    splitter = RecursiveCharacterTextSplitter(chunk_size=4000)\n    docs = [Document(page_content=t) for t in splitter.split_text(text)]\n    \n    chain = load_summarize_chain(llm, chain_type="map_reduce")\n    return chain.run(docs)\n\n# Refine \u6458\u8981\ndef langchain_refine(text: str) -> str:\n    splitter = RecursiveCharacterTextSplitter(chunk_size=4000)\n    docs = [Document(page_content=t) for t in splitter.split_text(text)]\n    \n    chain = load_summarize_chain(llm, chain_type="refine")\n    return chain.run(docs)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u7b56\u7565\u9009\u62e9\u6307\u5357",children:"\u7b56\u7565\u9009\u62e9\u6307\u5357"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"\u573a\u666f"}),(0,r.jsx)(e.th,{children:"\u63a8\u8350\u7b56\u7565"}),(0,r.jsx)(e.th,{children:"\u539f\u56e0"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u6587\u6863\u6458\u8981"}),(0,r.jsx)(e.td,{children:"Map-Reduce"}),(0,r.jsx)(e.td,{children:"\u5e76\u884c\u5904\u7406\uff0c\u901f\u5ea6\u5feb"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u8be6\u7ec6\u5206\u6790"}),(0,r.jsx)(e.td,{children:"Refine"}),(0,r.jsx)(e.td,{children:"\u4fdd\u6301\u8fde\u8d2f\u6027"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u8d85\u957f\u6587\u6863\u6458\u8981"}),(0,r.jsx)(e.td,{children:"\u5c42\u6b21\u5316\u6458\u8981"}),(0,r.jsx)(e.td,{children:"\u5904\u7406\u4efb\u610f\u957f\u5ea6"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u95ee\u7b54"}),(0,r.jsx)(e.td,{children:"RAG"}),(0,r.jsx)(e.td,{children:"\u53ea\u68c0\u7d22\u76f8\u5173\u90e8\u5206"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u4fe1\u606f\u63d0\u53d6"}),(0,r.jsx)(e.td,{children:"\u6ed1\u52a8\u7a97\u53e3"}),(0,r.jsx)(e.td,{children:"\u4e0d\u9057\u6f0f\u4efb\u4f55\u90e8\u5206"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"\u6587\u6863 < \u4e0a\u4e0b\u6587"}),(0,r.jsx)(e.td,{children:"\u76f4\u63a5\u5904\u7406"}),(0,r.jsx)(e.td,{children:"\u6700\u7b80\u5355"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u5148\u8bc4\u4f30\u6587\u6863\u957f\u5ea6"}),"\uff1a\u9009\u62e9\u5408\u9002\u7684\u7b56\u7565"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u5408\u7406\u8bbe\u7f6e chunk_size"}),"\uff1a\u592a\u5c0f\u4e22\u5931\u4e0a\u4e0b\u6587\uff0c\u592a\u5927\u8d85\u51fa\u9650\u5236"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u4fdd\u7559 overlap"}),"\uff1a\u907f\u514d\u4fe1\u606f\u5728\u8fb9\u754c\u4e22\u5931"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u5e76\u884c\u5904\u7406"}),"\uff1aMap-Reduce \u53ef\u4ee5\u5e76\u884c\u52a0\u901f"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u4f7f\u7528\u5c0f\u6a21\u578b\u5904\u7406\u4e2d\u95f4\u6b65\u9aa4"}),"\uff1a\u964d\u4f4e\u6210\u672c"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://python.langchain.com/docs/tutorials/summarization/",children:"LangChain Summarization"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/",children:"LlamaIndex Document Summary"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(a,{...n})}):a(n)}}}]);