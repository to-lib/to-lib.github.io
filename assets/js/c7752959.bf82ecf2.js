"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[38828],{31403:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"ai/coding-assistant","title":"\ud83d\udcbb AI \u7f16\u7801\u52a9\u624b\u5f00\u53d1","description":"\u6784\u5efa\u7c7b\u4f3c GitHub Copilot \u7684 AI \u7f16\u7801\u52a9\u624b\uff0c\u5305\u62ec\u4ee3\u7801\u8865\u5168\u3001\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u89e3\u91ca\u7b49\u529f\u80fd\u3002","source":"@site/docs/ai/coding-assistant.md","sourceDirName":"ai","slug":"/ai/coding-assistant","permalink":"/docs/ai/coding-assistant","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/coding-assistant.md","tags":[],"version":"current","sidebarPosition":33,"frontMatter":{"sidebar_position":33,"title":"\ud83d\udcbb AI \u7f16\u7801\u52a9\u624b\u5f00\u53d1"},"sidebar":"ai","previous":{"title":"\ud83c\udfe0 \u672c\u5730\u90e8\u7f72 LLM","permalink":"/docs/ai/local-llm"},"next":{"title":"\ud83d\udccb \u5feb\u901f\u53c2\u8003","permalink":"/docs/ai/quick-reference"}}');var r=t(22714),o=t(48885);const i={sidebar_position:33,title:"\ud83d\udcbb AI \u7f16\u7801\u52a9\u624b\u5f00\u53d1"},l="AI \u7f16\u7801\u52a9\u624b\u5f00\u53d1",c={},a=[{value:"\u6838\u5fc3\u529f\u80fd",id:"\u6838\u5fc3\u529f\u80fd",level:2},{value:"\u4ee3\u7801\u8865\u5168",id:"\u4ee3\u7801\u8865\u5168",level:2},{value:"\u57fa\u7840\u5b9e\u73b0",id:"\u57fa\u7840\u5b9e\u73b0",level:3},{value:"\u4e0a\u4e0b\u6587\u6536\u96c6",id:"\u4e0a\u4e0b\u6587\u6536\u96c6",level:3},{value:"\u4ee3\u7801\u751f\u6210",id:"\u4ee3\u7801\u751f\u6210",level:2},{value:"\u4ee3\u7801\u89e3\u91ca",id:"\u4ee3\u7801\u89e3\u91ca",level:2},{value:"\u4ee3\u7801\u91cd\u6784",id:"\u4ee3\u7801\u91cd\u6784",level:2},{value:"\u6d4b\u8bd5\u751f\u6210",id:"\u6d4b\u8bd5\u751f\u6210",level:2},{value:"\u5b8c\u6574\u7f16\u7801\u52a9\u624b",id:"\u5b8c\u6574\u7f16\u7801\u52a9\u624b",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function d(n){const e={a:"a",code:"code",cursor:"cursor",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"ai-\u7f16\u7801\u52a9\u624b\u5f00\u53d1",children:"AI \u7f16\u7801\u52a9\u624b\u5f00\u53d1"})}),"\n",(0,r.jsx)(e.p,{children:"\u6784\u5efa\u7c7b\u4f3c GitHub Copilot \u7684 AI \u7f16\u7801\u52a9\u624b\uff0c\u5305\u62ec\u4ee3\u7801\u8865\u5168\u3001\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u89e3\u91ca\u7b49\u529f\u80fd\u3002"}),"\n",(0,r.jsx)(e.h2,{id:"\u6838\u5fc3\u529f\u80fd",children:"\u6838\u5fc3\u529f\u80fd"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   AI \u7f16\u7801\u52a9\u624b                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  \u4ee3\u7801\u8865\u5168\uff1a\u6839\u636e\u4e0a\u4e0b\u6587\u81ea\u52a8\u8865\u5168\u4ee3\u7801                        \u2502\n\u2502  \u4ee3\u7801\u751f\u6210\uff1a\u6839\u636e\u6ce8\u91ca/\u63cf\u8ff0\u751f\u6210\u4ee3\u7801                        \u2502\n\u2502  \u4ee3\u7801\u89e3\u91ca\uff1a\u89e3\u91ca\u4ee3\u7801\u529f\u80fd\u548c\u903b\u8f91                           \u2502\n\u2502  \u4ee3\u7801\u91cd\u6784\uff1a\u4f18\u5316\u548c\u91cd\u6784\u73b0\u6709\u4ee3\u7801                           \u2502\n\u2502  Bug \u4fee\u590d\uff1a\u8bc6\u522b\u5e76\u4fee\u590d\u4ee3\u7801\u95ee\u9898                           \u2502\n\u2502  \u6d4b\u8bd5\u751f\u6210\uff1a\u81ea\u52a8\u751f\u6210\u5355\u5143\u6d4b\u8bd5                             \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(e.h2,{id:"\u4ee3\u7801\u8865\u5168",children:"\u4ee3\u7801\u8865\u5168"}),"\n",(0,r.jsx)(e.h3,{id:"\u57fa\u7840\u5b9e\u73b0",children:"\u57fa\u7840\u5b9e\u73b0"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\ndef code_completion(\n    prefix: str,\n    suffix: str = "",\n    language: str = "python",\n    max_tokens: int = 150\n) -> str:\n    """\u4ee3\u7801\u8865\u5168"""\n    prompt = f"""Complete the following {language} code. Only return the completion, no explanation.\n\n```{language}\n{prefix}<CURSOR>{suffix}\n```\n\nCompletion:"""\n\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        max_tokens=max_tokens,\n        temperature=0\n    )\n\n    return response.choices[0].message.content\n\n\n# \u4f7f\u7528 Fill-in-the-Middle (FIM)\ndef fim_completion(prefix: str, suffix: str) -> str:\n    """FIM \u6a21\u5f0f\u8865\u5168"""\n    response = client.completions.create(\n        model="gpt-3.5-turbo-instruct",\n        prompt=f"<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|>",\n        max_tokens=150,\n        temperature=0\n    )\n    return response.choices[0].text\n'})}),"\n",(0,r.jsx)(e.h3,{id:"\u4e0a\u4e0b\u6587\u6536\u96c6",children:"\u4e0a\u4e0b\u6587\u6536\u96c6"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import os\nfrom pathlib import Path\n\nclass CodeContext:\n    """\u4ee3\u7801\u4e0a\u4e0b\u6587\u6536\u96c6\u5668"""\n\n    def __init__(self, workspace_path: str):\n        self.workspace = Path(workspace_path)\n\n    def get_file_content(self, file_path: str) -> str:\n        """\u83b7\u53d6\u6587\u4ef6\u5185\u5bb9"""\n        full_path = self.workspace / file_path\n        if full_path.exists():\n            return full_path.read_text()\n        return ""\n\n    def get_related_files(self, current_file: str, max_files: int = 5) -> list:\n        """\u83b7\u53d6\u76f8\u5173\u6587\u4ef6"""\n        current = Path(current_file)\n        related = []\n\n        # \u540c\u76ee\u5f55\u6587\u4ef6\n        for f in current.parent.glob(f"*{current.suffix}"):\n            if f != current and len(related) < max_files:\n                related.append(str(f))\n\n        return related\n\n    def build_context(\n        self,\n        current_file: str,\n        cursor_line: int,\n        cursor_col: int\n    ) -> dict:\n        """\u6784\u5efa\u8865\u5168\u4e0a\u4e0b\u6587"""\n        content = self.get_file_content(current_file)\n        lines = content.split("\\n")\n\n        # \u5206\u5272\u524d\u7f00\u548c\u540e\u7f00\n        prefix_lines = lines[:cursor_line]\n        suffix_lines = lines[cursor_line:]\n\n        if prefix_lines:\n            prefix_lines[-1] = prefix_lines[-1][:cursor_col]\n        if suffix_lines:\n            suffix_lines[0] = suffix_lines[0][cursor_col:]\n\n        prefix = "\\n".join(prefix_lines)\n        suffix = "\\n".join(suffix_lines)\n\n        # \u83b7\u53d6\u76f8\u5173\u6587\u4ef6\u4f5c\u4e3a\u989d\u5916\u4e0a\u4e0b\u6587\n        related = self.get_related_files(current_file)\n        related_content = []\n        for f in related[:3]:\n            related_content.append({\n                "file": f,\n                "content": self.get_file_content(f)[:2000]  # \u9650\u5236\u957f\u5ea6\n            })\n\n        return {\n            "prefix": prefix,\n            "suffix": suffix,\n            "related_files": related_content,\n            "language": self._detect_language(current_file)\n        }\n\n    def _detect_language(self, file_path: str) -> str:\n        """\u68c0\u6d4b\u7f16\u7a0b\u8bed\u8a00"""\n        ext_map = {\n            ".py": "python",\n            ".js": "javascript",\n            ".ts": "typescript",\n            ".java": "java",\n            ".go": "go",\n            ".rs": "rust"\n        }\n        ext = Path(file_path).suffix\n        return ext_map.get(ext, "text")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u4ee3\u7801\u751f\u6210",children:"\u4ee3\u7801\u751f\u6210"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def generate_code(\n    description: str,\n    language: str = "python",\n    context: str = ""\n) -> str:\n    """\u6839\u636e\u63cf\u8ff0\u751f\u6210\u4ee3\u7801"""\n    system_prompt = f"""\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684 {language} \u5f00\u53d1\u8005\u3002\n\u6839\u636e\u7528\u6237\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u3002\n\n\u8981\u6c42\uff1a\n1. \u4ee3\u7801\u7b80\u6d01\u3001\u9ad8\u6548\n2. \u6dfb\u52a0\u5fc5\u8981\u7684\u6ce8\u91ca\n3. \u9075\u5faa {language} \u6700\u4f73\u5b9e\u8df5\n4. \u5904\u7406\u8fb9\u754c\u60c5\u51b5"""\n\n    user_prompt = f"\u63cf\u8ff0\uff1a{description}"\n    if context:\n        user_prompt = f"\u4e0a\u4e0b\u6587\uff1a\\n{context}\\n\\n{user_prompt}"\n\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": user_prompt}\n        ],\n        temperature=0.2\n    )\n\n    return response.choices[0].message.content\n\ndef generate_from_comment(code_with_comment: str) -> str:\n    """\u6839\u636e\u6ce8\u91ca\u751f\u6210\u4ee3\u7801"""\n    prompt = f"""\u6839\u636e\u6ce8\u91ca\u751f\u6210\u4ee3\u7801\u5b9e\u73b0\uff1a\n\n'})}),"\n",(0,r.jsx)(e.p,{children:"{code_with_comment}"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'\n\u53ea\u8fd4\u56de\u5b8c\u6574\u7684\u4ee3\u7801\uff0c\u5305\u542b\u6ce8\u91ca\u3002"""\n\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        temperature=0\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u4ee3\u7801\u89e3\u91ca",children:"\u4ee3\u7801\u89e3\u91ca"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def explain_code(code: str, detail_level: str = "medium") -> str:\n    """\u89e3\u91ca\u4ee3\u7801"""\n    detail_prompts = {\n        "brief": "\u7528\u4e00\u4e24\u53e5\u8bdd\u7b80\u8981\u8bf4\u660e\u8fd9\u6bb5\u4ee3\u7801\u7684\u529f\u80fd\u3002",\n        "medium": "\u89e3\u91ca\u8fd9\u6bb5\u4ee3\u7801\u7684\u529f\u80fd\u3001\u4e3b\u8981\u903b\u8f91\u548c\u5173\u952e\u6b65\u9aa4\u3002",\n        "detailed": "\u8be6\u7ec6\u89e3\u91ca\u8fd9\u6bb5\u4ee3\u7801\uff0c\u5305\u62ec\u6bcf\u4e2a\u51fd\u6570\u3001\u53d8\u91cf\u7684\u4f5c\u7528\uff0c\u7b97\u6cd5\u903b\u8f91\uff0c\u4ee5\u53ca\u53ef\u80fd\u7684\u6539\u8fdb\u70b9\u3002"\n    }\n\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {\n                "role": "system",\n                "content": f"\u4f60\u662f\u4e00\u4e2a\u4ee3\u7801\u89e3\u91ca\u4e13\u5bb6\u3002{detail_prompts[detail_level]}"\n            },\n            {"role": "user", "content": f"```\\n{code}\\n```"}\n        ]\n    )\n\n    return response.choices[0].message.content\n\ndef explain_error(code: str, error_message: str) -> str:\n    """\u89e3\u91ca\u9519\u8bef"""\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {\n                "role": "system",\n                "content": "\u5206\u6790\u4ee3\u7801\u9519\u8bef\uff0c\u89e3\u91ca\u539f\u56e0\u5e76\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u3002"\n            },\n            {\n                "role": "user",\n                "content": f"\u4ee3\u7801\uff1a\\n```\\n{code}\\n```\\n\\n\u9519\u8bef\u4fe1\u606f\uff1a\\n{error_message}"\n            }\n        ]\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u4ee3\u7801\u91cd\u6784",children:"\u4ee3\u7801\u91cd\u6784"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def refactor_code(\n    code: str,\n    refactor_type: str = "general"\n) -> str:\n    """\u4ee3\u7801\u91cd\u6784"""\n    refactor_prompts = {\n        "general": "\u4f18\u5316\u4ee3\u7801\u7ed3\u6784\u3001\u53ef\u8bfb\u6027\u548c\u6027\u80fd",\n        "performance": "\u4e13\u6ce8\u4e8e\u6027\u80fd\u4f18\u5316",\n        "readability": "\u63d0\u9ad8\u4ee3\u7801\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027",\n        "security": "\u4fee\u590d\u5b89\u5168\u95ee\u9898",\n        "modern": "\u4f7f\u7528\u73b0\u4ee3\u8bed\u6cd5\u548c\u6700\u4f73\u5b9e\u8df5\u91cd\u5199"\n    }\n\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {\n                "role": "system",\n                "content": f"\u4f60\u662f\u4ee3\u7801\u91cd\u6784\u4e13\u5bb6\u3002\u4efb\u52a1\uff1a{refactor_prompts[refactor_type]}\u3002\u8fd4\u56de\u91cd\u6784\u540e\u7684\u4ee3\u7801\u548c\u6539\u52a8\u8bf4\u660e\u3002"\n            },\n            {"role": "user", "content": f"```\\n{code}\\n```"}\n        ]\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u6d4b\u8bd5\u751f\u6210",children:"\u6d4b\u8bd5\u751f\u6210"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def generate_tests(\n    code: str,\n    framework: str = "pytest"\n) -> str:\n    """\u751f\u6210\u5355\u5143\u6d4b\u8bd5"""\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {\n                "role": "system",\n                "content": f"""\u4e3a\u4ee3\u7801\u751f\u6210\u5168\u9762\u7684\u5355\u5143\u6d4b\u8bd5\u3002\n\u4f7f\u7528 {framework} \u6846\u67b6\u3002\n\u5305\u542b\uff1a\u6b63\u5e38\u60c5\u51b5\u3001\u8fb9\u754c\u60c5\u51b5\u3001\u5f02\u5e38\u60c5\u51b5\u3002"""\n            },\n            {"role": "user", "content": f"```\\n{code}\\n```"}\n        ]\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u5b8c\u6574\u7f16\u7801\u52a9\u624b",children:"\u5b8c\u6574\u7f16\u7801\u52a9\u624b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class CodingAssistant:\n    """AI \u7f16\u7801\u52a9\u624b"""\n\n    def __init__(self, workspace: str = "."):\n        self.client = OpenAI()\n        self.context = CodeContext(workspace)\n        self.conversation = []\n\n    def complete(self, file_path: str, line: int, col: int) -> str:\n        """\u4ee3\u7801\u8865\u5168"""\n        ctx = self.context.build_context(file_path, line, col)\n\n        # \u6784\u5efa\u5e26\u4e0a\u4e0b\u6587\u7684\u63d0\u793a\n        related_context = ""\n        for f in ctx["related_files"]:\n            related_context += f"\\n// {f[\'file\']}\\n{f[\'content\'][:500]}\\n"\n\n        prompt = f"""Language: {ctx[\'language\']}\nRelated files:{related_context}\n\nComplete the code at <CURSOR>:\n'})}),"\n",(0,r.jsxs)(e.p,{children:["{ctx['prefix']}",(0,r.jsx)(e.cursor,{children:"{ctx['suffix']}"})]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:'language-"""',children:'\n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": prompt}],\n            max_tokens=200,\n            temperature=0\n        )\n\n        return response.choices[0].message.content\n\n    def chat(self, message: str, code_context: str = "") -> str:\n        """\u5bf9\u8bdd\u5f0f\u7f16\u7a0b\u52a9\u624b"""\n        self.conversation.append({"role": "user", "content": message})\n\n        system = """\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u7f16\u7a0b\u52a9\u624b\u3002\n\u5e2e\u52a9\u7528\u6237\u7f16\u5199\u3001\u8c03\u8bd5\u3001\u4f18\u5316\u4ee3\u7801\u3002\n\u56de\u7b54\u8981\u7b80\u6d01\u3001\u51c6\u786e\u3001\u5b9e\u7528\u3002"""\n\n        if code_context:\n            system += f"\\n\\n\u5f53\u524d\u4ee3\u7801\u4e0a\u4e0b\u6587\uff1a\\n```\\n{code_context}\\n```"\n\n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": system},\n                *self.conversation[-10:]  # \u4fdd\u7559\u6700\u8fd1 10 \u8f6e\n            ]\n        )\n\n        assistant_msg = response.choices[0].message.content\n        self.conversation.append({"role": "assistant", "content": assistant_msg})\n\n        return assistant_msg\n\n    def fix_bug(self, code: str, bug_description: str) -> str:\n        """\u4fee\u590d Bug"""\n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {\n                    "role": "system",\n                    "content": "\u4f60\u662f Bug \u4fee\u590d\u4e13\u5bb6\u3002\u5206\u6790\u95ee\u9898\uff0c\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u548c\u4fee\u590d\u540e\u7684\u4ee3\u7801\u3002"\n                },\n                {\n                    "role": "user",\n                    "content": f"\u4ee3\u7801\uff1a\\n```\\n{code}\\n```\\n\\n\u95ee\u9898\u63cf\u8ff0\uff1a{bug_description}"\n                }\n            ]\n        )\n\n        return response.choices[0].message.content\n\n# \u4f7f\u7528\nassistant = CodingAssistant("./my_project")\ncompletion = assistant.complete("src/main.py", 10, 0)\nanswer = assistant.chat("\u5982\u4f55\u4f18\u5316\u8fd9\u4e2a\u51fd\u6570\u7684\u6027\u80fd\uff1f", code_context)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u4e0a\u4e0b\u6587\u5f88\u91cd\u8981"}),"\uff1a\u63d0\u4f9b\u8db3\u591f\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u6d41\u5f0f\u8f93\u51fa"}),"\uff1a\u8865\u5168\u65f6\u4f7f\u7528\u6d41\u5f0f\u63d0\u5347\u4f53\u9a8c"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u7f13\u5b58\u7ed3\u679c"}),"\uff1a\u76f8\u4f3c\u8bf7\u6c42\u590d\u7528\u7ed3\u679c"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u672c\u5730\u6a21\u578b"}),"\uff1a\u8003\u8651\u4f7f\u7528\u672c\u5730\u6a21\u578b\u964d\u4f4e\u5ef6\u8fdf"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u5b89\u5168\u8fc7\u6ee4"}),"\uff1a\u8fc7\u6ee4\u654f\u611f\u4ee3\u7801\u548c\u51ed\u8bc1"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://github.com/features/copilot",children:"GitHub Copilot"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://continue.dev/",children:"Continue.dev"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://cursor.sh/",children:"Cursor"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},48885:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>l});var s=t(99378);const r={},o=s.createContext(r);function i(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:i(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);