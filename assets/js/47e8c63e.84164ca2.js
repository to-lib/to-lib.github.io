"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[1716],{1094:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ai/cost-optimization","title":"\ud83d\udcb0 \u6210\u672c\u4f18\u5316","description":"AI \u5e94\u7528\u7684\u6210\u672c\u4e3b\u8981\u6765\u81ea API \u8c03\u7528\u8d39\u7528\u3002\u672c\u6587\u4ecb\u7ecd\u5404\u79cd\u964d\u4f4e\u6210\u672c\u7684\u7b56\u7565\u3002","source":"@site/docs/ai/cost-optimization.md","sourceDirName":"ai","slug":"/ai/cost-optimization","permalink":"/docs/ai/cost-optimization","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/cost-optimization.md","tags":[],"version":"current","sidebarPosition":27,"frontMatter":{"sidebar_position":27,"title":"\ud83d\udcb0 \u6210\u672c\u4f18\u5316"}}');var i=t(22714),r=t(48885);const l={sidebar_position:27,title:"\ud83d\udcb0 \u6210\u672c\u4f18\u5316"},o="\u6210\u672c\u4f18\u5316",c={},d=[{value:"\u6210\u672c\u6784\u6210",id:"\u6210\u672c\u6784\u6210",level:2},{value:"\u4e3b\u6d41\u6a21\u578b\u4ef7\u683c\uff082025\uff09",id:"\u4e3b\u6d41\u6a21\u578b\u4ef7\u683c2025",level:3},{value:"\u7b56\u7565 1: \u6a21\u578b\u9009\u62e9",id:"\u7b56\u7565-1-\u6a21\u578b\u9009\u62e9",level:2},{value:"\u6309\u4efb\u52a1\u9009\u62e9\u6a21\u578b",id:"\u6309\u4efb\u52a1\u9009\u62e9\u6a21\u578b",level:3},{value:"\u7ea7\u8054\u8c03\u7528",id:"\u7ea7\u8054\u8c03\u7528",level:3},{value:"\u7b56\u7565 2: Token \u4f18\u5316",id:"\u7b56\u7565-2-token-\u4f18\u5316",level:2},{value:"\u7cbe\u7b80 Prompt",id:"\u7cbe\u7b80-prompt",level:3},{value:"\u538b\u7f29\u4e0a\u4e0b\u6587",id:"\u538b\u7f29\u4e0a\u4e0b\u6587",level:3},{value:"\u9650\u5236\u8f93\u51fa\u957f\u5ea6",id:"\u9650\u5236\u8f93\u51fa\u957f\u5ea6",level:3},{value:"\u7b56\u7565 3: \u7f13\u5b58",id:"\u7b56\u7565-3-\u7f13\u5b58",level:2},{value:"\u8bed\u4e49\u7f13\u5b58",id:"\u8bed\u4e49\u7f13\u5b58",level:3},{value:"Prompt Caching",id:"prompt-caching",level:3},{value:"\u7b56\u7565 4: \u6279\u5904\u7406",id:"\u7b56\u7565-4-\u6279\u5904\u7406",level:2},{value:"\u6279\u91cf\u8bf7\u6c42",id:"\u6279\u91cf\u8bf7\u6c42",level:3},{value:"OpenAI Batch API",id:"openai-batch-api",level:3},{value:"\u7b56\u7565 5: \u672c\u5730\u6a21\u578b",id:"\u7b56\u7565-5-\u672c\u5730\u6a21\u578b",level:2},{value:"\u7b56\u7565 6: \u6210\u672c\u76d1\u63a7",id:"\u7b56\u7565-6-\u6210\u672c\u76d1\u63a7",level:2},{value:"Token \u8ba1\u6570",id:"token-\u8ba1\u6570",level:3},{value:"\u4f7f\u7528\u91cf\u8ffd\u8e2a",id:"\u4f7f\u7528\u91cf\u8ffd\u8e2a",level:3},{value:"\u6210\u672c\u4f18\u5316\u6e05\u5355",id:"\u6210\u672c\u4f18\u5316\u6e05\u5355",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function a(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"\u6210\u672c\u4f18\u5316",children:"\u6210\u672c\u4f18\u5316"})}),"\n",(0,i.jsx)(e.p,{children:"AI \u5e94\u7528\u7684\u6210\u672c\u4e3b\u8981\u6765\u81ea API \u8c03\u7528\u8d39\u7528\u3002\u672c\u6587\u4ecb\u7ecd\u5404\u79cd\u964d\u4f4e\u6210\u672c\u7684\u7b56\u7565\u3002"}),"\n",(0,i.jsx)(e.h2,{id:"\u6210\u672c\u6784\u6210",children:"\u6210\u672c\u6784\u6210"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u603b\u6210\u672c = \u8f93\u5165 Token \xd7 \u8f93\u5165\u4ef7\u683c + \u8f93\u51fa Token \xd7 \u8f93\u51fa\u4ef7\u683c\n"})}),"\n",(0,i.jsx)(e.h3,{id:"\u4e3b\u6d41\u6a21\u578b\u4ef7\u683c2025",children:"\u4e3b\u6d41\u6a21\u578b\u4ef7\u683c\uff082025\uff09"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"\u6a21\u578b"}),(0,i.jsx)(e.th,{children:"\u8f93\u5165\u4ef7\u683c"}),(0,i.jsx)(e.th,{children:"\u8f93\u51fa\u4ef7\u683c"}),(0,i.jsx)(e.th,{children:"\u7279\u70b9"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"GPT-4o"}),(0,i.jsx)(e.td,{children:"$2.50/1M"}),(0,i.jsx)(e.td,{children:"$10.00/1M"}),(0,i.jsx)(e.td,{children:"\u6027\u80fd\u6700\u5f3a"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"GPT-4o-mini"}),(0,i.jsx)(e.td,{children:"$0.15/1M"}),(0,i.jsx)(e.td,{children:"$0.60/1M"}),(0,i.jsx)(e.td,{children:"\u6027\u4ef7\u6bd4\u9ad8"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Claude 3.5 Sonnet"}),(0,i.jsx)(e.td,{children:"$3.00/1M"}),(0,i.jsx)(e.td,{children:"$15.00/1M"}),(0,i.jsx)(e.td,{children:"\u957f\u4e0a\u4e0b\u6587"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Claude 3.5 Haiku"}),(0,i.jsx)(e.td,{children:"$0.80/1M"}),(0,i.jsx)(e.td,{children:"$4.00/1M"}),(0,i.jsx)(e.td,{children:"\u5feb\u901f\u54cd\u5e94"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Gemini 1.5 Pro"}),(0,i.jsx)(e.td,{children:"$1.25/1M"}),(0,i.jsx)(e.td,{children:"$5.00/1M"}),(0,i.jsx)(e.td,{children:"\u8d85\u957f\u4e0a\u4e0b\u6587"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Gemini 1.5 Flash"}),(0,i.jsx)(e.td,{children:"$0.075/1M"}),(0,i.jsx)(e.td,{children:"$0.30/1M"}),(0,i.jsx)(e.td,{children:"\u6781\u81f4\u6027\u4ef7\u6bd4"})]})]})]}),"\n",(0,i.jsx)(e.h2,{id:"\u7b56\u7565-1-\u6a21\u578b\u9009\u62e9",children:"\u7b56\u7565 1: \u6a21\u578b\u9009\u62e9"}),"\n",(0,i.jsx)(e.h3,{id:"\u6309\u4efb\u52a1\u9009\u62e9\u6a21\u578b",children:"\u6309\u4efb\u52a1\u9009\u62e9\u6a21\u578b"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\ndef select_model(task_type: str) -> str:\n    """\u6839\u636e\u4efb\u52a1\u7c7b\u578b\u9009\u62e9\u6a21\u578b"""\n    model_map = {\n        "simple_qa": "gpt-4o-mini",      # \u7b80\u5355\u95ee\u7b54\n        "classification": "gpt-4o-mini",  # \u5206\u7c7b\u4efb\u52a1\n        "summarization": "gpt-4o-mini",   # \u6458\u8981\n        "code_generation": "gpt-4o",      # \u4ee3\u7801\u751f\u6210\n        "complex_reasoning": "o1-mini",   # \u590d\u6742\u63a8\u7406\n        "creative_writing": "gpt-4o",     # \u521b\u610f\u5199\u4f5c\n    }\n    return model_map.get(task_type, "gpt-4o-mini")\n\n\ndef smart_query(query: str, task_type: str) -> str:\n    model = select_model(task_type)\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{"role": "user", "content": query}]\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,i.jsx)(e.h3,{id:"\u7ea7\u8054\u8c03\u7528",children:"\u7ea7\u8054\u8c03\u7528"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def cascade_query(query: str) -> str:\n    """\u5148\u7528\u5c0f\u6a21\u578b\uff0c\u4e0d\u786e\u5b9a\u65f6\u7528\u5927\u6a21\u578b"""\n    # \u7b2c\u4e00\u6b21\u5c1d\u8bd5\uff1a\u5c0f\u6a21\u578b\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {\n                "role": "system",\n                "content": "\u56de\u7b54\u95ee\u9898\u3002\u5982\u679c\u4e0d\u786e\u5b9a\uff0c\u56de\u590d \'UNCERTAIN\'\u3002"\n            },\n            {"role": "user", "content": query}\n        ]\n    )\n    \n    answer = response.choices[0].message.content\n    \n    # \u5982\u679c\u4e0d\u786e\u5b9a\uff0c\u5347\u7ea7\u5230\u5927\u6a21\u578b\n    if "UNCERTAIN" in answer:\n        response = client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": query}]\n        )\n        answer = response.choices[0].message.content\n    \n    return answer\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u7b56\u7565-2-token-\u4f18\u5316",children:"\u7b56\u7565 2: Token \u4f18\u5316"}),"\n",(0,i.jsx)(e.h3,{id:"\u7cbe\u7b80-prompt",children:"\u7cbe\u7b80 Prompt"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u274c \u5197\u957f\u7684 Prompt\nbad_prompt = """\n\u4f60\u662f\u4e00\u4e2a\u975e\u5e38\u4e13\u4e1a\u7684\u3001\u7ecf\u9a8c\u4e30\u5bcc\u7684\u3001\u77e5\u8bc6\u6e0a\u535a\u7684\u52a9\u624b\u3002\n\u4f60\u9700\u8981\u4ed4\u7ec6\u5730\u3001\u8ba4\u771f\u5730\u3001\u5168\u9762\u5730\u56de\u7b54\u7528\u6237\u7684\u95ee\u9898\u3002\n\u8bf7\u786e\u4fdd\u4f60\u7684\u56de\u7b54\u662f\u51c6\u786e\u7684\u3001\u6709\u5e2e\u52a9\u7684\u3001\u8be6\u7ec6\u7684\u3002\n\u7528\u6237\u7684\u95ee\u9898\u662f\uff1a{question}\n"""\n\n# \u2705 \u7cbe\u7b80\u7684 Prompt\ngood_prompt = """\n\u7b80\u6d01\u56de\u7b54\uff1a{question}\n"""\n'})}),"\n",(0,i.jsx)(e.h3,{id:"\u538b\u7f29\u4e0a\u4e0b\u6587",children:"\u538b\u7f29\u4e0a\u4e0b\u6587"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def compress_context(context: str, max_tokens: int = 2000) -> str:\n    """\u538b\u7f29\u4e0a\u4e0b\u6587"""\n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[\n            {\n                "role": "system",\n                "content": f"\u5c06\u4ee5\u4e0b\u5185\u5bb9\u538b\u7f29\u5230 {max_tokens} tokens \u4ee5\u5185\uff0c\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002"\n            },\n            {"role": "user", "content": context}\n        ],\n        max_tokens=max_tokens\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,i.jsx)(e.h3,{id:"\u9650\u5236\u8f93\u51fa\u957f\u5ea6",children:"\u9650\u5236\u8f93\u51fa\u957f\u5ea6"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'response = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[{"role": "user", "content": query}],\n    max_tokens=500  # \u9650\u5236\u8f93\u51fa\u957f\u5ea6\n)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u7b56\u7565-3-\u7f13\u5b58",children:"\u7b56\u7565 3: \u7f13\u5b58"}),"\n",(0,i.jsx)(e.h3,{id:"\u8bed\u4e49\u7f13\u5b58",children:"\u8bed\u4e49\u7f13\u5b58"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nimport hashlib\n\nclass SemanticCache:\n    """\u8bed\u4e49\u7f13\u5b58"""\n    \n    def __init__(self, similarity_threshold: float = 0.95):\n        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")\n        self.vectorstore = Chroma(embedding_function=self.embeddings)\n        self.cache = {}  # query_id -> response\n        self.threshold = similarity_threshold\n    \n    def _get_similar(self, query: str) -> str | None:\n        """\u67e5\u627e\u76f8\u4f3c\u67e5\u8be2"""\n        results = self.vectorstore.similarity_search_with_score(query, k=1)\n        \n        if results and results[0][1] >= self.threshold:\n            query_id = results[0][0].metadata.get("query_id")\n            return self.cache.get(query_id)\n        \n        return None\n    \n    def get(self, query: str) -> str | None:\n        return self._get_similar(query)\n    \n    def set(self, query: str, response: str):\n        query_id = hashlib.md5(query.encode()).hexdigest()\n        self.cache[query_id] = response\n        self.vectorstore.add_texts(\n            texts=[query],\n            metadatas=[{"query_id": query_id}]\n        )\n    \n    def query(self, query: str) -> str:\n        # \u68c0\u67e5\u7f13\u5b58\n        cached = self.get(query)\n        if cached:\n            print("Cache hit!")\n            return cached\n        \n        # \u8c03\u7528 API\n        response = client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": query}]\n        )\n        \n        result = response.choices[0].message.content\n        self.set(query, result)\n        return result\n'})}),"\n",(0,i.jsx)(e.h3,{id:"prompt-caching",children:"Prompt Caching"}),"\n",(0,i.jsxs)(e.p,{children:["\u53c2\u8003 ",(0,i.jsx)(e.a,{href:"./prompt-caching",children:"Prompt Caching"})," \u6587\u6863\u3002"]}),"\n",(0,i.jsx)(e.h2,{id:"\u7b56\u7565-4-\u6279\u5904\u7406",children:"\u7b56\u7565 4: \u6279\u5904\u7406"}),"\n",(0,i.jsx)(e.h3,{id:"\u6279\u91cf\u8bf7\u6c42",children:"\u6279\u91cf\u8bf7\u6c42"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import asyncio\nfrom openai import AsyncOpenAI\n\nasync_client = AsyncOpenAI()\n\nasync def batch_process(queries: list[str], batch_size: int = 10) -> list[str]:\n    """\u6279\u91cf\u5904\u7406\u8bf7\u6c42"""\n    results = []\n    \n    for i in range(0, len(queries), batch_size):\n        batch = queries[i:i + batch_size]\n        \n        tasks = [\n            async_client.chat.completions.create(\n                model="gpt-4o-mini",\n                messages=[{"role": "user", "content": q}]\n            )\n            for q in batch\n        ]\n        \n        responses = await asyncio.gather(*tasks)\n        results.extend([r.choices[0].message.content for r in responses])\n    \n    return results\n'})}),"\n",(0,i.jsx)(e.h3,{id:"openai-batch-api",children:"OpenAI Batch API"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import json\n\ndef create_batch_file(requests: list[dict], filename: str):\n    """\u521b\u5efa\u6279\u5904\u7406\u6587\u4ef6"""\n    with open(filename, \'w\') as f:\n        for i, req in enumerate(requests):\n            line = {\n                "custom_id": f"request-{i}",\n                "method": "POST",\n                "url": "/v1/chat/completions",\n                "body": {\n                    "model": "gpt-4o-mini",\n                    "messages": req["messages"]\n                }\n            }\n            f.write(json.dumps(line) + "\\n")\n\n# \u4e0a\u4f20\u6587\u4ef6\nbatch_file = client.files.create(\n    file=open("batch_requests.jsonl", "rb"),\n    purpose="batch"\n)\n\n# \u521b\u5efa\u6279\u5904\u7406\u4efb\u52a1\nbatch = client.batches.create(\n    input_file_id=batch_file.id,\n    endpoint="/v1/chat/completions",\n    completion_window="24h"  # 24 \u5c0f\u65f6\u5185\u5b8c\u6210\uff0c\u4ef7\u683c\u51cf\u534a\n)\n\n# \u68c0\u67e5\u72b6\u6001\nstatus = client.batches.retrieve(batch.id)\nprint(f"Status: {status.status}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u7b56\u7565-5-\u672c\u5730\u6a21\u578b",children:"\u7b56\u7565 5: \u672c\u5730\u6a21\u578b"}),"\n",(0,i.jsx)(e.p,{children:"\u5bf9\u4e8e\u9ad8\u9891\u3001\u4f4e\u590d\u6742\u5ea6\u4efb\u52a1\uff0c\u4f7f\u7528\u672c\u5730\u6a21\u578b\u3002"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import ollama\n\ndef local_or_cloud(query: str, complexity: str = "low") -> str:\n    """\u6839\u636e\u590d\u6742\u5ea6\u9009\u62e9\u672c\u5730\u6216\u4e91\u7aef\u6a21\u578b"""\n    if complexity == "low":\n        # \u4f7f\u7528\u672c\u5730\u6a21\u578b\n        response = ollama.chat(\n            model="qwen2.5:7b",\n            messages=[{"role": "user", "content": query}]\n        )\n        return response["message"]["content"]\n    else:\n        # \u4f7f\u7528\u4e91\u7aef\u6a21\u578b\n        response = client.chat.completions.create(\n            model="gpt-4o",\n            messages=[{"role": "user", "content": query}]\n        )\n        return response.choices[0].message.content\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u7b56\u7565-6-\u6210\u672c\u76d1\u63a7",children:"\u7b56\u7565 6: \u6210\u672c\u76d1\u63a7"}),"\n",(0,i.jsx)(e.h3,{id:"token-\u8ba1\u6570",children:"Token \u8ba1\u6570"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import tiktoken\n\ndef count_tokens(text: str, model: str = "gpt-4o") -> int:\n    """\u8ba1\u7b97 token \u6570\u91cf"""\n    enc = tiktoken.encoding_for_model(model)\n    return len(enc.encode(text))\n\ndef estimate_cost(\n    input_text: str,\n    output_tokens: int,\n    model: str = "gpt-4o"\n) -> float:\n    """\u4f30\u7b97\u6210\u672c"""\n    prices = {\n        "gpt-4o": {"input": 2.50, "output": 10.00},\n        "gpt-4o-mini": {"input": 0.15, "output": 0.60},\n    }\n    \n    input_tokens = count_tokens(input_text, model)\n    price = prices.get(model, prices["gpt-4o"])\n    \n    cost = (input_tokens * price["input"] + output_tokens * price["output"]) / 1_000_000\n    return cost\n'})}),"\n",(0,i.jsx)(e.h3,{id:"\u4f7f\u7528\u91cf\u8ffd\u8e2a",children:"\u4f7f\u7528\u91cf\u8ffd\u8e2a"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from dataclasses import dataclass, field\nfrom datetime import datetime\n\n@dataclass\nclass UsageTracker:\n    """\u4f7f\u7528\u91cf\u8ffd\u8e2a"""\n    total_input_tokens: int = 0\n    total_output_tokens: int = 0\n    total_cost: float = 0.0\n    requests: int = 0\n    daily_stats: dict = field(default_factory=dict)\n    \n    def record(self, input_tokens: int, output_tokens: int, model: str):\n        self.requests += 1\n        self.total_input_tokens += input_tokens\n        self.total_output_tokens += output_tokens\n        \n        # \u8ba1\u7b97\u6210\u672c\n        prices = {\n            "gpt-4o": {"input": 2.50, "output": 10.00},\n            "gpt-4o-mini": {"input": 0.15, "output": 0.60},\n        }\n        price = prices.get(model, prices["gpt-4o"])\n        cost = (input_tokens * price["input"] + output_tokens * price["output"]) / 1_000_000\n        self.total_cost += cost\n        \n        # \u6309\u65e5\u7edf\u8ba1\n        today = datetime.now().strftime("%Y-%m-%d")\n        if today not in self.daily_stats:\n            self.daily_stats[today] = {"tokens": 0, "cost": 0}\n        self.daily_stats[today]["tokens"] += input_tokens + output_tokens\n        self.daily_stats[today]["cost"] += cost\n    \n    def report(self) -> str:\n        return f"""\n\u4f7f\u7528\u62a5\u544a\uff1a\n- \u603b\u8bf7\u6c42\u6570\uff1a{self.requests}\n- \u603b\u8f93\u5165 tokens\uff1a{self.total_input_tokens:,}\n- \u603b\u8f93\u51fa tokens\uff1a{self.total_output_tokens:,}\n- \u603b\u6210\u672c\uff1a${self.total_cost:.4f}\n"""\n\ntracker = UsageTracker()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u6210\u672c\u4f18\u5316\u6e05\u5355",children:"\u6210\u672c\u4f18\u5316\u6e05\u5355"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"\u7b56\u7565"}),(0,i.jsx)(e.th,{children:"\u8282\u7701\u6bd4\u4f8b"}),(0,i.jsx)(e.th,{children:"\u9002\u7528\u573a\u666f"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"\u4f7f\u7528\u5c0f\u6a21\u578b"}),(0,i.jsx)(e.td,{children:"90%+"}),(0,i.jsx)(e.td,{children:"\u7b80\u5355\u4efb\u52a1"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Prompt Caching"}),(0,i.jsx)(e.td,{children:"50-90%"}),(0,i.jsx)(e.td,{children:"\u91cd\u590d\u524d\u7f00"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"\u6279\u5904\u7406 API"}),(0,i.jsx)(e.td,{children:"50%"}),(0,i.jsx)(e.td,{children:"\u975e\u5b9e\u65f6\u4efb\u52a1"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"\u8bed\u4e49\u7f13\u5b58"}),(0,i.jsx)(e.td,{children:"\u53d8\u5316\u5927"}),(0,i.jsx)(e.td,{children:"\u91cd\u590d\u67e5\u8be2"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"\u672c\u5730\u6a21\u578b"}),(0,i.jsx)(e.td,{children:"100%"}),(0,i.jsx)(e.td,{children:"\u9ad8\u9891\u4f4e\u590d\u6742\u5ea6"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"\u538b\u7f29\u4e0a\u4e0b\u6587"}),(0,i.jsx)(e.td,{children:"30-50%"}),(0,i.jsx)(e.td,{children:"\u957f\u6587\u6863"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"\u9650\u5236\u8f93\u51fa"}),(0,i.jsx)(e.td,{children:"20-40%"}),(0,i.jsx)(e.td,{children:"\u7b80\u6d01\u56de\u7b54"})]})]})]}),"\n",(0,i.jsx)(e.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u76d1\u63a7\u5148\u884c"}),"\uff1a\u5148\u4e86\u89e3\u6210\u672c\u5206\u5e03\u518d\u4f18\u5316"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u5206\u7ea7\u5904\u7406"}),"\uff1a\u4e0d\u540c\u4efb\u52a1\u7528\u4e0d\u540c\u6a21\u578b"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u7f13\u5b58\u4f18\u5148"}),"\uff1a\u76f8\u4f3c\u67e5\u8be2\u590d\u7528\u7ed3\u679c"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u6279\u91cf\u5904\u7406"}),"\uff1a\u975e\u5b9e\u65f6\u4efb\u52a1\u7528 Batch API"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u8bbe\u7f6e\u9884\u7b97"}),"\uff1a\u914d\u7f6e\u7528\u91cf\u544a\u8b66"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://openai.com/pricing",children:"OpenAI Pricing"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/batch",children:"OpenAI Batch API"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://github.com/openai/tiktoken",children:"tiktoken"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(a,{...n})}):a(n)}},48885:(n,e,t)=>{t.d(e,{R:()=>l,x:()=>o});var s=t(99378);const i={},r=s.createContext(i);function l(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:l(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);