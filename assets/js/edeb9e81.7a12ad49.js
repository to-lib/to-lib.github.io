"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[18642],{16779:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ai/voice","title":"\ud83c\udf99\ufe0f \u8bed\u97f3\u4ea4\u4e92","description":"\u8bed\u97f3\u4ea4\u4e92\u8ba9 AI \u5e94\u7528\u80fd\u591f\u542c\u548c\u8bf4\uff0c\u5305\u62ec\u8bed\u97f3\u8bc6\u522b\uff08STT\uff09\u3001\u8bed\u97f3\u5408\u6210\uff08TTS\uff09\u548c\u5b9e\u65f6\u8bed\u97f3\u5bf9\u8bdd\u3002","source":"@site/docs/ai/voice.md","sourceDirName":"ai","slug":"/ai/voice","permalink":"/docs/ai/voice","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/voice.md","tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"title":"\ud83c\udf99\ufe0f \u8bed\u97f3\u4ea4\u4e92"},"sidebar":"ai","previous":{"title":"\ud83e\udde9 Mixture of Experts","permalink":"/docs/ai/moe"},"next":{"title":"\ud83d\udddc\ufe0f \u6a21\u578b\u91cf\u5316","permalink":"/docs/ai/quantization"}}');var i=t(22714),a=t(48885);const r={sidebar_position:30,title:"\ud83c\udf99\ufe0f \u8bed\u97f3\u4ea4\u4e92"},o="\u8bed\u97f3\u4ea4\u4e92",l={},d=[{value:"\u6280\u672f\u6808",id:"\u6280\u672f\u6808",level:2},{value:"OpenAI \u8bed\u97f3 API",id:"openai-\u8bed\u97f3-api",level:2},{value:"\u8bed\u97f3\u5408\u6210\uff08TTS\uff09",id:"\u8bed\u97f3\u5408\u6210tts",level:3},{value:"\u6d41\u5f0f TTS",id:"\u6d41\u5f0f-tts",level:3},{value:"\u8bed\u97f3\u8bc6\u522b\uff08STT\uff09",id:"\u8bed\u97f3\u8bc6\u522bstt",level:3},{value:"OpenAI Realtime API",id:"openai-realtime-api",level:2},{value:"WebSocket \u8fde\u63a5",id:"websocket-\u8fde\u63a5",level:3},{value:"\u53d1\u9001\u97f3\u9891",id:"\u53d1\u9001\u97f3\u9891",level:3},{value:"\u5b8c\u6574\u793a\u4f8b",id:"\u5b8c\u6574\u793a\u4f8b",level:3},{value:"\u5176\u4ed6\u8bed\u97f3\u670d\u52a1",id:"\u5176\u4ed6\u8bed\u97f3\u670d\u52a1",level:2},{value:"Azure Speech",id:"azure-speech",level:3},{value:"ElevenLabs",id:"elevenlabs",level:3},{value:"\u8bed\u97f3\u52a9\u624b\u67b6\u6784",id:"\u8bed\u97f3\u52a9\u624b\u67b6\u6784",level:2},{value:"\u5b9e\u65f6\u8bed\u97f3\u7ffb\u8bd1",id:"\u5b9e\u65f6\u8bed\u97f3\u7ffb\u8bd1",level:2},{value:"\u4ef7\u683c\u53c2\u8003",id:"\u4ef7\u683c\u53c2\u8003",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"\u8bed\u97f3\u4ea4\u4e92",children:"\u8bed\u97f3\u4ea4\u4e92"})}),"\n",(0,i.jsx)(n.p,{children:"\u8bed\u97f3\u4ea4\u4e92\u8ba9 AI \u5e94\u7528\u80fd\u591f\u542c\u548c\u8bf4\uff0c\u5305\u62ec\u8bed\u97f3\u8bc6\u522b\uff08STT\uff09\u3001\u8bed\u97f3\u5408\u6210\uff08TTS\uff09\u548c\u5b9e\u65f6\u8bed\u97f3\u5bf9\u8bdd\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"\u6280\u672f\u6808",children:"\u6280\u672f\u6808"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u8bed\u97f3\u8f93\u5165 \u2500\u2500> STT \u2500\u2500> \u6587\u672c \u2500\u2500> LLM \u2500\u2500> \u6587\u672c \u2500\u2500> TTS \u2500\u2500> \u8bed\u97f3\u8f93\u51fa\n                              \u2502\n                    Realtime API\uff08\u7aef\u5230\u7aef\uff09\n"})}),"\n",(0,i.jsx)(n.h2,{id:"openai-\u8bed\u97f3-api",children:"OpenAI \u8bed\u97f3 API"}),"\n",(0,i.jsx)(n.h3,{id:"\u8bed\u97f3\u5408\u6210tts",children:"\u8bed\u97f3\u5408\u6210\uff08TTS\uff09"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nfrom pathlib import Path\n\nclient = OpenAI()\n\ndef text_to_speech(text: str, output_file: str = "output.mp3"):\n    """\u6587\u672c\u8f6c\u8bed\u97f3"""\n    response = client.audio.speech.create(\n        model="tts-1",  # tts-1 \u6216 tts-1-hd\n        voice="alloy",  # alloy, echo, fable, onyx, nova, shimmer\n        input=text\n    )\n    \n    response.stream_to_file(Path(output_file))\n    return output_file\n\n# \u4f7f\u7528\ntext_to_speech("\u4f60\u597d\uff0c\u6211\u662f AI \u52a9\u624b\u3002", "greeting.mp3")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"\u6d41\u5f0f-tts",children:"\u6d41\u5f0f TTS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def stream_tts(text: str):\n    """\u6d41\u5f0f\u8bed\u97f3\u5408\u6210"""\n    response = client.audio.speech.create(\n        model="tts-1",\n        voice="nova",\n        input=text,\n        response_format="pcm"  # \u6d41\u5f0f\u9700\u8981 pcm \u683c\u5f0f\n    )\n    \n    # \u6d41\u5f0f\u64ad\u653e\n    for chunk in response.iter_bytes(chunk_size=1024):\n        yield chunk\n\n# \u914d\u5408\u97f3\u9891\u64ad\u653e\u5e93\u4f7f\u7528\nimport pyaudio\n\ndef play_stream(text: str):\n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16, channels=1, rate=24000, output=True)\n    \n    for chunk in stream_tts(text):\n        stream.write(chunk)\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"\u8bed\u97f3\u8bc6\u522bstt",children:"\u8bed\u97f3\u8bc6\u522b\uff08STT\uff09"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def speech_to_text(audio_file: str) -> str:\n    """\u8bed\u97f3\u8f6c\u6587\u672c"""\n    with open(audio_file, "rb") as f:\n        transcript = client.audio.transcriptions.create(\n            model="whisper-1",\n            file=f,\n            response_format="text"\n        )\n    return transcript\n\n# \u5e26\u65f6\u95f4\u6233\ndef speech_to_text_with_timestamps(audio_file: str) -> dict:\n    """\u8bed\u97f3\u8f6c\u6587\u672c\uff08\u5e26\u65f6\u95f4\u6233\uff09"""\n    with open(audio_file, "rb") as f:\n        transcript = client.audio.transcriptions.create(\n            model="whisper-1",\n            file=f,\n            response_format="verbose_json",\n            timestamp_granularities=["word", "segment"]\n        )\n    return transcript\n\n# \u7ffb\u8bd1\uff08\u4efb\u610f\u8bed\u8a00\u8f6c\u82f1\u8bed\uff09\ndef translate_audio(audio_file: str) -> str:\n    """\u8bed\u97f3\u7ffb\u8bd1"""\n    with open(audio_file, "rb") as f:\n        translation = client.audio.translations.create(\n            model="whisper-1",\n            file=f\n        )\n    return translation.text\n'})}),"\n",(0,i.jsx)(n.h2,{id:"openai-realtime-api",children:"OpenAI Realtime API"}),"\n",(0,i.jsx)(n.p,{children:"Realtime API \u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u5b9e\u65f6\u8bed\u97f3\u5bf9\u8bdd\u80fd\u529b\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"websocket-\u8fde\u63a5",children:"WebSocket \u8fde\u63a5"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport websockets\nimport json\nimport base64\n\nasync def realtime_conversation():\n    """\u5b9e\u65f6\u8bed\u97f3\u5bf9\u8bdd"""\n    url = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview"\n    headers = {\n        "Authorization": f"Bearer {os.environ[\'OPENAI_API_KEY\']}",\n        "OpenAI-Beta": "realtime=v1"\n    }\n    \n    async with websockets.connect(url, extra_headers=headers) as ws:\n        # \u914d\u7f6e\u4f1a\u8bdd\n        await ws.send(json.dumps({\n            "type": "session.update",\n            "session": {\n                "modalities": ["text", "audio"],\n                "instructions": "\u4f60\u662f\u4e00\u4e2a\u53cb\u597d\u7684\u52a9\u624b\uff0c\u7528\u4e2d\u6587\u56de\u7b54\u3002",\n                "voice": "alloy",\n                "input_audio_format": "pcm16",\n                "output_audio_format": "pcm16",\n                "turn_detection": {\n                    "type": "server_vad",  # \u670d\u52a1\u7aef\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\n                    "threshold": 0.5\n                }\n            }\n        }))\n        \n        # \u5904\u7406\u6d88\u606f\n        async for message in ws:\n            event = json.loads(message)\n            await handle_event(event)\n\nasync def handle_event(event: dict):\n    """\u5904\u7406 Realtime API \u4e8b\u4ef6"""\n    event_type = event.get("type")\n    \n    if event_type == "response.audio.delta":\n        # \u6536\u5230\u97f3\u9891\u6570\u636e\n        audio_data = base64.b64decode(event["delta"])\n        # \u64ad\u653e\u97f3\u9891...\n        \n    elif event_type == "response.text.delta":\n        # \u6536\u5230\u6587\u672c\n        print(event["delta"], end="", flush=True)\n        \n    elif event_type == "response.done":\n        print("\\n--- \u56de\u590d\u5b8c\u6210 ---")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"\u53d1\u9001\u97f3\u9891",children:"\u53d1\u9001\u97f3\u9891"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def send_audio(ws, audio_data: bytes):\n    """\u53d1\u9001\u97f3\u9891\u6570\u636e"""\n    await ws.send(json.dumps({\n        "type": "input_audio_buffer.append",\n        "audio": base64.b64encode(audio_data).decode()\n    }))\n\nasync def commit_audio(ws):\n    """\u63d0\u4ea4\u97f3\u9891\u7f13\u51b2\u533a"""\n    await ws.send(json.dumps({\n        "type": "input_audio_buffer.commit"\n    }))\n    \n    # \u8bf7\u6c42\u54cd\u5e94\n    await ws.send(json.dumps({\n        "type": "response.create"\n    }))\n'})}),"\n",(0,i.jsx)(n.h3,{id:"\u5b8c\u6574\u793a\u4f8b",children:"\u5b8c\u6574\u793a\u4f8b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import sounddevice as sd\nimport numpy as np\n\nclass RealtimeVoiceChat:\n    """\u5b9e\u65f6\u8bed\u97f3\u804a\u5929"""\n    \n    def __init__(self):\n        self.sample_rate = 24000\n        self.channels = 1\n        self.ws = None\n    \n    async def connect(self):\n        url = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview"\n        headers = {\n            "Authorization": f"Bearer {os.environ[\'OPENAI_API_KEY\']}",\n            "OpenAI-Beta": "realtime=v1"\n        }\n        self.ws = await websockets.connect(url, extra_headers=headers)\n        \n        # \u914d\u7f6e\n        await self.ws.send(json.dumps({\n            "type": "session.update",\n            "session": {\n                "modalities": ["text", "audio"],\n                "voice": "nova",\n                "input_audio_format": "pcm16",\n                "output_audio_format": "pcm16"\n            }\n        }))\n    \n    async def start(self):\n        await self.connect()\n        \n        # \u542f\u52a8\u5f55\u97f3\u548c\u64ad\u653e\n        asyncio.create_task(self.record_audio())\n        asyncio.create_task(self.receive_messages())\n    \n    async def record_audio(self):\n        """\u5f55\u5236\u97f3\u9891\u5e76\u53d1\u9001"""\n        def callback(indata, frames, time, status):\n            audio_bytes = indata.tobytes()\n            asyncio.create_task(self.send_audio(audio_bytes))\n        \n        with sd.InputStream(\n            samplerate=self.sample_rate,\n            channels=self.channels,\n            dtype=np.int16,\n            callback=callback\n        ):\n            await asyncio.sleep(float(\'inf\'))\n    \n    async def send_audio(self, audio_data: bytes):\n        if self.ws:\n            await self.ws.send(json.dumps({\n                "type": "input_audio_buffer.append",\n                "audio": base64.b64encode(audio_data).decode()\n            }))\n    \n    async def receive_messages(self):\n        """\u63a5\u6536\u5e76\u5904\u7406\u6d88\u606f"""\n        audio_buffer = []\n        \n        async for message in self.ws:\n            event = json.loads(message)\n            \n            if event["type"] == "response.audio.delta":\n                audio_data = base64.b64decode(event["delta"])\n                audio_buffer.append(audio_data)\n                \n            elif event["type"] == "response.audio.done":\n                # \u64ad\u653e\u5b8c\u6574\u97f3\u9891\n                full_audio = b"".join(audio_buffer)\n                self.play_audio(full_audio)\n                audio_buffer = []\n    \n    def play_audio(self, audio_data: bytes):\n        """\u64ad\u653e\u97f3\u9891"""\n        audio_array = np.frombuffer(audio_data, dtype=np.int16)\n        sd.play(audio_array, self.sample_rate)\n        sd.wait()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"\u5176\u4ed6\u8bed\u97f3\u670d\u52a1",children:"\u5176\u4ed6\u8bed\u97f3\u670d\u52a1"}),"\n",(0,i.jsx)(n.h3,{id:"azure-speech",children:"Azure Speech"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import azure.cognitiveservices.speech as speechsdk\n\n# \u914d\u7f6e\nspeech_config = speechsdk.SpeechConfig(\n    subscription="your_key",\n    region="eastus"\n)\nspeech_config.speech_synthesis_voice_name = "zh-CN-XiaoxiaoNeural"\n\n# TTS\nsynthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\nresult = synthesizer.speak_text_async("\u4f60\u597d").get()\n\n# STT\naudio_config = speechsdk.AudioConfig(filename="audio.wav")\nrecognizer = speechsdk.SpeechRecognizer(\n    speech_config=speech_config,\n    audio_config=audio_config\n)\nresult = recognizer.recognize_once()\nprint(result.text)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"elevenlabs",children:"ElevenLabs"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from elevenlabs import generate, play, set_api_key\n\nset_api_key("your_api_key")\n\n# \u751f\u6210\u8bed\u97f3\naudio = generate(\n    text="Hello, this is a test.",\n    voice="Rachel",\n    model="eleven_multilingual_v2"\n)\n\nplay(audio)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"\u8bed\u97f3\u52a9\u624b\u67b6\u6784",children:"\u8bed\u97f3\u52a9\u624b\u67b6\u6784"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VoiceAssistant:\n    """\u8bed\u97f3\u52a9\u624b"""\n    \n    def __init__(self):\n        self.client = OpenAI()\n        self.conversation_history = []\n    \n    def listen(self, audio_file: str) -> str:\n        """\u542c\u53d6\u7528\u6237\u8f93\u5165"""\n        with open(audio_file, "rb") as f:\n            transcript = self.client.audio.transcriptions.create(\n                model="whisper-1",\n                file=f\n            )\n        return transcript.text\n    \n    def think(self, user_input: str) -> str:\n        """\u5904\u7406\u5e76\u751f\u6210\u56de\u590d"""\n        self.conversation_history.append({\n            "role": "user",\n            "content": user_input\n        })\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4o",\n            messages=[\n                {"role": "system", "content": "\u4f60\u662f\u4e00\u4e2a\u8bed\u97f3\u52a9\u624b\uff0c\u56de\u590d\u8981\u7b80\u6d01\u81ea\u7136\u3002"},\n                *self.conversation_history\n            ]\n        )\n        \n        assistant_message = response.choices[0].message.content\n        self.conversation_history.append({\n            "role": "assistant",\n            "content": assistant_message\n        })\n        \n        return assistant_message\n    \n    def speak(self, text: str, output_file: str = "response.mp3"):\n        """\u8bed\u97f3\u8f93\u51fa"""\n        response = self.client.audio.speech.create(\n            model="tts-1",\n            voice="nova",\n            input=text\n        )\n        response.stream_to_file(output_file)\n        return output_file\n    \n    def process(self, audio_file: str) -> str:\n        """\u5b8c\u6574\u5904\u7406\u6d41\u7a0b"""\n        # 1. \u8bed\u97f3\u8f6c\u6587\u672c\n        user_input = self.listen(audio_file)\n        print(f"\u7528\u6237\uff1a{user_input}")\n        \n        # 2. \u751f\u6210\u56de\u590d\n        response = self.think(user_input)\n        print(f"\u52a9\u624b\uff1a{response}")\n        \n        # 3. \u6587\u672c\u8f6c\u8bed\u97f3\n        output_file = self.speak(response)\n        \n        return output_file\n'})}),"\n",(0,i.jsx)(n.h2,{id:"\u5b9e\u65f6\u8bed\u97f3\u7ffb\u8bd1",children:"\u5b9e\u65f6\u8bed\u97f3\u7ffb\u8bd1"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RealtimeTranslator:\n    """\u5b9e\u65f6\u8bed\u97f3\u7ffb\u8bd1"""\n    \n    def __init__(self, source_lang: str = "zh", target_lang: str = "en"):\n        self.client = OpenAI()\n        self.source_lang = source_lang\n        self.target_lang = target_lang\n    \n    def translate(self, audio_file: str) -> tuple[str, str]:\n        """\u7ffb\u8bd1\u97f3\u9891"""\n        # 1. \u8bed\u97f3\u8bc6\u522b\n        with open(audio_file, "rb") as f:\n            transcript = self.client.audio.transcriptions.create(\n                model="whisper-1",\n                file=f,\n                language=self.source_lang\n            )\n        \n        original_text = transcript.text\n        \n        # 2. \u6587\u672c\u7ffb\u8bd1\n        response = self.client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[\n                {\n                    "role": "system",\n                    "content": f"\u5c06\u4ee5\u4e0b\u6587\u672c\u7ffb\u8bd1\u6210{self.target_lang}\uff0c\u53ea\u8fd4\u56de\u7ffb\u8bd1\u7ed3\u679c\u3002"\n                },\n                {"role": "user", "content": original_text}\n            ]\n        )\n        \n        translated_text = response.choices[0].message.content\n        \n        # 3. \u8bed\u97f3\u5408\u6210\n        audio_response = self.client.audio.speech.create(\n            model="tts-1",\n            voice="nova",\n            input=translated_text\n        )\n        \n        output_file = "translated.mp3"\n        audio_response.stream_to_file(output_file)\n        \n        return original_text, translated_text\n'})}),"\n",(0,i.jsx)(n.h2,{id:"\u4ef7\u683c\u53c2\u8003",children:"\u4ef7\u683c\u53c2\u8003"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"\u670d\u52a1"}),(0,i.jsx)(n.th,{children:"\u4ef7\u683c"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Whisper (STT)"}),(0,i.jsx)(n.td,{children:"$0.006/\u5206\u949f"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"TTS-1"}),(0,i.jsx)(n.td,{children:"$15/1M \u5b57\u7b26"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"TTS-1-HD"}),(0,i.jsx)(n.td,{children:"$30/1M \u5b57\u7b26"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Realtime API"}),(0,i.jsx)(n.td,{children:"$0.06/\u5206\u949f\uff08\u97f3\u9891\uff09+ Token \u8d39\u7528"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"\u964d\u566a\u5904\u7406"}),"\uff1a\u8f93\u5165\u97f3\u9891\u5148\u505a\u964d\u566a"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"\u5206\u6bb5\u5904\u7406"}),"\uff1a\u957f\u97f3\u9891\u5206\u6bb5\u8bc6\u522b"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"\u6d41\u5f0f\u8f93\u51fa"}),"\uff1aTTS \u4f7f\u7528\u6d41\u5f0f\u63d0\u5347\u4f53\u9a8c"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"\u9519\u8bef\u5904\u7406"}),"\uff1a\u5904\u7406\u7f51\u7edc\u4e2d\u65ad\u548c\u8bc6\u522b\u5931\u8d25"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"\u9690\u79c1\u4fdd\u62a4"}),"\uff1a\u654f\u611f\u97f3\u9891\u672c\u5730\u5904\u7406"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/speech-to-text",children:"OpenAI Audio API"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime",children:"OpenAI Realtime API"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/speech-services",children:"Azure Speech"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},48885:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(99378);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);