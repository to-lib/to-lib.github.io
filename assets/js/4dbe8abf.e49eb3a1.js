"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[19205],{48885:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>a});var s=t(99378);const r={},o=s.createContext(r);function l(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(o.Provider,{value:n},e.children)}},88102:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>a,default:()=>p,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ai/production","title":"\ud83d\ude80 Production\uff08\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72\uff09","description":"\u628a Demo \u53d8\u6210\u53ef\u7528\u7684\u751f\u4ea7\u7cfb\u7edf\uff0c\u5173\u952e\u662f\\"\u7a33\u5b9a\u3001\u53ef\u89c2\u6d4b\u3001\u53ef\u63a7\u6210\u672c\\"\u3002\u672c\u9875\u4ece\u5ef6\u8fdf\u3001\u7a33\u5b9a\u6027\u3001\u7f13\u5b58\u3001\u89c2\u6d4b\u4e0e\u53d1\u5e03\u7b56\u7565\u603b\u7ed3\u5e38\u7528\u505a\u6cd5\u3002","source":"@site/docs/ai/production.md","sourceDirName":"ai","slug":"/ai/production","permalink":"/docs/ai/production","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/production.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"\ud83d\ude80 Production\uff08\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72\uff09"},"sidebar":"ai","previous":{"title":"\ud83d\udccf Evaluation\uff08\u8bc4\u4f30\u4e0e\u6d4b\u8bd5\uff09","permalink":"/docs/ai/evaluation"},"next":{"title":"\ud83d\udd10 Security\uff08\u5b89\u5168\u4e0e\u9690\u79c1\uff09","permalink":"/docs/ai/security"}}');var r=t(22714),o=t(48885);const l={sidebar_position:11,title:"\ud83d\ude80 Production\uff08\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72\uff09"},a="Production\uff08\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72\uff09",i={},c=[{value:"\u5178\u578b\u751f\u4ea7\u67b6\u6784",id:"\u5178\u578b\u751f\u4ea7\u67b6\u6784",level:2},{value:"\u5ef6\u8fdf\u4f18\u5316",id:"\u5ef6\u8fdf\u4f18\u5316",level:2},{value:"1. \u6a21\u578b\u9009\u62e9\u7b56\u7565",id:"1-\u6a21\u578b\u9009\u62e9\u7b56\u7565",level:3},{value:"2. \u6d41\u5f0f\u8f93\u51fa",id:"2-\u6d41\u5f0f\u8f93\u51fa",level:3},{value:"3. \u5e76\u884c\u5316\u5904\u7406",id:"3-\u5e76\u884c\u5316\u5904\u7406",level:3},{value:"4. \u4e0a\u4e0b\u6587\u538b\u7f29",id:"4-\u4e0a\u4e0b\u6587\u538b\u7f29",level:3},{value:"\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519",id:"\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519",level:2},{value:"1. \u91cd\u8bd5\u7b56\u7565",id:"1-\u91cd\u8bd5\u7b56\u7565",level:3},{value:"2. \u8d85\u65f6\u4e0e\u53d6\u6d88",id:"2-\u8d85\u65f6\u4e0e\u53d6\u6d88",level:3},{value:"3. \u964d\u7ea7\u7b56\u7565",id:"3-\u964d\u7ea7\u7b56\u7565",level:3},{value:"\u7f13\u5b58\u7b56\u7565",id:"\u7f13\u5b58\u7b56\u7565",level:2},{value:"1. \u8bed\u4e49\u7f13\u5b58",id:"1-\u8bed\u4e49\u7f13\u5b58",level:3},{value:"2. Redis \u7f13\u5b58",id:"2-redis-\u7f13\u5b58",level:3},{value:"\u53ef\u89c2\u6d4b\u6027\uff08Observability\uff09",id:"\u53ef\u89c2\u6d4b\u6027observability",level:2},{value:"1. \u7ed3\u6784\u5316\u65e5\u5fd7",id:"1-\u7ed3\u6784\u5316\u65e5\u5fd7",level:3},{value:"2. \u6307\u6807\u6536\u96c6",id:"2-\u6307\u6807\u6536\u96c6",level:3},{value:"3. \u5206\u5e03\u5f0f\u8ffd\u8e2a",id:"3-\u5206\u5e03\u5f0f\u8ffd\u8e2a",level:3},{value:"\u53d1\u5e03\u4e0e\u56de\u6eda",id:"\u53d1\u5e03\u4e0e\u56de\u6eda",level:2},{value:"1. \u7070\u5ea6\u53d1\u5e03",id:"1-\u7070\u5ea6\u53d1\u5e03",level:3},{value:"2. Feature Flag",id:"2-feature-flag",level:3},{value:"\u6210\u672c\u63a7\u5236",id:"\u6210\u672c\u63a7\u5236",level:2},{value:"\u9884\u7b97\u7ba1\u7406",id:"\u9884\u7b97\u7ba1\u7406",level:3},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"production\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72",children:"Production\uff08\u751f\u4ea7\u5316\u4e0e\u90e8\u7f72\uff09"})}),"\n",(0,r.jsx)(n.p,{children:'\u628a Demo \u53d8\u6210\u53ef\u7528\u7684\u751f\u4ea7\u7cfb\u7edf\uff0c\u5173\u952e\u662f"\u7a33\u5b9a\u3001\u53ef\u89c2\u6d4b\u3001\u53ef\u63a7\u6210\u672c"\u3002\u672c\u9875\u4ece\u5ef6\u8fdf\u3001\u7a33\u5b9a\u6027\u3001\u7f13\u5b58\u3001\u89c2\u6d4b\u4e0e\u53d1\u5e03\u7b56\u7565\u603b\u7ed3\u5e38\u7528\u505a\u6cd5\u3002'}),"\n",(0,r.jsx)(n.h2,{id:"\u5178\u578b\u751f\u4ea7\u67b6\u6784",children:"\u5178\u578b\u751f\u4ea7\u67b6\u6784"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        API Gateway                               \u2502\n\u2502              (\u9274\u6743\u3001\u9650\u6d41\u3001\u914d\u989d\u3001\u8d1f\u8f7d\u5747\u8861)                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc               \u25bc               \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 LLM      \u2502    \u2502 RAG      \u2502    \u2502 Agent    \u2502\n       \u2502 Service  \u2502    \u2502 Service  \u2502    \u2502 Service  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502               \u2502               \u2502\n            \u25bc               \u25bc               \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502           Observability Layer            \u2502\n       \u2502     (\u65e5\u5fd7\u3001\u6307\u6807\u3001\u8ffd\u8e2a\u3001\u8bc4\u4f30\u56de\u653e)           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"\u5ef6\u8fdf\u4f18\u5316",children:"\u5ef6\u8fdf\u4f18\u5316"}),"\n",(0,r.jsx)(n.h3,{id:"1-\u6a21\u578b\u9009\u62e9\u7b56\u7565",children:"1. \u6a21\u578b\u9009\u62e9\u7b56\u7565"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ModelRouter:\n    """\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u9009\u62e9\u6a21\u578b"""\n    \n    def __init__(self):\n        self.models = {\n            "simple": "gpt-4o-mini",      # \u7b80\u5355\u4efb\u52a1\n            "complex": "gpt-4o",           # \u590d\u6742\u4efb\u52a1\n            "code": "gpt-4o",              # \u4ee3\u7801\u751f\u6210\n        }\n    \n    def classify_task(self, prompt: str) -> str:\n        """\u7b80\u5355\u7684\u4efb\u52a1\u5206\u7c7b"""\n        if len(prompt) < 100:\n            return "simple"\n        if any(kw in prompt.lower() for kw in ["\u4ee3\u7801", "code", "function", "class"]):\n            return "code"\n        return "complex"\n    \n    def get_model(self, prompt: str) -> str:\n        task_type = self.classify_task(prompt)\n        return self.models[task_type]\n\nrouter = ModelRouter()\nmodel = router.get_model(user_prompt)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-\u6d41\u5f0f\u8f93\u51fa",children:"2. \u6d41\u5f0f\u8f93\u51fa"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\ndef stream_response(prompt: str):\n    """\u6d41\u5f0f\u8f93\u51fa\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c"""\n    stream = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}],\n        stream=True\n    )\n    \n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            yield chunk.choices[0].delta.content\n\n# FastAPI \u6d41\u5f0f\u54cd\u5e94\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.post("/chat/stream")\nasync def chat_stream(prompt: str):\n    return StreamingResponse(\n        stream_response(prompt),\n        media_type="text/event-stream"\n    )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-\u5e76\u884c\u5316\u5904\u7406",children:"3. \u5e76\u884c\u5316\u5904\u7406"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def parallel_requests(prompts: list[str]) -> list[str]:\n    """\u5e76\u884c\u5904\u7406\u591a\u4e2a\u8bf7\u6c42"""\n    tasks = [\n        client.chat.completions.create(\n            model="gpt-4o-mini",\n            messages=[{"role": "user", "content": p}]\n        )\n        for p in prompts\n    ]\n    \n    responses = await asyncio.gather(*tasks)\n    return [r.choices[0].message.content for r in responses]\n\n# RAG \u573a\u666f\uff1a\u5e76\u884c\u68c0\u7d22\u548c\u751f\u6210\nasync def rag_parallel(query: str):\n    """\u5e76\u884c\u6267\u884c\u68c0\u7d22\u548c\u67e5\u8be2\u91cd\u5199"""\n    rewrite_task = rewrite_query(query)\n    retrieve_task = retrieve_documents(query)\n    \n    rewritten, docs = await asyncio.gather(rewrite_task, retrieve_task)\n    \n    # \u4f7f\u7528\u91cd\u5199\u540e\u7684\u67e5\u8be2\u518d\u6b21\u68c0\u7d22\n    more_docs = await retrieve_documents(rewritten)\n    \n    return generate_answer(query, docs + more_docs)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"4-\u4e0a\u4e0b\u6587\u538b\u7f29",children:"4. \u4e0a\u4e0b\u6587\u538b\u7f29"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def compress_context(messages: list[dict], max_tokens: int = 4000) -> list[dict]:\n    """\u538b\u7f29\u5bf9\u8bdd\u5386\u53f2"""\n    import tiktoken\n    enc = tiktoken.encoding_for_model("gpt-4o")\n    \n    # \u4fdd\u7559\u7cfb\u7edf\u6d88\u606f\n    system_msg = [m for m in messages if m["role"] == "system"]\n    other_msgs = [m for m in messages if m["role"] != "system"]\n    \n    # \u4ece\u6700\u65b0\u6d88\u606f\u5f00\u59cb\u4fdd\u7559\n    compressed = []\n    total_tokens = sum(len(enc.encode(m["content"])) for m in system_msg)\n    \n    for msg in reversed(other_msgs):\n        msg_tokens = len(enc.encode(msg["content"]))\n        if total_tokens + msg_tokens > max_tokens:\n            break\n        compressed.insert(0, msg)\n        total_tokens += msg_tokens\n    \n    return system_msg + compressed\n\ndef summarize_history(messages: list[dict]) -> str:\n    """\u5c06\u5386\u53f2\u5bf9\u8bdd\u603b\u7ed3\u4e3a\u6458\u8981"""\n    history_text = "\\n".join([f"{m[\'role\']}: {m[\'content\']}" for m in messages])\n    \n    response = client.chat.completions.create(\n        model="gpt-4o-mini",\n        messages=[{\n            "role": "user",\n            "content": f"\u8bf7\u7528 100 \u5b57\u4ee5\u5185\u603b\u7ed3\u4ee5\u4e0b\u5bf9\u8bdd\u7684\u8981\u70b9\uff1a\\n{history_text}"\n        }],\n        max_tokens=150\n    )\n    \n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519",children:"\u7a33\u5b9a\u6027\u4e0e\u5bb9\u9519"}),"\n",(0,r.jsx)(n.h3,{id:"1-\u91cd\u8bd5\u7b56\u7565",children:"1. \u91cd\u8bd5\u7b56\u7565"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom functools import wraps\nfrom openai import RateLimitError, APIError\n\ndef retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):\n    """\u6307\u6570\u9000\u907f\u91cd\u8bd5\u88c5\u9970\u5668"""\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except RateLimitError as e:\n                    if attempt == max_retries - 1:\n                        raise\n                    delay = base_delay * (2 ** attempt)\n                    print(f"Rate limited, retrying in {delay}s...")\n                    time.sleep(delay)\n                except APIError as e:\n                    if e.status_code >= 500:  # \u670d\u52a1\u7aef\u9519\u8bef\u624d\u91cd\u8bd5\n                        if attempt == max_retries - 1:\n                            raise\n                        time.sleep(base_delay)\n                    else:\n                        raise\n            return None\n        return wrapper\n    return decorator\n\n@retry_with_backoff(max_retries=3)\ndef call_llm(prompt: str) -> str:\n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}]\n    )\n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-\u8d85\u65f6\u4e0e\u53d6\u6d88",children:"2. \u8d85\u65f6\u4e0e\u53d6\u6d88"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def timeout_context(seconds: float):\n    """\u8d85\u65f6\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668"""\n    try:\n        yield asyncio.wait_for(asyncio.sleep(0), timeout=seconds)\n    except asyncio.TimeoutError:\n        raise TimeoutError(f"Operation timed out after {seconds}s")\n\nasync def call_with_timeout(prompt: str, timeout: float = 30.0) -> str:\n    """\u5e26\u8d85\u65f6\u7684 LLM \u8c03\u7528"""\n    try:\n        response = await asyncio.wait_for(\n            client.chat.completions.create(\n                model="gpt-4o",\n                messages=[{"role": "user", "content": prompt}]\n            ),\n            timeout=timeout\n        )\n        return response.choices[0].message.content\n    except asyncio.TimeoutError:\n        return "\u8bf7\u6c42\u8d85\u65f6\uff0c\u8bf7\u7a0d\u540e\u91cd\u8bd5"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-\u964d\u7ea7\u7b56\u7565",children:"3. \u964d\u7ea7\u7b56\u7565"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class LLMService:\n    """\u5e26\u964d\u7ea7\u7b56\u7565\u7684 LLM \u670d\u52a1"""\n    \n    def __init__(self):\n        self.primary_model = "gpt-4o"\n        self.fallback_model = "gpt-4o-mini"\n        self.error_count = 0\n        self.error_threshold = 5\n        self.circuit_open = False\n    \n    async def call(self, prompt: str) -> str:\n        # \u7194\u65ad\u5668\u6253\u5f00\u65f6\u76f4\u63a5\u4f7f\u7528\u964d\u7ea7\u6a21\u578b\n        if self.circuit_open:\n            return await self._call_fallback(prompt)\n        \n        try:\n            response = await self._call_primary(prompt)\n            self.error_count = 0  # \u6210\u529f\u5219\u91cd\u7f6e\u9519\u8bef\u8ba1\u6570\n            return response\n        except Exception as e:\n            self.error_count += 1\n            if self.error_count >= self.error_threshold:\n                self.circuit_open = True\n                print("\u26a0\ufe0f \u7194\u65ad\u5668\u6253\u5f00\uff0c\u5207\u6362\u5230\u964d\u7ea7\u6a21\u578b")\n            return await self._call_fallback(prompt)\n    \n    async def _call_primary(self, prompt: str) -> str:\n        response = await client.chat.completions.create(\n            model=self.primary_model,\n            messages=[{"role": "user", "content": prompt}]\n        )\n        return response.choices[0].message.content\n    \n    async def _call_fallback(self, prompt: str) -> str:\n        response = await client.chat.completions.create(\n            model=self.fallback_model,\n            messages=[{"role": "user", "content": prompt}]\n        )\n        return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u7f13\u5b58\u7b56\u7565",children:"\u7f13\u5b58\u7b56\u7565"}),"\n",(0,r.jsx)(n.h3,{id:"1-\u8bed\u4e49\u7f13\u5b58",children:"1. \u8bed\u4e49\u7f13\u5b58"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import hashlib\nfrom typing import Optional\nimport numpy as np\n\nclass SemanticCache:\n    """\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u7f13\u5b58"""\n    \n    def __init__(self, similarity_threshold: float = 0.95):\n        self.cache = {}  # {embedding_hash: (embedding, response)}\n        self.threshold = similarity_threshold\n    \n    def _get_embedding(self, text: str) -> np.ndarray:\n        response = client.embeddings.create(\n            model="text-embedding-3-small",\n            input=text\n        )\n        return np.array(response.data[0].embedding)\n    \n    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n    \n    def get(self, query: str) -> Optional[str]:\n        """\u67e5\u627e\u8bed\u4e49\u76f8\u4f3c\u7684\u7f13\u5b58"""\n        query_embedding = self._get_embedding(query)\n        \n        for key, (cached_embedding, response) in self.cache.items():\n            similarity = self._cosine_similarity(query_embedding, cached_embedding)\n            if similarity >= self.threshold:\n                return response\n        \n        return None\n    \n    def set(self, query: str, response: str):\n        """\u8bbe\u7f6e\u7f13\u5b58"""\n        embedding = self._get_embedding(query)\n        key = hashlib.md5(query.encode()).hexdigest()\n        self.cache[key] = (embedding, response)\n\n# \u4f7f\u7528\u793a\u4f8b\ncache = SemanticCache(similarity_threshold=0.95)\n\ndef cached_llm_call(prompt: str) -> str:\n    # \u5148\u67e5\u7f13\u5b58\n    cached = cache.get(prompt)\n    if cached:\n        return cached\n    \n    # \u7f13\u5b58\u672a\u547d\u4e2d\uff0c\u8c03\u7528 LLM\n    response = call_llm(prompt)\n    cache.set(prompt, response)\n    return response\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-redis-\u7f13\u5b58",children:"2. Redis \u7f13\u5b58"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import redis\nimport json\nimport hashlib\n\nclass RedisCache:\n    """Redis \u7f13\u5b58"""\n    \n    def __init__(self, host: str = "localhost", port: int = 6379, ttl: int = 3600):\n        self.client = redis.Redis(host=host, port=port, decode_responses=True)\n        self.ttl = ttl\n    \n    def _make_key(self, prompt: str, model: str) -> str:\n        content = f"{model}:{prompt}"\n        return f"llm_cache:{hashlib.sha256(content.encode()).hexdigest()}"\n    \n    def get(self, prompt: str, model: str) -> Optional[str]:\n        key = self._make_key(prompt, model)\n        return self.client.get(key)\n    \n    def set(self, prompt: str, model: str, response: str):\n        key = self._make_key(prompt, model)\n        self.client.setex(key, self.ttl, response)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u53ef\u89c2\u6d4b\u6027observability",children:"\u53ef\u89c2\u6d4b\u6027\uff08Observability\uff09"}),"\n",(0,r.jsx)(n.h3,{id:"1-\u7ed3\u6784\u5316\u65e5\u5fd7",children:"1. \u7ed3\u6784\u5316\u65e5\u5fd7"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nimport json\nfrom datetime import datetime\n\nclass LLMLogger:\n    """LLM \u8c03\u7528\u65e5\u5fd7\u8bb0\u5f55\u5668"""\n    \n    def __init__(self):\n        self.logger = logging.getLogger("llm")\n        handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        self.logger.addHandler(handler)\n        self.logger.setLevel(logging.INFO)\n    \n    def log_request(self, request_id: str, model: str, prompt: str, \n                    response: str, latency_ms: float, tokens: dict):\n        log_entry = {\n            "timestamp": datetime.utcnow().isoformat(),\n            "request_id": request_id,\n            "model": model,\n            "prompt_preview": prompt[:100] + "..." if len(prompt) > 100 else prompt,\n            "response_preview": response[:100] + "..." if len(response) > 100 else response,\n            "latency_ms": latency_ms,\n            "prompt_tokens": tokens.get("prompt_tokens", 0),\n            "completion_tokens": tokens.get("completion_tokens", 0),\n            "total_tokens": tokens.get("total_tokens", 0)\n        }\n        self.logger.info(json.dumps(log_entry, ensure_ascii=False))\n\nllm_logger = LLMLogger()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-\u6307\u6807\u6536\u96c6",children:"2. \u6307\u6807\u6536\u96c6"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from prometheus_client import Counter, Histogram, Gauge\n\n# \u5b9a\u4e49\u6307\u6807\nllm_requests_total = Counter(\n    'llm_requests_total',\n    'Total LLM requests',\n    ['model', 'status']\n)\n\nllm_latency_seconds = Histogram(\n    'llm_latency_seconds',\n    'LLM request latency',\n    ['model'],\n    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]\n)\n\nllm_tokens_total = Counter(\n    'llm_tokens_total',\n    'Total tokens used',\n    ['model', 'type']  # type: prompt/completion\n)\n\n# \u4f7f\u7528\u793a\u4f8b\ndef call_llm_with_metrics(prompt: str, model: str) -> str:\n    import time\n    start = time.time()\n    \n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        latency = time.time() - start\n        llm_requests_total.labels(model=model, status=\"success\").inc()\n        llm_latency_seconds.labels(model=model).observe(latency)\n        llm_tokens_total.labels(model=model, type=\"prompt\").inc(response.usage.prompt_tokens)\n        llm_tokens_total.labels(model=model, type=\"completion\").inc(response.usage.completion_tokens)\n        \n        return response.choices[0].message.content\n    except Exception as e:\n        llm_requests_total.labels(model=model, status=\"error\").inc()\n        raise\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-\u5206\u5e03\u5f0f\u8ffd\u8e2a",children:"3. \u5206\u5e03\u5f0f\u8ffd\u8e2a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\n# \u521d\u59cb\u5316\u8ffd\u8e2a\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\n# \u6dfb\u52a0\u5bfc\u51fa\u5668\notlp_exporter = OTLPSpanExporter(endpoint="http://localhost:4317")\ntrace.get_tracer_provider().add_span_processor(BatchSpanProcessor(otlp_exporter))\n\n# \u4f7f\u7528\u8ffd\u8e2a\n@tracer.start_as_current_span("llm_call")\ndef traced_llm_call(prompt: str) -> str:\n    span = trace.get_current_span()\n    span.set_attribute("llm.model", "gpt-4o")\n    span.set_attribute("llm.prompt_length", len(prompt))\n    \n    response = call_llm(prompt)\n    \n    span.set_attribute("llm.response_length", len(response))\n    return response\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u53d1\u5e03\u4e0e\u56de\u6eda",children:"\u53d1\u5e03\u4e0e\u56de\u6eda"}),"\n",(0,r.jsx)(n.h3,{id:"1-\u7070\u5ea6\u53d1\u5e03",children:"1. \u7070\u5ea6\u53d1\u5e03"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class GradualRollout:\n    """\u7070\u5ea6\u53d1\u5e03\u7ba1\u7406"""\n    \n    def __init__(self):\n        self.rollout_percentage = 0.0\n        self.new_config = None\n        self.old_config = None\n    \n    def start_rollout(self, new_config: dict, initial_percentage: float = 0.05):\n        """\u5f00\u59cb\u7070\u5ea6\u53d1\u5e03"""\n        self.old_config = self.get_current_config()\n        self.new_config = new_config\n        self.rollout_percentage = initial_percentage\n    \n    def increase_rollout(self, percentage: float):\n        """\u589e\u52a0\u7070\u5ea6\u6bd4\u4f8b"""\n        self.rollout_percentage = min(1.0, percentage)\n    \n    def get_config_for_request(self, user_id: str) -> dict:\n        """\u83b7\u53d6\u8bf7\u6c42\u5e94\u4f7f\u7528\u7684\u914d\u7f6e"""\n        import hashlib\n        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n        if (hash_value % 100) / 100 < self.rollout_percentage:\n            return self.new_config\n        return self.old_config\n    \n    def rollback(self):\n        """\u56de\u6eda\u5230\u65e7\u914d\u7f6e"""\n        self.rollout_percentage = 0.0\n        self.new_config = None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-feature-flag",children:"2. Feature Flag"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class FeatureFlags:\n    """\u529f\u80fd\u5f00\u5173\u7ba1\u7406"""\n    \n    def __init__(self):\n        self.flags = {\n            "use_new_prompt": False,\n            "enable_rag": True,\n            "use_gpt4": False,\n            "enable_caching": True\n        }\n    \n    def is_enabled(self, flag_name: str, user_id: str = None) -> bool:\n        """\u68c0\u67e5\u529f\u80fd\u662f\u5426\u542f\u7528"""\n        if flag_name not in self.flags:\n            return False\n        \n        flag_value = self.flags[flag_name]\n        \n        # \u652f\u6301\u767e\u5206\u6bd4\u7070\u5ea6\n        if isinstance(flag_value, float):\n            if user_id:\n                import hashlib\n                hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n                return (hash_value % 100) / 100 < flag_value\n            return False\n        \n        return flag_value\n    \n    def set_flag(self, flag_name: str, value):\n        """\u8bbe\u7f6e\u529f\u80fd\u5f00\u5173"""\n        self.flags[flag_name] = value\n\n# \u4f7f\u7528\u793a\u4f8b\nflags = FeatureFlags()\n\ndef process_request(prompt: str, user_id: str) -> str:\n    if flags.is_enabled("use_new_prompt", user_id):\n        prompt = apply_new_prompt_template(prompt)\n    \n    if flags.is_enabled("enable_rag", user_id):\n        context = retrieve_context(prompt)\n        prompt = f"Context: {context}\\n\\nQuestion: {prompt}"\n    \n    model = "gpt-4o" if flags.is_enabled("use_gpt4", user_id) else "gpt-4o-mini"\n    \n    return call_llm(prompt, model)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u6210\u672c\u63a7\u5236",children:"\u6210\u672c\u63a7\u5236"}),"\n",(0,r.jsx)(n.h3,{id:"\u9884\u7b97\u7ba1\u7406",children:"\u9884\u7b97\u7ba1\u7406"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from datetime import datetime, timedelta\nfrom collections import defaultdict\n\nclass BudgetManager:\n    """\u9884\u7b97\u7ba1\u7406\u5668"""\n    \n    # \u6a21\u578b\u4ef7\u683c (USD per 1M tokens)\n    PRICES = {\n        "gpt-4o": {"input": 2.5, "output": 10.0},\n        "gpt-4o-mini": {"input": 0.15, "output": 0.6},\n    }\n    \n    def __init__(self, daily_budget_usd: float = 100.0):\n        self.daily_budget = daily_budget_usd\n        self.usage = defaultdict(float)  # {date: cost}\n    \n    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:\n        """\u8ba1\u7b97\u5355\u6b21\u8c03\u7528\u6210\u672c"""\n        prices = self.PRICES.get(model, {"input": 0, "output": 0})\n        input_cost = (input_tokens / 1_000_000) * prices["input"]\n        output_cost = (output_tokens / 1_000_000) * prices["output"]\n        return input_cost + output_cost\n    \n    def record_usage(self, model: str, input_tokens: int, output_tokens: int):\n        """\u8bb0\u5f55\u4f7f\u7528\u91cf"""\n        cost = self.calculate_cost(model, input_tokens, output_tokens)\n        today = datetime.now().strftime("%Y-%m-%d")\n        self.usage[today] += cost\n    \n    def can_proceed(self) -> bool:\n        """\u68c0\u67e5\u662f\u5426\u8d85\u51fa\u9884\u7b97"""\n        today = datetime.now().strftime("%Y-%m-%d")\n        return self.usage[today] < self.daily_budget\n    \n    def get_remaining_budget(self) -> float:\n        """\u83b7\u53d6\u5269\u4f59\u9884\u7b97"""\n        today = datetime.now().strftime("%Y-%m-%d")\n        return max(0, self.daily_budget - self.usage[today])\n\nbudget = BudgetManager(daily_budget_usd=50.0)\n\ndef call_with_budget_check(prompt: str) -> str:\n    if not budget.can_proceed():\n        raise Exception("Daily budget exceeded")\n    \n    response = client.chat.completions.create(\n        model="gpt-4o",\n        messages=[{"role": "user", "content": prompt}]\n    )\n    \n    budget.record_usage(\n        "gpt-4o",\n        response.usage.prompt_tokens,\n        response.usage.completion_tokens\n    )\n    \n    return response.choices[0].message.content\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/latency-optimization",children:"OpenAI \u5ef6\u8fdf\u4f18\u5316\u6307\u5357"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://python.langchain.com/docs/guides/productionization/",children:"LangChain \u751f\u4ea7\u5316\u6307\u5357"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://opentelemetry.io/docs/",children:"OpenTelemetry \u6587\u6863"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);