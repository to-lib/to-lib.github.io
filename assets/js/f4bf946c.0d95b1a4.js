"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[47227],{48885:(n,e,t)=>{t.d(e,{R:()=>d,x:()=>a});var i=t(99378);const l={},r=i.createContext(l);function d(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:d(n.components),i.createElement(r.Provider,{value:e},n.children)}},78350:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>s,contentTitle:()=>a,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"ai/quantization","title":"\ud83d\udddc\ufe0f \u6a21\u578b\u91cf\u5316","description":"\u6a21\u578b\u91cf\u5316\u662f\u5c06\u6a21\u578b\u6743\u91cd\u4ece\u9ad8\u7cbe\u5ea6\uff08FP32/FP16\uff09\u8f6c\u6362\u4e3a\u4f4e\u7cbe\u5ea6\uff08INT8/INT4\uff09\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u5185\u5b58\u5360\u7528\uff0c\u52a0\u901f\u63a8\u7406\u3002","source":"@site/docs/ai/quantization.md","sourceDirName":"ai","slug":"/ai/quantization","permalink":"/docs/ai/quantization","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ai/quantization.md","tags":[],"version":"current","sidebarPosition":31,"frontMatter":{"sidebar_position":31,"title":"\ud83d\udddc\ufe0f \u6a21\u578b\u91cf\u5316"},"sidebar":"ai","previous":{"title":"\ud83c\udf99\ufe0f \u8bed\u97f3\u4ea4\u4e92","permalink":"/docs/ai/voice"},"next":{"title":"\ud83e\uddec \u6a21\u578b\u84b8\u998f","permalink":"/docs/ai/distillation"}}');var l=t(22714),r=t(48885);const d={sidebar_position:31,title:"\ud83d\udddc\ufe0f \u6a21\u578b\u91cf\u5316"},a="\u6a21\u578b\u91cf\u5316",s={},o=[{value:"\u4e3a\u4ec0\u4e48\u9700\u8981\u91cf\u5316\uff1f",id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u91cf\u5316",level:2},{value:"\u91cf\u5316\u65b9\u6cd5\u5bf9\u6bd4",id:"\u91cf\u5316\u65b9\u6cd5\u5bf9\u6bd4",level:2},{value:"GGUF \u683c\u5f0f",id:"gguf-\u683c\u5f0f",level:2},{value:"\u91cf\u5316\u7ea7\u522b",id:"\u91cf\u5316\u7ea7\u522b",level:3},{value:"\u4f7f\u7528 llama.cpp \u91cf\u5316",id:"\u4f7f\u7528-llamacpp-\u91cf\u5316",level:3},{value:"\u4f7f\u7528 Ollama \u8fd0\u884c GGUF",id:"\u4f7f\u7528-ollama-\u8fd0\u884c-gguf",level:3},{value:"Python \u4f7f\u7528 llama-cpp-python",id:"python-\u4f7f\u7528-llama-cpp-python",level:3},{value:"GPTQ \u91cf\u5316",id:"gptq-\u91cf\u5316",level:2},{value:"\u4f7f\u7528 AutoGPTQ",id:"\u4f7f\u7528-autogptq",level:3},{value:"\u52a0\u8f7d GPTQ \u6a21\u578b",id:"\u52a0\u8f7d-gptq-\u6a21\u578b",level:3},{value:"AWQ \u91cf\u5316",id:"awq-\u91cf\u5316",level:2},{value:"\u4f7f\u7528 AutoAWQ",id:"\u4f7f\u7528-autoawq",level:3},{value:"vLLM \u4f7f\u7528 AWQ \u6a21\u578b",id:"vllm-\u4f7f\u7528-awq-\u6a21\u578b",level:3},{value:"bitsandbytes \u91cf\u5316",id:"bitsandbytes-\u91cf\u5316",level:2},{value:"8-bit \u91cf\u5316",id:"8-bit-\u91cf\u5316",level:3},{value:"4-bit \u91cf\u5316 (QLoRA)",id:"4-bit-\u91cf\u5316-qlora",level:3},{value:"HuggingFace \u91cf\u5316\u6a21\u578b",id:"huggingface-\u91cf\u5316\u6a21\u578b",level:2},{value:"\u5e38\u7528\u91cf\u5316\u6a21\u578b\u6765\u6e90",id:"\u5e38\u7528\u91cf\u5316\u6a21\u578b\u6765\u6e90",level:3},{value:"\u91cf\u5316\u8d28\u91cf\u8bc4\u4f30",id:"\u91cf\u5316\u8d28\u91cf\u8bc4\u4f30",level:2},{value:"\u91cf\u5316\u9009\u62e9\u6307\u5357",id:"\u91cf\u5316\u9009\u62e9\u6307\u5357",level:2},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u5ef6\u4f38\u9605\u8bfb",id:"\u5ef6\u4f38\u9605\u8bfb",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"\u6a21\u578b\u91cf\u5316",children:"\u6a21\u578b\u91cf\u5316"})}),"\n",(0,l.jsx)(e.p,{children:"\u6a21\u578b\u91cf\u5316\u662f\u5c06\u6a21\u578b\u6743\u91cd\u4ece\u9ad8\u7cbe\u5ea6\uff08FP32/FP16\uff09\u8f6c\u6362\u4e3a\u4f4e\u7cbe\u5ea6\uff08INT8/INT4\uff09\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u5185\u5b58\u5360\u7528\uff0c\u52a0\u901f\u63a8\u7406\u3002"}),"\n",(0,l.jsx)(e.h2,{id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u91cf\u5316",children:"\u4e3a\u4ec0\u4e48\u9700\u8981\u91cf\u5316\uff1f"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"\u7cbe\u5ea6"}),(0,l.jsx)(e.th,{children:"\u6bcf\u53c2\u6570\u5927\u5c0f"}),(0,l.jsx)(e.th,{children:"7B \u6a21\u578b\u5927\u5c0f"}),(0,l.jsx)(e.th,{children:"\u663e\u5b58\u9700\u6c42"})]})}),(0,l.jsxs)(e.tbody,{children:[(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"FP32"}),(0,l.jsx)(e.td,{children:"4 bytes"}),(0,l.jsx)(e.td,{children:"28 GB"}),(0,l.jsx)(e.td,{children:"~32 GB"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"FP16"}),(0,l.jsx)(e.td,{children:"2 bytes"}),(0,l.jsx)(e.td,{children:"14 GB"}),(0,l.jsx)(e.td,{children:"~16 GB"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"INT8"}),(0,l.jsx)(e.td,{children:"1 byte"}),(0,l.jsx)(e.td,{children:"7 GB"}),(0,l.jsx)(e.td,{children:"~8 GB"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"INT4"}),(0,l.jsx)(e.td,{children:"0.5 byte"}),(0,l.jsx)(e.td,{children:"3.5 GB"}),(0,l.jsx)(e.td,{children:"~4 GB"})]})]})]}),"\n",(0,l.jsx)(e.h2,{id:"\u91cf\u5316\u65b9\u6cd5\u5bf9\u6bd4",children:"\u91cf\u5316\u65b9\u6cd5\u5bf9\u6bd4"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"\u65b9\u6cd5"}),(0,l.jsx)(e.th,{children:"\u7cbe\u5ea6\u635f\u5931"}),(0,l.jsx)(e.th,{children:"\u901f\u5ea6"}),(0,l.jsx)(e.th,{children:"\u9002\u7528\u573a\u666f"})]})}),(0,l.jsxs)(e.tbody,{children:[(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"GGUF"}),(0,l.jsx)(e.td,{children:"\u4f4e"}),(0,l.jsx)(e.td,{children:"\u5feb"}),(0,l.jsx)(e.td,{children:"CPU/\u6df7\u5408\u63a8\u7406"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"GPTQ"}),(0,l.jsx)(e.td,{children:"\u4f4e"}),(0,l.jsx)(e.td,{children:"\u5feb"}),(0,l.jsx)(e.td,{children:"GPU \u63a8\u7406"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"AWQ"}),(0,l.jsx)(e.td,{children:"\u5f88\u4f4e"}),(0,l.jsx)(e.td,{children:"\u5feb"}),(0,l.jsx)(e.td,{children:"GPU \u63a8\u7406"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"bitsandbytes"}),(0,l.jsx)(e.td,{children:"\u4e2d"}),(0,l.jsx)(e.td,{children:"\u4e2d"}),(0,l.jsx)(e.td,{children:"\u8bad\u7ec3/\u5fae\u8c03"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"EETQ"}),(0,l.jsx)(e.td,{children:"\u4f4e"}),(0,l.jsx)(e.td,{children:"\u5f88\u5feb"}),(0,l.jsx)(e.td,{children:"GPU \u63a8\u7406"})]})]})]}),"\n",(0,l.jsx)(e.h2,{id:"gguf-\u683c\u5f0f",children:"GGUF \u683c\u5f0f"}),"\n",(0,l.jsx)(e.p,{children:"GGUF \u662f llama.cpp \u4f7f\u7528\u7684\u91cf\u5316\u683c\u5f0f\uff0c\u652f\u6301 CPU \u548c GPU \u6df7\u5408\u63a8\u7406\u3002"}),"\n",(0,l.jsx)(e.h3,{id:"\u91cf\u5316\u7ea7\u522b",children:"\u91cf\u5316\u7ea7\u522b"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"\u91cf\u5316\u7c7b\u578b"}),(0,l.jsx)(e.th,{children:"\u5927\u5c0f\u6bd4\u4f8b"}),(0,l.jsx)(e.th,{children:"\u8d28\u91cf"}),(0,l.jsx)(e.th,{children:"\u8bf4\u660e"})]})}),(0,l.jsxs)(e.tbody,{children:[(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Q2_K"}),(0,l.jsx)(e.td,{children:"~29%"}),(0,l.jsx)(e.td,{children:"\u8f83\u5dee"}),(0,l.jsx)(e.td,{children:"\u6781\u9650\u538b\u7f29"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Q3_K_M"}),(0,l.jsx)(e.td,{children:"~37%"}),(0,l.jsx)(e.td,{children:"\u53ef\u7528"}),(0,l.jsx)(e.td,{children:"\u4f4e\u7aef\u8bbe\u5907"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Q4_K_M"}),(0,l.jsx)(e.td,{children:"~45%"}),(0,l.jsx)(e.td,{children:"\u826f\u597d"}),(0,l.jsx)(e.td,{children:"\u63a8\u8350\u5e73\u8861"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Q5_K_M"}),(0,l.jsx)(e.td,{children:"~52%"}),(0,l.jsx)(e.td,{children:"\u5f88\u597d"}),(0,l.jsx)(e.td,{children:"\u8d28\u91cf\u4f18\u5148"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Q6_K"}),(0,l.jsx)(e.td,{children:"~59%"}),(0,l.jsx)(e.td,{children:"\u4f18\u79c0"}),(0,l.jsx)(e.td,{children:"\u63a5\u8fd1\u539f\u59cb"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Q8_0"}),(0,l.jsx)(e.td,{children:"~75%"}),(0,l.jsx)(e.td,{children:"\u6781\u4f73"}),(0,l.jsx)(e.td,{children:"\u51e0\u4e4e\u65e0\u635f"})]})]})]}),"\n",(0,l.jsx)(e.h3,{id:"\u4f7f\u7528-llamacpp-\u91cf\u5316",children:"\u4f7f\u7528 llama.cpp \u91cf\u5316"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"# \u514b\u9686 llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\n# \u7f16\u8bd1\nmake -j\n\n# \u8f6c\u6362 HuggingFace \u6a21\u578b\u4e3a GGUF\npython convert_hf_to_gguf.py /path/to/model --outfile model.gguf\n\n# \u91cf\u5316\n./llama-quantize model.gguf model-q4_k_m.gguf Q4_K_M\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\u4f7f\u7528-ollama-\u8fd0\u884c-gguf",children:"\u4f7f\u7528 Ollama \u8fd0\u884c GGUF"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:'# \u521b\u5efa Modelfile\ncat > Modelfile << EOF\nFROM ./model-q4_k_m.gguf\nPARAMETER temperature 0.7\nPARAMETER num_ctx 4096\nSYSTEM "\u4f60\u662f\u4e00\u4e2a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002"\nEOF\n\n# \u521b\u5efa\u6a21\u578b\nollama create my-model -f Modelfile\n\n# \u8fd0\u884c\nollama run my-model\n'})}),"\n",(0,l.jsx)(e.h3,{id:"python-\u4f7f\u7528-llama-cpp-python",children:"Python \u4f7f\u7528 llama-cpp-python"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from llama_cpp import Llama\n\n# \u52a0\u8f7d\u91cf\u5316\u6a21\u578b\nllm = Llama(\n    model_path="./model-q4_k_m.gguf",\n    n_ctx=4096,        # \u4e0a\u4e0b\u6587\u957f\u5ea6\n    n_gpu_layers=35,   # GPU \u5c42\u6570\uff0c-1 \u8868\u793a\u5168\u90e8\n    n_threads=8        # CPU \u7ebf\u7a0b\u6570\n)\n\n# \u63a8\u7406\noutput = llm(\n    "\u7528\u6237\uff1a\u4f60\u597d\\n\u52a9\u624b\uff1a",\n    max_tokens=256,\n    temperature=0.7,\n    stop=["\u7528\u6237\uff1a"]\n)\n\nprint(output["choices"][0]["text"])\n'})}),"\n",(0,l.jsx)(e.h2,{id:"gptq-\u91cf\u5316",children:"GPTQ \u91cf\u5316"}),"\n",(0,l.jsx)(e.p,{children:"GPTQ \u662f\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u6570\u636e\u7684 Post-Training Quantization \u65b9\u6cd5\u3002"}),"\n",(0,l.jsx)(e.h3,{id:"\u4f7f\u7528-autogptq",children:"\u4f7f\u7528 AutoGPTQ"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"pip install auto-gptq\n"})}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_path = "meta-llama/Llama-2-7b-hf"\nquantized_path = "./llama2-7b-gptq"\n\n# \u52a0\u8f7d tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# \u51c6\u5907\u6821\u51c6\u6570\u636e\ncalibration_data = [\n    tokenizer("\u8fd9\u662f\u4e00\u6bb5\u7528\u4e8e\u6821\u51c6\u7684\u6587\u672c\u3002" * 10, return_tensors="pt"),\n    # \u6dfb\u52a0\u66f4\u591a\u6837\u672c...\n]\n\n# \u91cf\u5316\u914d\u7f6e\nquantize_config = BaseQuantizeConfig(\n    bits=4,              # \u91cf\u5316\u4f4d\u6570\n    group_size=128,      # \u5206\u7ec4\u5927\u5c0f\n    desc_act=True        # \u6fc0\u6d3b\u503c\u6392\u5e8f\n)\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_path,\n    quantize_config=quantize_config\n)\n\n# \u6267\u884c\u91cf\u5316\nmodel.quantize(calibration_data)\n\n# \u4fdd\u5b58\nmodel.save_quantized(quantized_path)\ntokenizer.save_pretrained(quantized_path)\n'})}),"\n",(0,l.jsx)(e.h3,{id:"\u52a0\u8f7d-gptq-\u6a21\u578b",children:"\u52a0\u8f7d GPTQ \u6a21\u578b"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\n\nmodel = AutoGPTQForCausalLM.from_quantized(\n    "./llama2-7b-gptq",\n    device="cuda:0",\n    use_safetensors=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained("./llama2-7b-gptq")\n\n# \u63a8\u7406\ninputs = tokenizer("\u4f60\u597d\uff0c", return_tensors="pt").to("cuda")\noutputs = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0]))\n'})}),"\n",(0,l.jsx)(e.h2,{id:"awq-\u91cf\u5316",children:"AWQ \u91cf\u5316"}),"\n",(0,l.jsx)(e.p,{children:"AWQ (Activation-aware Weight Quantization) \u901a\u8fc7\u4fdd\u62a4\u91cd\u8981\u6743\u91cd\u6765\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002"}),"\n",(0,l.jsx)(e.h3,{id:"\u4f7f\u7528-autoawq",children:"\u4f7f\u7528 AutoAWQ"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-bash",children:"pip install autoawq\n"})}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = "meta-llama/Llama-2-7b-hf"\nquant_path = "./llama2-7b-awq"\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# \u91cf\u5316\u914d\u7f6e\nquant_config = {\n    "zero_point": True,\n    "q_group_size": 128,\n    "w_bit": 4,\n    "version": "GEMM"\n}\n\n# \u6267\u884c\u91cf\u5316\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# \u4fdd\u5b58\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n'})}),"\n",(0,l.jsx)(e.h3,{id:"vllm-\u4f7f\u7528-awq-\u6a21\u578b",children:"vLLM \u4f7f\u7528 AWQ \u6a21\u578b"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\n# \u52a0\u8f7d AWQ \u6a21\u578b\nllm = LLM(\n    model="./llama2-7b-awq",\n    quantization="awq",\n    dtype="half"\n)\n\nsampling_params = SamplingParams(\n    temperature=0.7,\n    max_tokens=256\n)\n\noutputs = llm.generate(["\u4f60\u597d\uff0c\u8bf7\u4ecb\u7ecd\u4e00\u4e0b\u81ea\u5df1\u3002"], sampling_params)\nprint(outputs[0].outputs[0].text)\n'})}),"\n",(0,l.jsx)(e.h2,{id:"bitsandbytes-\u91cf\u5316",children:"bitsandbytes \u91cf\u5316"}),"\n",(0,l.jsx)(e.p,{children:"bitsandbytes \u652f\u6301\u8bad\u7ec3\u65f6\u91cf\u5316\uff0c\u5e38\u7528\u4e8e QLoRA \u5fae\u8c03\u3002"}),"\n",(0,l.jsx)(e.h3,{id:"8-bit-\u91cf\u5316",children:"8-bit \u91cf\u5316"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# 8-bit \u914d\u7f6e\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "meta-llama/Llama-2-7b-hf",\n    quantization_config=bnb_config,\n    device_map="auto"\n)\n'})}),"\n",(0,l.jsx)(e.h3,{id:"4-bit-\u91cf\u5316-qlora",children:"4-bit \u91cf\u5316 (QLoRA)"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from transformers import BitsAndBytesConfig\nimport torch\n\n# 4-bit \u914d\u7f6e\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type="nf4",           # nf4 \u6216 fp4\n    bnb_4bit_compute_dtype=torch.float16, # \u8ba1\u7b97\u7cbe\u5ea6\n    bnb_4bit_use_double_quant=True        # \u53cc\u91cd\u91cf\u5316\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "meta-llama/Llama-2-7b-hf",\n    quantization_config=bnb_config,\n    device_map="auto"\n)\n\n# \u914d\u5408 LoRA \u5fae\u8c03\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=["q_proj", "v_proj"],\n    lora_dropout=0.05,\n    bias="none",\n    task_type="CAUSAL_LM"\n)\n\nmodel = get_peft_model(model, lora_config)\n'})}),"\n",(0,l.jsx)(e.h2,{id:"huggingface-\u91cf\u5316\u6a21\u578b",children:"HuggingFace \u91cf\u5316\u6a21\u578b"}),"\n",(0,l.jsx)(e.p,{children:"\u76f4\u63a5\u4f7f\u7528\u793e\u533a\u91cf\u5316\u597d\u7684\u6a21\u578b\u3002"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# \u4f7f\u7528 TheBloke \u7684\u91cf\u5316\u6a21\u578b\nmodel_id = "TheBloke/Llama-2-7B-GPTQ"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map="auto",\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n'})}),"\n",(0,l.jsx)(e.h3,{id:"\u5e38\u7528\u91cf\u5316\u6a21\u578b\u6765\u6e90",children:"\u5e38\u7528\u91cf\u5316\u6a21\u578b\u6765\u6e90"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"\u6765\u6e90"}),(0,l.jsx)(e.th,{children:"\u683c\u5f0f"}),(0,l.jsx)(e.th,{children:"\u8bf4\u660e"})]})}),(0,l.jsxs)(e.tbody,{children:[(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"TheBloke"}),(0,l.jsx)(e.td,{children:"GPTQ/AWQ/GGUF"}),(0,l.jsx)(e.td,{children:"\u6700\u5168\u9762\u7684\u91cf\u5316\u6a21\u578b"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Qwen"}),(0,l.jsx)(e.td,{children:"GPTQ/AWQ"}),(0,l.jsx)(e.td,{children:"\u5b98\u65b9\u91cf\u5316\u7248\u672c"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"unsloth"}),(0,l.jsx)(e.td,{children:"4-bit"}),(0,l.jsx)(e.td,{children:"\u4f18\u5316\u7684\u8bad\u7ec3\u6a21\u578b"})]})]})]}),"\n",(0,l.jsx)(e.h2,{id:"\u91cf\u5316\u8d28\u91cf\u8bc4\u4f30",children:"\u91cf\u5316\u8d28\u91cf\u8bc4\u4f30"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'from datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndef evaluate_perplexity(model, tokenizer, dataset_name="wikitext", split="test"):\n    """\u8bc4\u4f30\u56f0\u60d1\u5ea6"""\n    dataset = load_dataset(dataset_name, "wikitext-2-raw-v1", split=split)\n    \n    encodings = tokenizer("\\n\\n".join(dataset["text"]), return_tensors="pt")\n    max_length = 2048\n    stride = 512\n    \n    nlls = []\n    for i in range(0, encodings.input_ids.size(1), stride):\n        begin_loc = max(i + stride - max_length, 0)\n        end_loc = min(i + stride, encodings.input_ids.size(1))\n        \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-stride] = -100\n        \n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            nlls.append(outputs.loss)\n    \n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()\n\n# \u6bd4\u8f83\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\noriginal_ppl = evaluate_perplexity(original_model, tokenizer)\nquantized_ppl = evaluate_perplexity(quantized_model, tokenizer)\n\nprint(f"\u539f\u59cb\u6a21\u578b PPL: {original_ppl:.2f}")\nprint(f"\u91cf\u5316\u6a21\u578b PPL: {quantized_ppl:.2f}")\nprint(f"\u7cbe\u5ea6\u635f\u5931: {(quantized_ppl - original_ppl) / original_ppl * 100:.2f}%")\n'})}),"\n",(0,l.jsx)(e.h2,{id:"\u91cf\u5316\u9009\u62e9\u6307\u5357",children:"\u91cf\u5316\u9009\u62e9\u6307\u5357"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    \u91cf\u5316\u65b9\u6cd5\u9009\u62e9                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  \u573a\u666f\uff1aCPU \u63a8\u7406 / \u4f4e\u663e\u5b58                                \u2502\n\u2502  \u2514\u2500> GGUF (Q4_K_M \u6216 Q5_K_M)                           \u2502\n\u2502                                                         \u2502\n\u2502  \u573a\u666f\uff1aGPU \u63a8\u7406\uff0c\u8ffd\u6c42\u901f\u5ea6                               \u2502\n\u2502  \u2514\u2500> AWQ \u6216 GPTQ                                       \u2502\n\u2502                                                         \u2502\n\u2502  \u573a\u666f\uff1a\u5fae\u8c03\u8bad\u7ec3                                         \u2502\n\u2502  \u2514\u2500> bitsandbytes (4-bit QLoRA)                        \u2502\n\u2502                                                         \u2502\n\u2502  \u573a\u666f\uff1a\u8d28\u91cf\u4f18\u5148                                         \u2502\n\u2502  \u2514\u2500> AWQ > GPTQ > GGUF Q6_K                            \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,l.jsx)(e.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"\u9009\u62e9\u5408\u9002\u7684\u91cf\u5316\u7ea7\u522b"}),"\uff1aQ4 \u5e73\u8861\u5927\u5c0f\u548c\u8d28\u91cf\uff0cQ5/Q6 \u8d28\u91cf\u4f18\u5148"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"\u4f7f\u7528\u6821\u51c6\u6570\u636e"}),"\uff1aGPTQ/AWQ \u9700\u8981\u4ee3\u8868\u6027\u7684\u6821\u51c6\u6570\u636e"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"\u8bc4\u4f30\u8d28\u91cf"}),"\uff1a\u91cf\u5316\u540e\u6d4b\u8bd5\u56f0\u60d1\u5ea6\u548c\u4efb\u52a1\u6027\u80fd"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"\u8003\u8651\u786c\u4ef6"}),"\uff1aCPU \u7528 GGUF\uff0cGPU \u7528 AWQ/GPTQ"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"\u4f7f\u7528\u793e\u533a\u6a21\u578b"}),"\uff1a\u4f18\u5148\u4f7f\u7528 TheBloke \u7b49\u5df2\u91cf\u5316\u7684\u6a21\u578b"]}),"\n"]}),"\n",(0,l.jsx)(e.h2,{id:"\u5ef6\u4f38\u9605\u8bfb",children:"\u5ef6\u4f38\u9605\u8bfb"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"})}),"\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.a,{href:"https://github.com/AutoGPTQ/AutoGPTQ",children:"AutoGPTQ"})}),"\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.a,{href:"https://github.com/casper-hansen/AutoAWQ",children:"AutoAWQ"})}),"\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.a,{href:"https://github.com/TimDettmers/bitsandbytes",children:"bitsandbytes"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,l.jsx)(e,{...n,children:(0,l.jsx)(c,{...n})}):c(n)}}}]);