"use strict";(globalThis.webpackChunkto_lib_github_io=globalThis.webpackChunkto_lib_github_io||[]).push([[42213],{39237:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"ml/advanced-rl","title":"\ud83c\udfae \u5f3a\u5316\u5b66\u4e60\u8fdb\u9636","description":"\u6df1\u5165\u4ecb\u7ecd\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u7b97\u6cd5\u3002","source":"@site/docs/ml/advanced-rl.md","sourceDirName":"ml","slug":"/ml/advanced-rl","permalink":"/docs/ml/advanced-rl","draft":false,"unlisted":false,"editUrl":"https://github.com/to-lib/to-lib.github.io/tree/main/docs/ml/advanced-rl.md","tags":[],"version":"current","sidebarPosition":39,"frontMatter":{"sidebar_position":39,"title":"\ud83c\udfae \u5f3a\u5316\u5b66\u4e60\u8fdb\u9636"},"sidebar":"ml","previous":{"title":"\ud83d\udd0d \u5f02\u5e38\u68c0\u6d4b\u8be6\u89e3","permalink":"/docs/ml/anomaly-detection"},"next":{"title":"\ud83d\udd27 \u7279\u5f81\u5de5\u7a0b\u8fdb\u9636","permalink":"/docs/ml/advanced-feature-engineering"}}');var a=n(22714),r=n(48885);const i={sidebar_position:39,title:"\ud83c\udfae \u5f3a\u5316\u5b66\u4e60\u8fdb\u9636"},c="\u5f3a\u5316\u5b66\u4e60\u8fdb\u9636",l={},o=[{value:"PPO (Proximal Policy Optimization)",id:"ppo-proximal-policy-optimization",level:2},{value:"SAC (Soft Actor-Critic)",id:"sac-soft-actor-critic",level:2},{value:"TD3 (Twin Delayed DDPG)",id:"td3-twin-delayed-ddpg",level:2},{value:"\u7b97\u6cd5\u5bf9\u6bd4",id:"\u7b97\u6cd5\u5bf9\u6bd4",level:2},{value:"\u5e38\u7528\u5e93",id:"\u5e38\u7528\u5e93",level:2}];function d(t){const e={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...t.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"\u5f3a\u5316\u5b66\u4e60\u8fdb\u9636",children:"\u5f3a\u5316\u5b66\u4e60\u8fdb\u9636"})}),"\n",(0,a.jsx)(e.p,{children:"\u6df1\u5165\u4ecb\u7ecd\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u7b97\u6cd5\u3002"}),"\n",(0,a.jsx)(e.h2,{id:"ppo-proximal-policy-optimization",children:"PPO (Proximal Policy Optimization)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass PPO:\n    def __init__(self, actor, critic, lr=3e-4, clip_ratio=0.2):\n        self.actor = actor\n        self.critic = critic\n        self.clip_ratio = clip_ratio\n        self.optimizer = torch.optim.Adam(\n            list(actor.parameters()) + list(critic.parameters()), lr=lr\n        )\n\n    def compute_advantages(self, rewards, values, gamma=0.99, lam=0.95):\n        advantages = []\n        gae = 0\n        for t in reversed(range(len(rewards))):\n            delta = rewards[t] + gamma * values[t + 1] - values[t]\n            gae = delta + gamma * lam * gae\n            advantages.insert(0, gae)\n        return torch.tensor(advantages)\n\n    def update(self, states, actions, old_log_probs, rewards, values):\n        advantages = self.compute_advantages(rewards, values)\n        returns = advantages + values[:-1]\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        for _ in range(10):  # PPO epochs\n            new_log_probs = self.actor.log_prob(states, actions)\n            ratio = torch.exp(new_log_probs - old_log_probs)\n\n            # Clipped objective\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n            actor_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            new_values = self.critic(states)\n            critic_loss = nn.functional.mse_loss(new_values, returns)\n\n            loss = actor_loss + 0.5 * critic_loss\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"sac-soft-actor-critic",children:"SAC (Soft Actor-Critic)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class SAC:\n    def __init__(self, actor, critic1, critic2, alpha=0.2):\n        self.actor = actor\n        self.critic1 = critic1\n        self.critic2 = critic2\n        self.alpha = alpha  # \u71b5\u7cfb\u6570\n\n    def update_critic(self, states, actions, rewards, next_states, dones):\n        with torch.no_grad():\n            next_actions, next_log_probs = self.actor.sample(next_states)\n            q1_next = self.target_critic1(next_states, next_actions)\n            q2_next = self.target_critic2(next_states, next_actions)\n            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n            target_q = rewards + (1 - dones) * self.gamma * q_next\n\n        q1 = self.critic1(states, actions)\n        q2 = self.critic2(states, actions)\n        critic_loss = nn.functional.mse_loss(q1, target_q) + nn.functional.mse_loss(q2, target_q)\n\n        return critic_loss\n\n    def update_actor(self, states):\n        actions, log_probs = self.actor.sample(states)\n        q1 = self.critic1(states, actions)\n        q2 = self.critic2(states, actions)\n        q = torch.min(q1, q2)\n\n        actor_loss = (self.alpha * log_probs - q).mean()\n        return actor_loss\n"})}),"\n",(0,a.jsx)(e.h2,{id:"td3-twin-delayed-ddpg",children:"TD3 (Twin Delayed DDPG)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class TD3:\n    def __init__(self, actor, critic1, critic2, policy_delay=2):\n        self.policy_delay = policy_delay\n        self.update_count = 0\n\n    def update(self, batch):\n        # \u66f4\u65b0 critic\n        with torch.no_grad():\n            noise = torch.randn_like(batch.actions) * 0.2\n            next_actions = (self.target_actor(batch.next_states) + noise).clamp(-1, 1)\n\n            q1_next = self.target_critic1(batch.next_states, next_actions)\n            q2_next = self.target_critic2(batch.next_states, next_actions)\n            target_q = batch.rewards + self.gamma * torch.min(q1_next, q2_next)\n\n        q1 = self.critic1(batch.states, batch.actions)\n        q2 = self.critic2(batch.states, batch.actions)\n        critic_loss = nn.functional.mse_loss(q1, target_q) + nn.functional.mse_loss(q2, target_q)\n\n        self.update_count += 1\n\n        # \u5ef6\u8fdf\u66f4\u65b0 actor\n        if self.update_count % self.policy_delay == 0:\n            actor_loss = -self.critic1(batch.states, self.actor(batch.states)).mean()\n            # \u66f4\u65b0 actor\n            # \u8f6f\u66f4\u65b0 target \u7f51\u7edc\n"})}),"\n",(0,a.jsx)(e.h2,{id:"\u7b97\u6cd5\u5bf9\u6bd4",children:"\u7b97\u6cd5\u5bf9\u6bd4"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"\u7b97\u6cd5"}),(0,a.jsx)(e.th,{children:"\u7c7b\u578b"}),(0,a.jsx)(e.th,{children:"\u52a8\u4f5c\u7a7a\u95f4"}),(0,a.jsx)(e.th,{children:"\u7279\u70b9"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"DQN"}),(0,a.jsx)(e.td,{children:"Value-based"}),(0,a.jsx)(e.td,{children:"\u79bb\u6563"}),(0,a.jsx)(e.td,{children:"\u7ecf\u5178\u5165\u95e8"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"PPO"}),(0,a.jsx)(e.td,{children:"Policy"}),(0,a.jsx)(e.td,{children:"\u4e24\u8005"}),(0,a.jsx)(e.td,{children:"\u7b80\u5355\u9c81\u68d2"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"SAC"}),(0,a.jsx)(e.td,{children:"Actor-Critic"}),(0,a.jsx)(e.td,{children:"\u8fde\u7eed"}),(0,a.jsx)(e.td,{children:"\u6837\u672c\u6548\u7387\u9ad8"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"TD3"}),(0,a.jsx)(e.td,{children:"Actor-Critic"}),(0,a.jsx)(e.td,{children:"\u8fde\u7eed"}),(0,a.jsx)(e.td,{children:"\u7a33\u5b9a\u6027\u597d"})]})]})]}),"\n",(0,a.jsx)(e.h2,{id:"\u5e38\u7528\u5e93",children:"\u5e38\u7528\u5e93"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Stable Baselines 3\nfrom stable_baselines3 import PPO, SAC, TD3\n\nmodel = PPO("MlpPolicy", env, verbose=1)\nmodel.learn(total_timesteps=100000)\n\n# \u8bc4\u4f30\nobs = env.reset()\nfor _ in range(1000):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n'})})]})}function h(t={}){const{wrapper:e}={...(0,r.R)(),...t.components};return e?(0,a.jsx)(e,{...t,children:(0,a.jsx)(d,{...t})}):d(t)}},48885:(t,e,n)=>{n.d(e,{R:()=>i,x:()=>c});var s=n(99378);const a={},r=s.createContext(a);function i(t){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof t?t(e):{...e,...t}},[e,t])}function c(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(a):t.components||a:i(t.components),s.createElement(r.Provider,{value:e},t.children)}}}]);